{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9e6ace3f-a9f1-47bb-a399-673687c59bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\mrpea\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from gensim.utils import deaccent\n",
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "import re\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "import pickle\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "#\n",
    "LOAD_FILES = True\n",
    "\n",
    "#d_evidence = pd.read_json(\"data/evidence.json\", typ='series')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# contraction_dict from WS7\n",
    "contraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\",\n",
    "                    \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\",\n",
    "                    \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\",\n",
    "                    \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\",\n",
    "                    \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\",\n",
    "                    \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "                    \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\",\n",
    "                    \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\n",
    "                    \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
    "                    \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\",\n",
    "                    \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\",\n",
    "                    \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\n",
    "                    \"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "                    \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "                    \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\",\n",
    "                    \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\",\n",
    "                    \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\",\n",
    "                    \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\",\n",
    "                    \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\",\n",
    "                    \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\",\n",
    "                    \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                    \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "\n",
    "\n",
    "# https://stackoverflow.com/a/46231553\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None # for easy if-statement \n",
    "\n",
    "\n",
    "def sentence_preprocessing(sentence):\n",
    "\n",
    "    out_list = []\n",
    "    # Use gensim deaccent to match more characters to [a-z]\n",
    "    sentence = deaccent(sentence.lower())\n",
    "\n",
    "    for old, new in contraction_dict.items():\n",
    "        sentence.replace(old, new)\n",
    "\n",
    "    tokenized = word_tokenize(sentence)\n",
    "\n",
    "    # now remove all tokens that don't contain any alphanumeric characters\n",
    "    # then strip non alphanumeric characters afterwards\n",
    "    tokenized = [re.sub(r\"[^a-z0-9\\s]\", \"\", token) for token in tokenized if re.match(r\"[a-z0-9\\s]\", token)]\n",
    "\n",
    "    # now lemmatize with pos\n",
    "    tagged = pos_tag(tokenized)\n",
    "    for token, tag in tagged:\n",
    "        wntag = get_wordnet_pos(tag)\n",
    "\n",
    "        if wntag is None: # do not supply tag in case of None\n",
    "            lemma = lemmatizer.lemmatize(token) \n",
    "        else:\n",
    "            lemma = lemmatizer.lemmatize(token, pos=wntag) \n",
    "\n",
    "        out_list.append(lemma)\n",
    "    \n",
    "    return out_list\n",
    "\n",
    "\n",
    "def evidence_preprocessing(evidences):\n",
    "  t = time.time()\n",
    "  processed = []\n",
    "  for index, item in enumerate(evidences.items()):\n",
    "    id, evidence = item\n",
    "\n",
    "    row = []\n",
    "    \n",
    "    row.append(id)\n",
    "    row.append(evidence)\n",
    "\n",
    "    # break the text into sentences before tokenizing by each sentence\n",
    "    processed_sentences = [sentence_preprocessing(sentence) for sentence in sent_tokenize(evidence)]\n",
    "    row.append(processed_sentences)\n",
    "\n",
    "\n",
    "    # Appending an empty list to populate with embeddings later\n",
    "    row.append([])\n",
    "\n",
    "    processed.append(row)\n",
    "\n",
    "    if (index + 1) % 50000 == 0:\n",
    "        print(f\"{time.time() - t:.2f} - {index+1} rows processed\")\n",
    "\n",
    "  return pd.DataFrame(processed, columns = [\"id\", \"raw evidence\", \"processed evidence\", \"embeddings\"])\n",
    "\n",
    "\n",
    "# Evidence processing\n",
    "if not LOAD_FILES:\n",
    "    evidence = evidence_preprocessing(d_evidence)\n",
    "    with open(\"evidence_preprocessed_v3.pkl\", \"wb\") as f:\n",
    "        pickle.dump(evidence, f)\n",
    "else:\n",
    "    with open(\"evidence_preprocessed_v3.pkl\", \"rb\") as f:\n",
    "        evidence = pickle.load(f)\n",
    "    \n",
    "    evidence.head()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4d54a50a-961d-4961-898c-f77c239c7a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\mrpea\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from gensim.utils import deaccent\n",
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "import re\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "import pickle\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "#d_evidence = pd.read_json(\"data/evidence.json\", typ='series')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# contraction_dict from WS7\n",
    "contraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\",\n",
    "                    \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\",\n",
    "                    \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\",\n",
    "                    \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\",\n",
    "                    \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\",\n",
    "                    \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "                    \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\",\n",
    "                    \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\n",
    "                    \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
    "                    \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\",\n",
    "                    \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\",\n",
    "                    \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\n",
    "                    \"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "                    \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "                    \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\",\n",
    "                    \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\",\n",
    "                    \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\",\n",
    "                    \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\",\n",
    "                    \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\",\n",
    "                    \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\",\n",
    "                    \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                    \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# https://stackoverflow.com/a/46231553\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None # for easy if-statement \n",
    "    \n",
    "def sentence_preprocessing(sentence):\n",
    "\n",
    "    out_list = []\n",
    "    # Use gensim deaccent to match more characters to [a-z]\n",
    "    sentence = deaccent(sentence.lower())\n",
    "\n",
    "    for old, new in contraction_dict.items():\n",
    "        sentence.replace(old, new)\n",
    "\n",
    "    tokenized = word_tokenize(sentence)\n",
    "\n",
    "    # now remove all tokens that don't contain any alphanumeric characters\n",
    "    # then strip non alphanumeric characters afterwards\n",
    "    tokenized = [re.sub(r\"[^a-z0-9\\s]\", \"\", token) for token in tokenized if re.match(r\"[a-z0-9\\s]\", token)]\n",
    "\n",
    "    # now lemmatize with pos\n",
    "    tagged = pos_tag(tokenized)\n",
    "    for token, tag in tagged:\n",
    "        wntag = get_wordnet_pos(tag)\n",
    "\n",
    "        if wntag is None: # do not supply tag in case of None\n",
    "            lemma = lemmatizer.lemmatize(token) \n",
    "        else:\n",
    "            lemma = lemmatizer.lemmatize(token, pos=wntag) \n",
    "\n",
    "        out_list.append(lemma)\n",
    "    \n",
    "    return out_list\n",
    "\n",
    "\n",
    "# https://huggingface.co/learn/nlp-course/chapter6/6\n",
    "def encode_word(word):\n",
    "    tokens = []\n",
    "    while len(word) > 0:\n",
    "        i = len(word)\n",
    "        while i > 0 and word[:i] not in vocab:\n",
    "            i -= 1\n",
    "        if i == 0:\n",
    "            return [\"[UNK]\"]\n",
    "        tokens.append(word[:i])\n",
    "        word = word[i:]\n",
    "        if len(word) > 0:\n",
    "            word = f\"##{word}\"\n",
    "    return tokens\n",
    "\n",
    "# adapted from https://huggingface.co/learn/nlp-course/chapter6/6\n",
    "def tokenize(sentence):\n",
    "\n",
    "    # janky workaround for preprocessed sentences\n",
    "    if type(sentence) is not list:\n",
    "        sentence = sentence_preprocessing(sentence)\n",
    "        \n",
    "    encoded_words = [encode_word(word) for word in sentence]\n",
    "    return sum(encoded_words, [])\n",
    "\n",
    "\n",
    "\n",
    "with open(\"BPETokenizer_merge_rules_v1.5.pkl\", \"rb\") as f:\n",
    "    merge_rules = pickle.load(f)\n",
    "    \n",
    "\n",
    "# Reconstruct vocab from merge rules due to lack of foresight\n",
    "# This grabs all vocab of length 2 or above (if contains first letter)\n",
    "# or 4 or above (##__)\n",
    "vocab = [v for v in merge_rules.values()]\n",
    "\n",
    "# So iterate through merge rules again to find starting letters\n",
    "# and one letter suffixes\n",
    "for pair, merge in merge_rules.items():\n",
    "    if len(pair[0]) == 1 and pair[0] not in vocab:\n",
    "        vocab.append(pair[0])\n",
    "    if len(pair[1]) == 3 and pair[1] not in vocab:\n",
    "        vocab.append(pair[1])\n",
    "\n",
    "\n",
    "def processed_evidence_to_bpe(paragraph):\n",
    "    # 2d array -> paragraph\n",
    "    if type(paragraph[0]) is list:\n",
    "        return [tokenize(sentence) for sentence in paragraph]\n",
    "\n",
    "    # 1 sentence -> tokenize as is \n",
    "    else:\n",
    "        return tokenize(paragraph)\n",
    "\n",
    "\n",
    "counter = 0\n",
    "def processed_evidence_to_bpe(paragraph):\n",
    "    global counter\n",
    "    counter += 1\n",
    "    if counter % 1000 == 0:\n",
    "        print(f\"{counter} rows processed\")\n",
    "    #2d array -> paragraph\n",
    "    if type(paragraph[0]) is list:\n",
    "        return [tokenize(sentence) for sentence in paragraph]\n",
    "\n",
    "    # 1 sentence -> tokenize as is \n",
    "    else:\n",
    "        return tokenize(paragraph)\n",
    "\n",
    "\n",
    "# Save\n",
    "\n",
    "\"\"\"\n",
    "e[\"bpe evidence\"] = e[\"processed evidence\"].apply(processed_evidence_to_bpe)\n",
    "with open(\"BPETokenized_evidence_v3.pkl\", \"wb\") as f:\n",
    "    pickle.dump(e, f)\n",
    "\"\"\"\n",
    "\n",
    "# Load\n",
    "with open(\"BPETokenized_evidence_v3.pkl\", \"rb\") as f:\n",
    "    evidence = pickle.load(f)\n",
    "\n",
    "\"\"\"\n",
    "sentences = []\n",
    "\n",
    "for paragraph in evidence[\"bpe evidence\"]:\n",
    "    if type(paragraph[0]) is list:\n",
    "        for sentence in paragraph:\n",
    "            sentences.append(sentence)\n",
    "    else:\n",
    "        sentences.append(paragraph)\n",
    "\"\"\"\n",
    "\n",
    "# Now do word2vec\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "EMBEDDING_DIM = 200\n",
    "\"\"\"\n",
    "embedding_model = Word2Vec(sentences=sentences,\n",
    "                           vector_size=EMBEDDING_DIM,\n",
    "                           window=4,\n",
    "                           min_count=3,\n",
    "                           workers=10,\n",
    "                           negative=5\n",
    "                           )\n",
    "\n",
    "version = 3\n",
    "with open(f\"BPE Tokenizer to embedding/embeddings_BPE_v{version}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(embedding_model, f)\n",
    "\"\"\"\n",
    "\n",
    "# Load embedding\n",
    "with open(\"embeddings_BPE_v3.pkl\", \"rb\") as f:\n",
    "    embedding_model = pickle.load(f)\n",
    "\n",
    "import numpy as np\n",
    "def sentence_embedding(sentence):\n",
    "\n",
    "  # Failsafe\n",
    "  if len(sentence) == 0:\n",
    "    return np.zeros(EMBEDDING_DIM)\n",
    "\n",
    "  if type(sentence[0]) is not list:\n",
    "      sentence = tokenize(sentence)\n",
    "\n",
    "\n",
    "  embedding = np.zeros(EMBEDDING_DIM)\n",
    "  for word in sentence:\n",
    "    word_embedding = np.zeros(EMBEDDING_DIM)\n",
    "\n",
    "    # get word vector for given word\n",
    "    # if not found, ignore (treat as having the zero vector)\n",
    "    try:\n",
    "      word_embedding = embedding_model.wv[str(word)]\n",
    "    except KeyError:\n",
    "      pass\n",
    "\n",
    "    embedding += word_embedding\n",
    "\n",
    "  return embedding / len(sentence)\n",
    "\n",
    "\n",
    "def paragraph_embedding(paragraph):\n",
    "    out = []\n",
    "\n",
    "    # One sentence\n",
    "    if type(paragraph[0]) is not list:\n",
    "        return [sentence_embedding(paragraph)]\n",
    "\n",
    "    else:\n",
    "        for sentence in paragraph:\n",
    "            out.append(sentence_embedding(sentence))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e2c3092-13a5-4651-936e-995c4ef3419f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>raw evidence</th>\n",
       "      <th>processed evidence</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>evidence-0</td>\n",
       "      <td>John Bennet Lawes, English entrepreneur and ag...</td>\n",
       "      <td>[[john, bennet, lawes, english, entrepreneur, ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>evidence-1</td>\n",
       "      <td>Lindberg began his professional career at the ...</td>\n",
       "      <td>[[lindberg, begin, his, professional, career, ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>evidence-2</td>\n",
       "      <td>``Boston (Ladies of Cambridge)'' by Vampire We...</td>\n",
       "      <td>[[boston, lady, of, cambridge, by, vampire, we...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>evidence-3</td>\n",
       "      <td>Gerald Francis Goyer (born October 20, 1936) w...</td>\n",
       "      <td>[[gerald, francis, goyer, born, october, 20, 1...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>evidence-4</td>\n",
       "      <td>He detected abnormalities of oxytocinergic fun...</td>\n",
       "      <td>[[he, detect, abnormality, of, oxytocinergic, ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                       raw evidence  \\\n",
       "0  evidence-0  John Bennet Lawes, English entrepreneur and ag...   \n",
       "1  evidence-1  Lindberg began his professional career at the ...   \n",
       "2  evidence-2  ``Boston (Ladies of Cambridge)'' by Vampire We...   \n",
       "3  evidence-3  Gerald Francis Goyer (born October 20, 1936) w...   \n",
       "4  evidence-4  He detected abnormalities of oxytocinergic fun...   \n",
       "\n",
       "                                  processed evidence embeddings  \n",
       "0  [[john, bennet, lawes, english, entrepreneur, ...         []  \n",
       "1  [[lindberg, begin, his, professional, career, ...         []  \n",
       "2  [[boston, lady, of, cambridge, by, vampire, we...         []  \n",
       "3  [[gerald, francis, goyer, born, october, 20, 1...         []  \n",
       "4  [[he, detect, abnormality, of, oxytocinergic, ...         []  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evidence.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8c896d0-3c27-40f1-80a2-9d1ea21473cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline retrieval: immediately use the raw embeddings to retrieve closest sentences\n",
    "# Train a cutoff distance threshold.\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Similarity based on cosine similarity ([0-1], higher the more similar)\n",
    "def similarity(text, evidence_ids):\n",
    "\n",
    "    # Seems stupid and retrieving everything from w2v is probably cleaner\n",
    "    # TODO: make this better\n",
    "    evidence_embeddings = [evidence.loc[evidence['id'] == id, 'embeddings'].values[0] for id in evidence_ids]\n",
    "    key_embedding = sentence_embedding(text)\n",
    "    \n",
    "    similarities = []\n",
    "    for evidence_embedding in evidence_embeddings:\n",
    "        similarities.append(1-cosine(key_embedding, evidence_embedding))\n",
    "\n",
    "    return similarities\n",
    "\n",
    "\n",
    "# Using 1 - fscore as the loss\n",
    "def retrieval_loss(prediction, target):\n",
    "    numerator = 0\n",
    "    denominator = 0\n",
    "    \n",
    "    for p in prediction:\n",
    "        if p in target:\n",
    "            denominator += 2\n",
    "            numerator += 2\n",
    "        else:\n",
    "            denominator += 1\n",
    "    \n",
    "    for t in target:\n",
    "        if t not in prediction:\n",
    "            denominator += 1\n",
    "    \n",
    "    return 1 - numerator/denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0f6aac8e-6daa-4d7e-9c73-2a8dbdbd7f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"dev-claims.json\") as f:\n",
    "    dev = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "05bd634f-9aff-47da-92d6-edf09db30293",
   "metadata": {},
   "outputs": [],
   "source": [
    "def claim_preprocessing(claims):\n",
    "  processed = []\n",
    "  for id, inner in claims.items():\n",
    "        \n",
    "    row = []\n",
    "    row.append(id)\n",
    "    row.append(inner.get(\"claim_text\"))\n",
    "    row.append(sentence_preprocessing(inner.get(\"claim_text\")))\n",
    "\n",
    "    # No label or evidence for unlabelled set\n",
    "    row.append(inner.get(\"claim_label\", None))\n",
    "    row.append(inner.get(\"evidences\", None))\n",
    "\n",
    "    processed.append(row)\n",
    "\n",
    "  return pd.DataFrame(processed, columns = [\"id\", \"claim_text\", \"processed text\", \"claim_label\", \"evidences\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2ca4fcfe-ac7a-4c68-bb22-062c8066aa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_dev = claim_preprocessing(dev)\n",
    "d_dev.head()\n",
    "\n",
    "d_dev[\"bpe evidence\"] = d_dev[\"processed text\"].apply(processed_evidence_to_bpe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2113055f-f73f-4c48-a12b-2634b51256ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>claim_text</th>\n",
       "      <th>processed text</th>\n",
       "      <th>claim_label</th>\n",
       "      <th>evidences</th>\n",
       "      <th>bpe evidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>claim-752</td>\n",
       "      <td>[South Australia] has the most expensive elect...</td>\n",
       "      <td>[south, australia, have, the, most, expensive,...</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>[evidence-67732, evidence-572512]</td>\n",
       "      <td>[south, australia, have, the, most, expensive,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>claim-375</td>\n",
       "      <td>when 3 per cent of total annual global emissio...</td>\n",
       "      <td>[when, 3, per, cent, of, total, annual, global...</td>\n",
       "      <td>NOT_ENOUGH_INFO</td>\n",
       "      <td>[evidence-996421, evidence-1080858, evidence-2...</td>\n",
       "      <td>[when, 3, per, cent, of, total, annual, global...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>claim-1266</td>\n",
       "      <td>This means that the world is now 1C warmer tha...</td>\n",
       "      <td>[this, mean, that, the, world, be, now, 1c, wa...</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>[evidence-889933, evidence-694262]</td>\n",
       "      <td>[this, mean, that, the, world, be, now, 1, ##c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>claim-871</td>\n",
       "      <td>“As it happens, Zika may also be a good model ...</td>\n",
       "      <td>[a, it, happen, zika, may, also, be, a, good, ...</td>\n",
       "      <td>NOT_ENOUGH_INFO</td>\n",
       "      <td>[evidence-422399, evidence-702226, evidence-28...</td>\n",
       "      <td>[a, it, happen, z, ##ika, may, also, be, a, go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>claim-2164</td>\n",
       "      <td>Greenland has only lost a tiny fraction of its...</td>\n",
       "      <td>[greenland, have, only, lose, a, tiny, fractio...</td>\n",
       "      <td>REFUTES</td>\n",
       "      <td>[evidence-52981, evidence-264761, evidence-947...</td>\n",
       "      <td>[greenland, have, only, lose, a, tiny, fractio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                         claim_text  \\\n",
       "0   claim-752  [South Australia] has the most expensive elect...   \n",
       "1   claim-375  when 3 per cent of total annual global emissio...   \n",
       "2  claim-1266  This means that the world is now 1C warmer tha...   \n",
       "3   claim-871  “As it happens, Zika may also be a good model ...   \n",
       "4  claim-2164  Greenland has only lost a tiny fraction of its...   \n",
       "\n",
       "                                      processed text      claim_label  \\\n",
       "0  [south, australia, have, the, most, expensive,...         SUPPORTS   \n",
       "1  [when, 3, per, cent, of, total, annual, global...  NOT_ENOUGH_INFO   \n",
       "2  [this, mean, that, the, world, be, now, 1c, wa...         SUPPORTS   \n",
       "3  [a, it, happen, zika, may, also, be, a, good, ...  NOT_ENOUGH_INFO   \n",
       "4  [greenland, have, only, lose, a, tiny, fractio...          REFUTES   \n",
       "\n",
       "                                           evidences  \\\n",
       "0                  [evidence-67732, evidence-572512]   \n",
       "1  [evidence-996421, evidence-1080858, evidence-2...   \n",
       "2                 [evidence-889933, evidence-694262]   \n",
       "3  [evidence-422399, evidence-702226, evidence-28...   \n",
       "4  [evidence-52981, evidence-264761, evidence-947...   \n",
       "\n",
       "                                        bpe evidence  \n",
       "0  [south, australia, have, the, most, expensive,...  \n",
       "1  [when, 3, per, cent, of, total, annual, global...  \n",
       "2  [this, mean, that, the, world, be, now, 1, ##c...  \n",
       "3  [a, it, happen, z, ##ika, may, also, be, a, go...  \n",
       "4  [greenland, have, only, lose, a, tiny, fractio...  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_dev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "417081ec-d51a-496c-ba28-3c5b2545fd80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>raw evidence</th>\n",
       "      <th>processed evidence</th>\n",
       "      <th>embeddings</th>\n",
       "      <th>bpe evidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>evidence-0</td>\n",
       "      <td>John Bennet Lawes, English entrepreneur and ag...</td>\n",
       "      <td>[[john, bennet, lawes, english, entrepreneur, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[john, benn, ##et, law, ##es, english, entrep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>evidence-1</td>\n",
       "      <td>Lindberg began his professional career at the ...</td>\n",
       "      <td>[[lindberg, begin, his, professional, career, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[lind, ##berg, begin, his, professional, care...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>evidence-2</td>\n",
       "      <td>``Boston (Ladies of Cambridge)'' by Vampire We...</td>\n",
       "      <td>[[boston, lady, of, cambridge, by, vampire, we...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[boston, lady, of, cambridge, by, vampire, we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>evidence-3</td>\n",
       "      <td>Gerald Francis Goyer (born October 20, 1936) w...</td>\n",
       "      <td>[[gerald, francis, goyer, born, october, 20, 1...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[gerald, francis, go, ##yer, born, october, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>evidence-4</td>\n",
       "      <td>He detected abnormalities of oxytocinergic fun...</td>\n",
       "      <td>[[he, detect, abnormality, of, oxytocinergic, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[he, detect, ab, ##normal, ##ity, of, oxy, ##...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                       raw evidence  \\\n",
       "0  evidence-0  John Bennet Lawes, English entrepreneur and ag...   \n",
       "1  evidence-1  Lindberg began his professional career at the ...   \n",
       "2  evidence-2  ``Boston (Ladies of Cambridge)'' by Vampire We...   \n",
       "3  evidence-3  Gerald Francis Goyer (born October 20, 1936) w...   \n",
       "4  evidence-4  He detected abnormalities of oxytocinergic fun...   \n",
       "\n",
       "                                  processed evidence embeddings  \\\n",
       "0  [[john, bennet, lawes, english, entrepreneur, ...         []   \n",
       "1  [[lindberg, begin, his, professional, career, ...         []   \n",
       "2  [[boston, lady, of, cambridge, by, vampire, we...         []   \n",
       "3  [[gerald, francis, goyer, born, october, 20, 1...         []   \n",
       "4  [[he, detect, abnormality, of, oxytocinergic, ...         []   \n",
       "\n",
       "                                        bpe evidence  \n",
       "0  [[john, benn, ##et, law, ##es, english, entrep...  \n",
       "1  [[lind, ##berg, begin, his, professional, care...  \n",
       "2  [[boston, lady, of, cambridge, by, vampire, we...  \n",
       "3  [[gerald, francis, go, ##yer, born, october, 2...  \n",
       "4  [[he, detect, ab, ##normal, ##ity, of, oxy, ##...  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"BPETokenized_evidence_v3.pkl\", \"rb\") as f:\n",
    "    d_evidence = pickle.load(f)\n",
    "d_evidence.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "de74cfa6-13fa-415c-b38d-6c8e5bc085e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m d_evidence[embeddings] \u001b[38;5;241m=\u001b[39m \u001b[43md_evidence\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbpe evidence\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparagraph_embedding\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python38env\\lib\\site-packages\\pandas\\core\\series.py:4630\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4520\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4521\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4522\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4525\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4526\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4527\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4528\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4529\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4628\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4629\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4630\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python38env\\lib\\site-packages\\pandas\\core\\apply.py:1025\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python38env\\lib\\site-packages\\pandas\\core\\apply.py:1076\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1074\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1075\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m-> 1076\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1077\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1078\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1079\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1080\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1082\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1083\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python38env\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2834\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[52], line 10\u001b[0m, in \u001b[0;36mparagraph_embedding\u001b[1;34m(paragraph)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m paragraph:\n\u001b[1;32m---> 10\u001b[0m         out\u001b[38;5;241m.\u001b[39mappend(\u001b[43msentence_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "Cell \u001b[1;32mIn[54], line 22\u001b[0m, in \u001b[0;36msentence_embedding\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m     19\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m   embedding \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m word_embedding\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embedding \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(sentence)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "d_evidence[embeddings] = d_evidence[\"bpe evidence\"].apply(paragraph_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b8853214-1e30-4216-86dc-c50da2f81aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_embedding(sentence):\n",
    "\n",
    "  # Failsafe\n",
    "  if len(sentence) == 0:\n",
    "    return np.zeros(EMBEDDING_DIM)\n",
    "\n",
    "  if type(sentence[0]) is not list:\n",
    "      sentence = tokenize(sentence)\n",
    "\n",
    "\n",
    "  embedding = np.zeros(EMBEDDING_DIM)\n",
    "  for word in sentence:\n",
    "    word_embedding = np.zeros(EMBEDDING_DIM)\n",
    "\n",
    "    # get word vector for given word\n",
    "    # if not found, ignore (treat as having the zero vector)\n",
    "    try:\n",
    "      word_embedding = embedding_model.wv[str(word)]\n",
    "    except KeyError:\n",
    "      pass\n",
    "\n",
    "    embedding += word_embedding\n",
    "\n",
    "  return embedding / len(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "405e49ef-aaca-4706-a743-0d89acfbddb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [[0.012712392210960387, 0.5824892453849315, 0....\n",
      "1    [[-0.4110906516250811, -0.13801924715210734, 0...\n",
      "2    [[-0.5756708331006978, 0.24082601070404053, 0....\n",
      "3    [[-0.23389491179715033, -0.2673430044365966, 0...\n",
      "Name: bpe evidence, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(d_evidence.loc[:3, \"bpe evidence\"].apply(paragraph_embedding))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd77e51c-6140-4ab4-89a1-c0d041f51795",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
