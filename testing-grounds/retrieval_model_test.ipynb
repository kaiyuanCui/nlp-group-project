{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32yCsRUo8H33"
      },
      "source": [
        "# 2024 COMP90042 Project\n",
        "*Make sure you change the file name with your group id.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCybYoGz8YWQ"
      },
      "source": [
        "# Readme\n",
        "*If there is something to be noted for the marker, please mention here.*\n",
        "\n",
        "*If you are planning to implement a program with Object Oriented Programming style, please put those the bottom of this ipynb file*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6po98qVA8bJD"
      },
      "source": [
        "# 1.DataSet Processing\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjGn8oHh9lew"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvff21Hv8zjk",
        "outputId": "3856e2ef-c764-4d97-a681-9a54b02c04f2"
      },
      "outputs": [],
      "source": [
        "LOCAL_DEV = True # to switch between developing locally and on colab\n",
        "\n",
        "if not LOCAL_DEV:\n",
        "    # TODO: need to upload data files on Google Drive?\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "MH1Hdrb5NmHM"
      },
      "outputs": [],
      "source": [
        "#Imports\n",
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "2piZvV4OMSa3",
        "outputId": "69009174-8de4-4d4f-e56e-be8b5a39e1aa"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>claim_text</th>\n",
              "      <th>claim_label</th>\n",
              "      <th>evidences</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>claim-1937</th>\n",
              "      <td>Not only is there no scientific evidence that ...</td>\n",
              "      <td>DISPUTED</td>\n",
              "      <td>[evidence-442946, evidence-1194317, evidence-1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-126</th>\n",
              "      <td>El Niño drove record highs in global temperatu...</td>\n",
              "      <td>REFUTES</td>\n",
              "      <td>[evidence-338219, evidence-1127398]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-2510</th>\n",
              "      <td>In 1946, PDO switched to a cool phase.</td>\n",
              "      <td>SUPPORTS</td>\n",
              "      <td>[evidence-530063, evidence-984887]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-2021</th>\n",
              "      <td>Weather Channel co-founder John Coleman provid...</td>\n",
              "      <td>DISPUTED</td>\n",
              "      <td>[evidence-1177431, evidence-782448, evidence-5...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-2449</th>\n",
              "      <td>\"January 2008 capped a 12 month period of glob...</td>\n",
              "      <td>NOT_ENOUGH_INFO</td>\n",
              "      <td>[evidence-1010750, evidence-91661, evidence-72...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   claim_text  \\\n",
              "claim-1937  Not only is there no scientific evidence that ...   \n",
              "claim-126   El Niño drove record highs in global temperatu...   \n",
              "claim-2510             In 1946, PDO switched to a cool phase.   \n",
              "claim-2021  Weather Channel co-founder John Coleman provid...   \n",
              "claim-2449  \"January 2008 capped a 12 month period of glob...   \n",
              "\n",
              "                claim_label                                          evidences  \n",
              "claim-1937         DISPUTED  [evidence-442946, evidence-1194317, evidence-1...  \n",
              "claim-126           REFUTES                [evidence-338219, evidence-1127398]  \n",
              "claim-2510         SUPPORTS                 [evidence-530063, evidence-984887]  \n",
              "claim-2021         DISPUTED  [evidence-1177431, evidence-782448, evidence-5...  \n",
              "claim-2449  NOT_ENOUGH_INFO  [evidence-1010750, evidence-91661, evidence-72...  "
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#visualising training data\n",
        "if LOCAL_DEV:\n",
        "    train = pd.read_json(\"../data/train-claims.json\") # for local dev\n",
        "    \n",
        "else:\n",
        "    train = pd.read_json(\"/content/drive/MyDrive/data/train-claims.json\") # on colab\n",
        "train = train.transpose()\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ZKbGFcA2THHP"
      },
      "outputs": [],
      "source": [
        "#visualising evidence data\n",
        "#visualising evidence data\n",
        "if LOCAL_DEV:\n",
        "    evidence = pd.read_json(\"../data/evidence.json\",typ='series')\n",
        "else:\n",
        "    evidence = pd.read_json(\"/content/drive/MyDrive/data/evidence.json\",typ='series')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFNTiC0UMS45",
        "outputId": "a6bd81bb-dbc0-4eb3-f080-16f6606b9e13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1208827\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "evidence-0    John Bennet Lawes, English entrepreneur and ag...\n",
              "evidence-1    Lindberg began his professional career at the ...\n",
              "evidence-2    ``Boston (Ladies of Cambridge)'' by Vampire We...\n",
              "evidence-3    Gerald Francis Goyer (born October 20, 1936) w...\n",
              "evidence-4    He detected abnormalities of oxytocinergic fun...\n",
              "dtype: object"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(len(evidence))\n",
        "evidence.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "-HQn6h2hYukS",
        "outputId": "edc2dff9-4fcd-4e88-d44d-21f6e35cbbeb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>claim_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>claim-2967</th>\n",
              "      <td>The contribution of waste heat to the global c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-979</th>\n",
              "      <td>“Warm weather worsened the most recent five-ye...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-1609</th>\n",
              "      <td>Greenland has only lost a tiny fraction of its...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-1020</th>\n",
              "      <td>“The global reef crisis does not necessarily m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-2599</th>\n",
              "      <td>Small amounts of very active substances can ca...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   claim_text\n",
              "claim-2967  The contribution of waste heat to the global c...\n",
              "claim-979   “Warm weather worsened the most recent five-ye...\n",
              "claim-1609  Greenland has only lost a tiny fraction of its...\n",
              "claim-1020  “The global reef crisis does not necessarily m...\n",
              "claim-2599  Small amounts of very active substances can ca..."
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "if LOCAL_DEV:\n",
        "    test = pd.read_json(\"../data/test-claims-unlabelled.json\")\n",
        "else:\n",
        "    test = pd.read_json(\"/content/drive/MyDrive/data/test-claims-unlabelled.json\")\n",
        "test = test.transpose()\n",
        "test.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Xf73PDzRTrft"
      },
      "outputs": [],
      "source": [
        "#preprocessing\n",
        "# punctuations should be removed, common words such as the, is, are, should be removed. all words also should be lemmentised and stemmed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qL-bItuvn19d",
        "outputId": "01e87758-b752-447a-bbd0-e15ea30fb27f",
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras in /Users/kaiyuancui/opt/anaconda3/lib/python3.9/site-packages (2.15.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHWbo7W7TuUC",
        "outputId": "5cd4f2b1-ab01-424a-d499-cdd5f2945056"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/kaiyuancui/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/kaiyuancui/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /Users/kaiyuancui/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import string\n",
        "import contractions\n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXFRgZCnRO1V",
        "outputId": "a74cee0a-709c-4014-8056-aac9d978b6e7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "claim-1937    not scientific evidence pollutant higher conce...\n",
              "claim-126     el niño drove record high global temperature s...\n",
              "claim-2510                              pdo switched cool phase\n",
              "claim-2021    weather channel john coleman provided evidence...\n",
              "claim-2449    january capped month period global temperature...\n",
              "dtype: object"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "\n",
        "def preprocess_data(data: pd.Series, limit=10000) -> pd.Series:\n",
        "  preprocessed_data = {}\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  stop_words.remove('not')\n",
        "  count = 0\n",
        "  for id, text in data.items():\n",
        "    text = text.lower()\n",
        "    text = contractions.fix(text)\n",
        "    tokens = word_tokenize(text)\n",
        "    wnl = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [wnl.lemmatize(word) for word in tokens if word.isalpha() and word not in stop_words]\n",
        "    preprocessed_data[id] = \" \".join(lemmatized_tokens)\n",
        "    count += 1\n",
        "    if count >= limit:\n",
        "      break\n",
        "\n",
        "  return pd.Series(preprocessed_data)\n",
        "\n",
        "processed_evidence = preprocess_data(evidence)\n",
        "\n",
        "test_claims = test['claim_text']\n",
        "train_claims = train['claim_text']\n",
        "processed_test = preprocess_data(test_claims)\n",
        "processed_test.head()\n",
        "processed_train = preprocess_data(train_claims)\n",
        "processed_train.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "x_5xtX0qzryS"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "evidence-0    john bennet lawes english entrepreneur agricul...\n",
              "evidence-1    lindberg began professional career age eventua...\n",
              "evidence-2                boston lady cambridge vampire weekend\n",
              "evidence-3    gerald francis goyer born october professional...\n",
              "evidence-4    detected abnormality oxytocinergic function sc...\n",
              "dtype: object"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "processed_evidence = processed_evidence[processed_evidence.str.strip().str.len() > 0]\n",
        "processed_evidence.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnNiS5byTvJM"
      },
      "source": [
        "# Two steps for the this task\n",
        "# first. find all relavent evidence, either use contextual embedding or similarity scoring\n",
        "# second. classify the evidents into 4 classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FA2ao2l8hOg"
      },
      "source": [
        "# 2. Model Implementation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "MXbiFXUfvngL"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from keras.layers import Dense\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Embedding, Flatten, Dense\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "# from tensorflow.keras.models import Sequential\n",
        "# from tensorflow.keras.layers import Embedding, Flatten, Dense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "QIEqDDT78q39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['aa' 'aaa' 'aabis' ... '楊璟翊' '盧翰' '민주국민당']\n",
            "  (0, 24182)\t0.5894022428982739\n",
            "  (0, 9912)\t0.43153385604356104\n",
            "  (0, 9096)\t0.3217964341887153\n",
            "  (0, 4988)\t0.5097918621830975\n",
            "  (0, 4330)\t0.32084706535977053\n"
          ]
        }
      ],
      "source": [
        "#Vectorizing preprocessed text\n",
        "vectorizer = TfidfVectorizer()\n",
        "all_texts = pd.concat([processed_evidence, processed_train])\n",
        "vectorizer.fit(all_texts)\n",
        "evidence_tfidf = vectorizer.transform(processed_evidence)\n",
        "test_tfidf = vectorizer.transform(processed_test)\n",
        "train_tfidf = vectorizer.transform(processed_train)\n",
        "\n",
        "print(vectorizer.get_feature_names_out()) # why does this contain non-alphabetic?\n",
        "print(test_tfidf[0])\n",
        "#print(test_tfidf.shape)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>claim_text</th>\n",
              "      <th>evidences</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>claim-2967</th>\n",
              "      <td>contribution waste heat global climate</td>\n",
              "      <td>[evidence-8950, evidence-4903, evidence-1294, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-979</th>\n",
              "      <td>warm weather worsened recent drought included ...</td>\n",
              "      <td>[evidence-2760, evidence-8828, evidence-5911, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-1609</th>\n",
              "      <td>greenland lost tiny fraction ice mass</td>\n",
              "      <td>[evidence-5928, evidence-4202, evidence-3680, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-1020</th>\n",
              "      <td>global reef crisis not necessarily mean extinc...</td>\n",
              "      <td>[evidence-3210, evidence-8721, evidence-2739, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-2599</th>\n",
              "      <td>small amount active substance cause large effect</td>\n",
              "      <td>[evidence-8207, evidence-8000, evidence-320, e...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   claim_text  \\\n",
              "claim-2967             contribution waste heat global climate   \n",
              "claim-979   warm weather worsened recent drought included ...   \n",
              "claim-1609              greenland lost tiny fraction ice mass   \n",
              "claim-1020  global reef crisis not necessarily mean extinc...   \n",
              "claim-2599   small amount active substance cause large effect   \n",
              "\n",
              "                                                    evidences  \n",
              "claim-2967  [evidence-8950, evidence-4903, evidence-1294, ...  \n",
              "claim-979   [evidence-2760, evidence-8828, evidence-5911, ...  \n",
              "claim-1609  [evidence-5928, evidence-4202, evidence-3680, ...  \n",
              "claim-1020  [evidence-3210, evidence-8721, evidence-2739, ...  \n",
              "claim-2599  [evidence-8207, evidence-8000, evidence-320, e...  "
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "similarity_matrix = cosine_similarity(test_tfidf, evidence_tfidf)\n",
        "\n",
        "def getTopN(similarity_matrix, test, evidence, n):\n",
        "  test = test.to_frame(name='claim_text')\n",
        "  top_indices = np.argsort(-similarity_matrix, axis = 1)[:, :n]\n",
        "  top_evidence = [[str(evidence.index[i]) for i in row] for row in top_indices]\n",
        "  test['evidences'] = top_evidence\n",
        "  return test\n",
        "\n",
        "test_with_evi = getTopN(similarity_matrix, processed_test, processed_evidence, 5)\n",
        "test_with_evi.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torchtext==0.4.0\n",
            "  Downloading torchtext-0.4.0-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /Users/kaiyuancui/opt/anaconda3/lib/python3.9/site-packages (from torchtext==0.4.0) (1.16.0)\n",
            "Requirement already satisfied: numpy in /Users/kaiyuancui/opt/anaconda3/lib/python3.9/site-packages (from torchtext==0.4.0) (1.24.4)\n",
            "Requirement already satisfied: torch in /Users/kaiyuancui/opt/anaconda3/lib/python3.9/site-packages (from torchtext==0.4.0) (2.2.1)\n",
            "Requirement already satisfied: requests in /Users/kaiyuancui/opt/anaconda3/lib/python3.9/site-packages (from torchtext==0.4.0) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /Users/kaiyuancui/opt/anaconda3/lib/python3.9/site-packages (from torchtext==0.4.0) (4.64.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/kaiyuancui/opt/anaconda3/lib/python3.9/site-packages (from requests->torchtext==0.4.0) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/kaiyuancui/opt/anaconda3/lib/python3.9/site-packages (from requests->torchtext==0.4.0) (3.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/kaiyuancui/opt/anaconda3/lib/python3.9/site-packages (from requests->torchtext==0.4.0) (2022.9.24)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kaiyuancui/opt/anaconda3/lib/python3.9/site-packages (from requests->torchtext==0.4.0) (1.26.11)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/kaiyuancui/opt/anaconda3/lib/python3.9/site-packages (from torch->torchtext==0.4.0) (4.10.0)\n",
            "Requirement already satisfied: filelock in /Users/kaiyuancui/opt/anaconda3/lib/python3.9/site-packages (from torch->torchtext==0.4.0) (3.6.0)\n",
            "Requirement already satisfied: networkx in /Users/kaiyuancui/opt/anaconda3/lib/python3.9/site-packages (from torch->torchtext==0.4.0) (2.8.4)\n",
            "Requirement already satisfied: sympy in /Users/kaiyuancui/opt/anaconda3/lib/python3.9/site-packages (from torch->torchtext==0.4.0) (1.10.1)\n",
            "Requirement already satisfied: jinja2 in /Users/kaiyuancui/opt/anaconda3/lib/python3.9/site-packages (from torch->torchtext==0.4.0) (2.11.3)\n",
            "Requirement already satisfied: fsspec in /Users/kaiyuancui/opt/anaconda3/lib/python3.9/site-packages (from torch->torchtext==0.4.0) (2022.7.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /Users/kaiyuancui/opt/anaconda3/lib/python3.9/site-packages (from jinja2->torch->torchtext==0.4.0) (2.0.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /Users/kaiyuancui/opt/anaconda3/lib/python3.9/site-packages (from sympy->torch->torchtext==0.4.0) (1.2.1)\n",
            "Installing collected packages: torchtext\n",
            "Successfully installed torchtext-0.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip3 install torchtext==0.4.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "VSq_5IHKuTUu",
        "outputId": "e53b3ea8-1625-46f6-9e8d-f963c83e9eea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "downloading aclImdb_v1.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:18<00:00, 4.62MB/s]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# using a classification model to test relevance of each evidence\n",
        "\n",
        "# assuming train data text formatted as follows:\n",
        "# trainX = [claim_text + SEPARATION_TOKEN + evidence_text, .....]\n",
        "# trainY = [RELEVANT, NOT_RELEVANT, ...... ]\n",
        "\n",
        "# WORKSHOP 8 ------\n",
        "import torch\n",
        "import torchtext\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "# Define tokenizer\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "# Define Field for text data\n",
        "TEXT = torchtext.data.Field(tokenize=tokenizer,\n",
        "                            init_token='<sos>',\n",
        "                            eos_token='<eos>',\n",
        "                            lower=True)\n",
        "\n",
        "# Define Field for label data\n",
        "LABEL = torchtext.data.LabelField(dtype=torch.float)\n",
        "\n",
        "# Load IMDb dataset\n",
        "train_txt, test_txt = torchtext.datasets.IMDB.splits(TEXT, LABEL)\n",
        "\n",
        "# Split train_txt into train and validation sets\n",
        "train_txt, valid_txt = train_txt.split(split_ratio=0.8)\n",
        "\n",
        "# Build vocabulary\n",
        "TEXT.build_vocab(train_txt)\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Batchify function\n",
        "def batchify(data, bsz):\n",
        "    data = TEXT.numericalize([data.examples[0].text])\n",
        "    # Divide the dataset into bsz parts.\n",
        "    nbatch = data.size(0) // bsz\n",
        "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "    data = data.narrow(0, 0, nbatch * bsz)\n",
        "    # Evenly divide the data across the bsz batches.\n",
        "    data = data.view(bsz, -1).t().contiguous()\n",
        "    return data.to(device)\n",
        "\n",
        "# Batch sizes\n",
        "batch_size = 20\n",
        "eval_batch_size = 10\n",
        "\n",
        "# Process the datasets\n",
        "train_data = batchify(train_txt, batch_size)\n",
        "val_data = batchify(valid_txt, eval_batch_size)\n",
        "test_data = batchify(test_txt, eval_batch_size)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['how', 'can', 'such', 'good', 'actors', 'like', 'jean', 'rochefort', 'and', 'carole', 'bouquet', 'could', 'have', 'been', 'involved', 'in', 'such', 'a', '.', '.', '.', 'a', '.', '.', '.', 'well', ',', 'such', 'a', 'thing', '?', 'i', 'can', \"'\", 't', 'get', 'it', '.', 'it', 'was', 'awful', ',', 'very', 'baldy', 'played', '(', 'but', 'some', 'of', 'the', 'few', 'leading', 'roles', ')', ',', 'the', 'jokes', 'are', 'dumb', 'and', 'absolutely', 'not', 'funny', '.', '.', '.', 'i', 'won', \"'\", 't', 'talk', 'more', 'about', 'this', 'movie', ',', 'except', 'for', 'one', 'little', 'piece', 'of', 'advice', 'do', 'not', 'go', 'see', 'it', ',', 'it', 'will', 'be', 'a', 'waste', 'of', 'time', 'and', 'money', '.']\n",
            "neg\n",
            "['wow', ',', 'finally', 'jim', 'carrey', 'has', 'returned', 'from', 'the', 'died', '.', 'this', 'movie', 'had', 'me', 'laughing', 'and', 'crying', '.', 'it', 'also', 'sends', 'a', 'message', 'that', 'we', 'should', 'all', 'know', 'and', 'learn', 'from', '.', 'jeniffer', 'aniston', 'was', 'great', ',', 'she', 'will', 'finally', 'have', 'a', 'hit', 'movie', 'under', 'her', 'belt', '.', 'if', 'you', 'liked', 'liar', 'liar', 'you', 'will', 'love', 'this', 'movie', '.', 'i', 'give', 'it', '9/10', '.']\n",
            "pos\n",
            "tensor([[   96,  9776,     5,   147,    13,    27,     6,     5,    52,     9],\n",
            "        [   60, 37708,     5,     8,     5,    24,     4,     5,    16,  1997],\n",
            "        [  147,   105,     5,   163,    13,    58,   632,     5,    23,    93],\n",
            "        [   59,    35,     8,    57,    19,     9,    33,    15,     6,    31],\n",
            "        [  164,    86,     5,    15,   386,     4,   964,   374,   540,   150],\n",
            "        [   47,   575,     5,    60,     6,   181,     7,    11,    22,    77],\n",
            "        [ 1881,    14,     5,    11,    64,   969,   436,    29,    37,    13],\n",
            "        [29759,   147,    82,    29,     0,   556,    31,   732,   123,     6],\n",
            "        [    7,     8,     6,    85,   261,    26,   165,    61,   420,    13]])\n"
          ]
        }
      ],
      "source": [
        "print(valid_txt[0].text)\n",
        "print(valid_txt[0].label)\n",
        "\n",
        "print(valid_txt[1].text)\n",
        "print(valid_txt[1].label)\n",
        "\n",
        "# ours can be:\n",
        "# text =  claim_tokens + [SEP_TOKEN] + evidence_tokens\n",
        "# label <- one of {'rel', 'irr'} (relevant, irrelevant)\n",
        "\n",
        "# and use the same batchify method ?\n",
        "\n",
        "print(val_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/kaiyuancui/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        }
      ],
      "source": [
        "bptt = 35\n",
        "def get_batch(source, i):\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].view(-1)\n",
        "    return data, target\n",
        "\n",
        "\n",
        "ntokens = len(TEXT.vocab.stoi) # the size of vocabulary\n",
        "emsize = 200 # embedding dimension\n",
        "nhid = 200 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
        "nlayers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
        "nhead = 2 # the number of heads in the multiheadattention models\n",
        "dropout = 0.2 # the dropout value\n",
        "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 5.0 # learning rate\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
        "\n",
        "import time\n",
        "def train():\n",
        "\n",
        "    model.train() # Turn on the train mode\n",
        "    total_loss = 0.\n",
        "    start_time = time.time()\n",
        "    ntokens = len(TEXT.vocab.stoi)\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output.view(-1, ntokens), targets)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        log_interval = 200\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            cur_loss = total_loss / log_interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
        "                  'lr {:02.2f} | ms/batch {:5.2f} | '\n",
        "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                    epoch, batch, len(train_data) // bptt, scheduler.get_lr()[0],\n",
        "                    elapsed * 1000 / log_interval,\n",
        "                    cur_loss, math.exp(cur_loss)))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "\n",
        "\n",
        "def evaluate(eval_model, data_source):\n",
        "    eval_model.eval() # Turn on the evaluation mode\n",
        "    total_loss = 0.\n",
        "    ntokens = len(TEXT.vocab.stoi)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, data_source.size(0) - 1, bptt):\n",
        "            data, targets = get_batch(data_source, i)\n",
        "            output = eval_model(data)\n",
        "            output_flat = output.view(-1, ntokens)\n",
        "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
        "    return total_loss / (len(data_source) - 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time:  0.43s | valid loss 10.19 | valid ppl 26557.67\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time:  0.16s | valid loss  9.88 | valid ppl 19441.85\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time:  0.17s | valid loss 11.90 | valid ppl 146983.00\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "best_val_loss = float(\"inf\")\n",
        "epochs = 3 # The number of epochs\n",
        "best_model = None\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train()\n",
        "    val_loss = evaluate(model, val_data)\n",
        "    print('-' * 89)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                     val_loss, math.exp(val_loss)))\n",
        "    print('-' * 89)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_model = model\n",
        "\n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=========================================================================================\n",
            "| End of training | test loss 11.18 | test ppl 71820.47\n",
            "=========================================================================================\n"
          ]
        }
      ],
      "source": [
        "test_loss = evaluate(best_model, test_data)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
        "    test_loss, math.exp(test_loss)))\n",
        "print('=' * 89)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cn5LPjkHAO_f",
        "outputId": "487a618f-ebc1-476c-bed4-b2f09571b922"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Global forcing from waste heat was 0.028 W/m2 in 2005.\n",
            "Thus, the waste heat engine may be one of the least expensive components of a complete waste heat recovery system.\n",
            "Only a tiny fraction of the original chemical energy is used for work:\n",
            "Land ice sheets in both Antarctica and Greenland have been losing mass since 2002 and have seen an acceleration of ice mass loss since 2009.\n"
          ]
        }
      ],
      "source": [
        "#print(train.head())\n",
        "#print(test_with_evi.head())\n",
        "# claim: contribution waste heat global climate\n",
        "print(evidence['evidence-308923'])\n",
        "print(evidence['evidence-213569'])\n",
        "\n",
        "#greenland lost tiny fraction ice mass\t\n",
        "print(evidence['evidence-962481'])\n",
        "print(evidence['evidence-1200633'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "pvjyFX7dD6ij"
      },
      "outputs": [],
      "source": [
        "from torch import nn, optim\n",
        "\n",
        "# define hypermeter\n",
        "sequence_len = 28\n",
        "input_len = 28\n",
        "hidden_size = 128\n",
        "num_layers = 2\n",
        "num_classes = 4\n",
        "num_epchos = 5\n",
        "learning_rate = 0.01\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "7nG03eBPFZ_K"
      },
      "outputs": [],
      "source": [
        "class LSTM(nn.Module):\n",
        "  def __init__(self, input_length, hidden_size, num_classes, num_layers):\n",
        "    super(LSTM, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "    self.lstm = nn.LSTM(input_len, hidden_size, num_layers, num_classes, batch_first=True)\n",
        "    self.output_layer = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "  def forward(self, X):\n",
        "    hidden_states = torch.zeros(self.num_layers, X.size(0), self.hidden_size)\n",
        "    cell_states = torch.zeros(self.num_layers, X.size(0), self.hidden_size)\n",
        "    out, _ = self.lstm(X, (hidden_states, cell_states))\n",
        "    out = self.output_layer(out[:, -1, :])\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_WMcOkxH9Sj",
        "outputId": "0316dddf-f21c-465d-a772-8b42cf9d365f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LSTM(\n",
            "  (lstm): LSTM(28, 128, num_layers=2, bias=4, batch_first=True)\n",
            "  (output_layer): Linear(in_features=128, out_features=4, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = LSTM(input_len, hidden_size, num_classes, num_layers)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "Oau1OuglI7Hh"
      },
      "outputs": [],
      "source": [
        "loss_func = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DFZXfcGJaPY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzGuzHPE87Ya"
      },
      "source": [
        "# 3.Testing and Evaluation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZVeNYIH9IaL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mefSOe8eTmGP"
      },
      "source": [
        "## Object Oriented Programming codes here\n",
        "\n",
        "*You can use multiple code snippets. Just add more if needed*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# model adpated from workshop 8\n",
        "class TransformerModel(nn.Module):\n",
        "\n",
        "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "        self.model_type = 'Transformer'\n",
        "        self.src_mask = None\n",
        "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
        "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.encoder = nn.Embedding(ntoken, ninp)\n",
        "        self.ninp = ninp\n",
        "        self.decoder = nn.Linear(ninp, ntoken)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def _generate_square_subsequent_mask(self, sz):\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src):\n",
        "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
        "            device = src.device\n",
        "            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
        "            self.src_mask = mask\n",
        "\n",
        "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
        "        src = self.pos_encoder(src)\n",
        "        output = self.transformer_encoder(src, self.src_mask)\n",
        "        output = self.decoder(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term) #0::2 means starting with index 0, step = 2\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
