{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32yCsRUo8H33"
      },
      "source": [
        "# 2024 COMP90042 Project\n",
        "*Make sure you change the file name with your group id.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCybYoGz8YWQ"
      },
      "source": [
        "# Readme\n",
        "*If there is something to be noted for the marker, please mention here.*\n",
        "\n",
        "*If you are planning to implement a program with Object Oriented Programming style, please put those the bottom of this ipynb file*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6po98qVA8bJD"
      },
      "source": [
        "# 1.DataSet Processing\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "hjGn8oHh9lew"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /Users/kaiyuancui/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "#### word embedding pipeline\n",
        "\n",
        "from gensim.utils import deaccent\n",
        "from nltk import pos_tag\n",
        "from nltk import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "import re\n",
        "import time\n",
        "\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "import pickle\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "#\n",
        "LOAD_FILES = True\n",
        "\n",
        "#d_evidence = pd.read_json(\"data/evidence.json\", typ='series')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# contraction_dict from WS7\n",
        "contraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\",\n",
        "                    \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\",\n",
        "                    \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\",\n",
        "                    \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\",\n",
        "                    \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\",\n",
        "                    \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
        "                    \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\",\n",
        "                    \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\n",
        "                    \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
        "                    \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\",\n",
        "                    \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\",\n",
        "                    \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\n",
        "                    \"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
        "                    \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
        "                    \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\",\n",
        "                    \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\",\n",
        "                    \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\",\n",
        "                    \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\",\n",
        "                    \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\",\n",
        "                    \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\",\n",
        "                    \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "                    \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
        "\n",
        "\n",
        "# https://stackoverflow.com/a/46231553\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return None # for easy if-statement \n",
        "\n",
        "\n",
        "def sentence_preprocessing(sentence):\n",
        "\n",
        "    out_list = []\n",
        "    # Use gensim deaccent to match more characters to [a-z]\n",
        "    sentence = deaccent(sentence.lower())\n",
        "\n",
        "    for old, new in contraction_dict.items():\n",
        "        sentence.replace(old, new)\n",
        "\n",
        "    tokenized = word_tokenize(sentence)\n",
        "\n",
        "    # now remove all tokens that don't contain any alphanumeric characters\n",
        "    # then strip non alphanumeric characters afterwards\n",
        "    tokenized = [re.sub(r\"[^a-z0-9\\s]\", \"\", token) for token in tokenized if re.match(r\"[a-z0-9\\s]\", token)]\n",
        "\n",
        "    # now lemmatize with pos\n",
        "    tagged = pos_tag(tokenized)\n",
        "    for token, tag in tagged:\n",
        "        wntag = get_wordnet_pos(tag)\n",
        "\n",
        "        if wntag is None: # do not supply tag in case of None\n",
        "            lemma = lemmatizer.lemmatize(token) \n",
        "        else:\n",
        "            lemma = lemmatizer.lemmatize(token, pos=wntag) \n",
        "\n",
        "        out_list.append(lemma)\n",
        "    \n",
        "    return out_list\n",
        "\n",
        "\n",
        "def evidence_preprocessing(evidences):\n",
        "  t = time.time()\n",
        "  processed = []\n",
        "  for index, item in enumerate(evidences.items()):\n",
        "    id, evidence = item\n",
        "\n",
        "    row = []\n",
        "    \n",
        "    row.append(id)\n",
        "    row.append(evidence)\n",
        "\n",
        "    # break the text into sentences before tokenizing by each sentence\n",
        "    processed_sentences = [sentence_preprocessing(sentence) for sentence in sent_tokenize(evidence)]\n",
        "    row.append(processed_sentences)\n",
        "\n",
        "\n",
        "    # Appending an empty list to populate with embeddings later\n",
        "    row.append([])\n",
        "\n",
        "    processed.append(row)\n",
        "\n",
        "    if (index + 1) % 50000 == 0:\n",
        "        print(f\"{time.time() - t:.2f} - {index+1} rows processed\")\n",
        "\n",
        "  return pd.DataFrame(processed, columns = [\"id\", \"raw evidence\", \"processed evidence\", \"embeddings\"])\n",
        "\n",
        "\n",
        "# Evidence processing\n",
        "if not LOAD_FILES:\n",
        "    evidence = evidence_preprocessing(d_evidence)\n",
        "    with open(\"../pipeline/evidence_preprocessed_v3.pkl\", \"wb\") as f:\n",
        "        pickle.dump(evidence, f)\n",
        "else:\n",
        "    with open(\"../pipeline/evidence_preprocessed_v3.pkl\", \"rb\") as f:\n",
        "        evidence = pickle.load(f)\n",
        "    \n",
        "    evidence.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /Users/kaiyuancui/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "from gensim.utils import deaccent\n",
        "from nltk import pos_tag\n",
        "from nltk import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "import re\n",
        "import time\n",
        "\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "import pickle\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "#d_evidence = pd.read_json(\"data/evidence.json\", typ='series')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# contraction_dict from WS7\n",
        "contraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\",\n",
        "                    \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\",\n",
        "                    \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\",\n",
        "                    \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\",\n",
        "                    \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\",\n",
        "                    \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
        "                    \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\",\n",
        "                    \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\n",
        "                    \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
        "                    \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\",\n",
        "                    \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\",\n",
        "                    \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\n",
        "                    \"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
        "                    \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
        "                    \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\",\n",
        "                    \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\",\n",
        "                    \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\",\n",
        "                    \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\",\n",
        "                    \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\",\n",
        "                    \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\",\n",
        "                    \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "                    \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# https://stackoverflow.com/a/46231553\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return None # for easy if-statement \n",
        "    \n",
        "def sentence_preprocessing(sentence):\n",
        "\n",
        "    out_list = []\n",
        "    # Use gensim deaccent to match more characters to [a-z]\n",
        "    sentence = deaccent(sentence.lower())\n",
        "\n",
        "    for old, new in contraction_dict.items():\n",
        "        sentence.replace(old, new)\n",
        "\n",
        "    tokenized = word_tokenize(sentence)\n",
        "\n",
        "    # now remove all tokens that don't contain any alphanumeric characters\n",
        "    # then strip non alphanumeric characters afterwards\n",
        "    tokenized = [re.sub(r\"[^a-z0-9\\s]\", \"\", token) for token in tokenized if re.match(r\"[a-z0-9\\s]\", token)]\n",
        "\n",
        "    # now lemmatize with pos\n",
        "    tagged = pos_tag(tokenized)\n",
        "    for token, tag in tagged:\n",
        "        wntag = get_wordnet_pos(tag)\n",
        "\n",
        "        if wntag is None: # do not supply tag in case of None\n",
        "            lemma = lemmatizer.lemmatize(token) \n",
        "        else:\n",
        "            lemma = lemmatizer.lemmatize(token, pos=wntag) \n",
        "\n",
        "        out_list.append(lemma)\n",
        "    \n",
        "    return out_list\n",
        "\n",
        "\n",
        "# https://huggingface.co/learn/nlp-course/chapter6/6\n",
        "def encode_word(word):\n",
        "    tokens = []\n",
        "    while len(word) > 0:\n",
        "        i = len(word)\n",
        "        while i > 0 and word[:i] not in vocab:\n",
        "            i -= 1\n",
        "        if i == 0:\n",
        "            return [\"[UNK]\"]\n",
        "        tokens.append(word[:i])\n",
        "        word = word[i:]\n",
        "        if len(word) > 0:\n",
        "            word = f\"##{word}\"\n",
        "    return tokens\n",
        "\n",
        "# adapted from https://huggingface.co/learn/nlp-course/chapter6/6\n",
        "def tokenize(sentence):\n",
        "\n",
        "    # janky workaround for preprocessed sentences\n",
        "    if type(sentence) is not list:\n",
        "        sentence = sentence_preprocessing(sentence)\n",
        "        \n",
        "    encoded_words = [encode_word(word) for word in sentence]\n",
        "    return sum(encoded_words, [])\n",
        "\n",
        "\n",
        "\n",
        "with open(\"../pipeline/BPETokenizer_merge_rules_v1.5.pkl\", \"rb\") as f:\n",
        "    merge_rules = pickle.load(f)\n",
        "    \n",
        "\n",
        "# Reconstruct vocab from merge rules due to lack of foresight\n",
        "# This grabs all vocab of length 2 or above (if contains first letter)\n",
        "# or 4 or above (##__)\n",
        "vocab = [v for v in merge_rules.values()]\n",
        "\n",
        "# So iterate through merge rules again to find starting letters\n",
        "# and one letter suffixes\n",
        "for pair, merge in merge_rules.items():\n",
        "    if len(pair[0]) == 1 and pair[0] not in vocab:\n",
        "        vocab.append(pair[0])\n",
        "    if len(pair[1]) == 3 and pair[1] not in vocab:\n",
        "        vocab.append(pair[1])\n",
        "\n",
        "\n",
        "def processed_evidence_to_bpe(paragraph):\n",
        "    # 2d array -> paragraph\n",
        "    if type(paragraph[0]) is list:\n",
        "        return [tokenize(sentence) for sentence in paragraph]\n",
        "\n",
        "    # 1 sentence -> tokenize as is \n",
        "    else:\n",
        "        return tokenize(paragraph)\n",
        "\n",
        "\n",
        "counter = 0\n",
        "def processed_evidence_to_bpe(paragraph):\n",
        "    global counter\n",
        "    counter += 1\n",
        "    if counter % 1000 == 0:\n",
        "        print(f\"{counter} rows processed\")\n",
        "    #2d array -> paragraph\n",
        "    if type(paragraph[0]) is list:\n",
        "        return [tokenize(sentence) for sentence in paragraph]\n",
        "\n",
        "    # 1 sentence -> tokenize as is \n",
        "    else:\n",
        "        return tokenize(paragraph)\n",
        "\n",
        "\n",
        "# Save\n",
        "\n",
        "\"\"\"\n",
        "e[\"bpe evidence\"] = e[\"processed evidence\"].apply(processed_evidence_to_bpe)\n",
        "with open(\"BPETokenized_evidence_v3.pkl\", \"wb\") as f:\n",
        "    pickle.dump(e, f)\n",
        "\"\"\"\n",
        "\n",
        "# Load\n",
        "with open(\"../pipeline/BPETokenized_evidence_v3.pkl\", \"rb\") as f:\n",
        "    evidence = pickle.load(f)\n",
        "\n",
        "\"\"\"\n",
        "sentences = []\n",
        "\n",
        "for paragraph in evidence[\"bpe evidence\"]:\n",
        "    if type(paragraph[0]) is list:\n",
        "        for sentence in paragraph:\n",
        "            sentences.append(sentence)\n",
        "    else:\n",
        "        sentences.append(paragraph)\n",
        "\"\"\"\n",
        "\n",
        "# Now do word2vec\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "\n",
        "EMBEDDING_DIM = 200\n",
        "\"\"\"\n",
        "embedding_model = Word2Vec(sentences=sentences,\n",
        "                           vector_size=EMBEDDING_DIM,\n",
        "                           window=4,\n",
        "                           min_count=3,\n",
        "                           workers=10,\n",
        "                           negative=5\n",
        "                           )\n",
        "\n",
        "version = 3\n",
        "with open(f\"BPE Tokenizer to embedding/embeddings_BPE_v{version}.pkl\", \"wb\") as f:\n",
        "    pickle.dump(embedding_model, f)\n",
        "\"\"\"\n",
        "\n",
        "# Load embedding\n",
        "with open(\"../pipeline/embeddings_BPE_v3.pkl\", \"rb\") as f:\n",
        "    embedding_model = pickle.load(f)\n",
        "\n",
        "import numpy as np\n",
        "def sentence_embedding(sentence):\n",
        "\n",
        "  # Failsafe\n",
        "  if len(sentence) == 0:\n",
        "    return np.zeros(EMBEDDING_DIM)\n",
        "\n",
        "  if type(sentence[0]) is not list:\n",
        "      sentence = tokenize(sentence)\n",
        "\n",
        "\n",
        "  embedding = np.zeros(EMBEDDING_DIM)\n",
        "  for word in sentence:\n",
        "    word_embedding = np.zeros(EMBEDDING_DIM)\n",
        "\n",
        "    # get word vector for given word\n",
        "    # if not found, ignore (treat as having the zero vector)\n",
        "    try:\n",
        "      word_embedding = embedding_model.wv[str(word)]\n",
        "    except KeyError:\n",
        "      pass\n",
        "\n",
        "    embedding += word_embedding\n",
        "\n",
        "  return embedding / len(sentence)\n",
        "\n",
        "\n",
        "def paragraph_embedding(paragraph):\n",
        "    out = []\n",
        "\n",
        "    # One sentence\n",
        "    if type(paragraph[0]) is not list:\n",
        "        return [sentence_embedding(paragraph)]\n",
        "\n",
        "    else:\n",
        "        for sentence in paragraph:\n",
        "            out.append(sentence_embedding(sentence))\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Baseline retrieval: immediately use the raw embeddings to retrieve closest sentences\n",
        "# Train a cutoff distance threshold.\n",
        "\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "# Similarity based on cosine similarity ([0-1], higher the more similar)\n",
        "def similarity(text, evidence_ids):\n",
        "\n",
        "    # Seems stupid and retrieving everything from w2v is probably cleaner\n",
        "    # TODO: make this better\n",
        "    evidence_embeddings = [evidence.loc[evidence['id'] == id, 'embeddings'].values[0] for id in evidence_ids]\n",
        "    key_embedding = sentence_embedding(text)\n",
        "    \n",
        "    similarities = []\n",
        "    for evidence_embedding in evidence_embeddings:\n",
        "        similarities.append(1-cosine(key_embedding, evidence_embedding))\n",
        "\n",
        "    return similarities\n",
        "\n",
        "\n",
        "# Using 1 - fscore as the loss\n",
        "def retrieval_loss(prediction, target):\n",
        "    numerator = 0\n",
        "    denominator = 0\n",
        "    \n",
        "    for p in prediction:\n",
        "        if p in target:\n",
        "            denominator += 2\n",
        "            numerator += 2\n",
        "        else:\n",
        "            denominator += 1\n",
        "    \n",
        "    for t in target:\n",
        "        if t not in prediction:\n",
        "            denominator += 1\n",
        "    \n",
        "    return 1 - numerator/denominator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sentence_embedding(sentence):\n",
        "\n",
        "  # Failsafe\n",
        "  if len(sentence) == 0:\n",
        "    return np.zeros(EMBEDDING_DIM)\n",
        "\n",
        "  if type(sentence[0]) is not list:\n",
        "      sentence = tokenize(sentence)\n",
        "\n",
        "\n",
        "  embedding = np.zeros(EMBEDDING_DIM)\n",
        "  for word in sentence:\n",
        "    word_embedding = np.zeros(EMBEDDING_DIM)\n",
        "\n",
        "    # get word vector for given word\n",
        "    # if not found, ignore (treat as having the zero vector)\n",
        "    try:\n",
        "      word_embedding = embedding_model.wv[str(word)]\n",
        "    except KeyError:\n",
        "      pass\n",
        "\n",
        "    embedding += word_embedding\n",
        "\n",
        "  return embedding / len(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvff21Hv8zjk",
        "outputId": "3856e2ef-c764-4d97-a681-9a54b02c04f2"
      },
      "outputs": [],
      "source": [
        "#TODO: combined preprocessing of embedding part with this:\n",
        "\n",
        "LOCAL_DEV = True # to switch between developing locally and on colab\n",
        "\n",
        "if not LOCAL_DEV:\n",
        "    # TODO: need to upload data files on Google Drive?\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "MH1Hdrb5NmHM"
      },
      "outputs": [],
      "source": [
        "#Imports\n",
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "2piZvV4OMSa3",
        "outputId": "69009174-8de4-4d4f-e56e-be8b5a39e1aa"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>claim_text</th>\n",
              "      <th>claim_label</th>\n",
              "      <th>evidences</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>claim-1937</th>\n",
              "      <td>Not only is there no scientific evidence that ...</td>\n",
              "      <td>DISPUTED</td>\n",
              "      <td>[evidence-442946, evidence-1194317, evidence-1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-126</th>\n",
              "      <td>El Niño drove record highs in global temperatu...</td>\n",
              "      <td>REFUTES</td>\n",
              "      <td>[evidence-338219, evidence-1127398]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-2510</th>\n",
              "      <td>In 1946, PDO switched to a cool phase.</td>\n",
              "      <td>SUPPORTS</td>\n",
              "      <td>[evidence-530063, evidence-984887]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-2021</th>\n",
              "      <td>Weather Channel co-founder John Coleman provid...</td>\n",
              "      <td>DISPUTED</td>\n",
              "      <td>[evidence-1177431, evidence-782448, evidence-5...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-2449</th>\n",
              "      <td>\"January 2008 capped a 12 month period of glob...</td>\n",
              "      <td>NOT_ENOUGH_INFO</td>\n",
              "      <td>[evidence-1010750, evidence-91661, evidence-72...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   claim_text  \\\n",
              "claim-1937  Not only is there no scientific evidence that ...   \n",
              "claim-126   El Niño drove record highs in global temperatu...   \n",
              "claim-2510             In 1946, PDO switched to a cool phase.   \n",
              "claim-2021  Weather Channel co-founder John Coleman provid...   \n",
              "claim-2449  \"January 2008 capped a 12 month period of glob...   \n",
              "\n",
              "                claim_label                                          evidences  \n",
              "claim-1937         DISPUTED  [evidence-442946, evidence-1194317, evidence-1...  \n",
              "claim-126           REFUTES                [evidence-338219, evidence-1127398]  \n",
              "claim-2510         SUPPORTS                 [evidence-530063, evidence-984887]  \n",
              "claim-2021         DISPUTED  [evidence-1177431, evidence-782448, evidence-5...  \n",
              "claim-2449  NOT_ENOUGH_INFO  [evidence-1010750, evidence-91661, evidence-72...  "
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#visualising training data\n",
        "if LOCAL_DEV:\n",
        "    train = pd.read_json(\"../data/train-claims.json\") # for local dev\n",
        "    dev = pd.read_json(\"../data/train-claims.json\")\n",
        "    \n",
        "else:\n",
        "    train = pd.read_json(\"/content/drive/MyDrive/data/train-claims.json\") # on colab\n",
        "\n",
        "\n",
        "train = train.transpose()\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ZKbGFcA2THHP"
      },
      "outputs": [],
      "source": [
        "#visualising evidence data\n",
        "#visualising evidence data\n",
        "if LOCAL_DEV:\n",
        "    evidence = pd.read_json(\"../data/evidence.json\",typ='series')\n",
        "else:\n",
        "    evidence = pd.read_json(\"/content/drive/MyDrive/data/evidence.json\",typ='series')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFNTiC0UMS45",
        "outputId": "a6bd81bb-dbc0-4eb3-f080-16f6606b9e13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1208827\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "evidence-0    John Bennet Lawes, English entrepreneur and ag...\n",
              "evidence-1    Lindberg began his professional career at the ...\n",
              "evidence-2    ``Boston (Ladies of Cambridge)'' by Vampire We...\n",
              "evidence-3    Gerald Francis Goyer (born October 20, 1936) w...\n",
              "evidence-4    He detected abnormalities of oxytocinergic fun...\n",
              "dtype: object"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(len(evidence))\n",
        "evidence.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "-HQn6h2hYukS",
        "outputId": "edc2dff9-4fcd-4e88-d44d-21f6e35cbbeb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>claim_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>claim-2967</th>\n",
              "      <td>The contribution of waste heat to the global c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-979</th>\n",
              "      <td>“Warm weather worsened the most recent five-ye...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-1609</th>\n",
              "      <td>Greenland has only lost a tiny fraction of its...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-1020</th>\n",
              "      <td>“The global reef crisis does not necessarily m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-2599</th>\n",
              "      <td>Small amounts of very active substances can ca...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   claim_text\n",
              "claim-2967  The contribution of waste heat to the global c...\n",
              "claim-979   “Warm weather worsened the most recent five-ye...\n",
              "claim-1609  Greenland has only lost a tiny fraction of its...\n",
              "claim-1020  “The global reef crisis does not necessarily m...\n",
              "claim-2599  Small amounts of very active substances can ca..."
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "if LOCAL_DEV:\n",
        "    test = pd.read_json(\"../data/test-claims-unlabelled.json\")\n",
        "else:\n",
        "    test = pd.read_json(\"/content/drive/MyDrive/data/test-claims-unlabelled.json\")\n",
        "test = test.transpose()\n",
        "test.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Xf73PDzRTrft"
      },
      "outputs": [],
      "source": [
        "#preprocessing\n",
        "# punctuations should be removed, common words such as the, is, are, should be removed. all words also should be lemmentised and stemmed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qL-bItuvn19d",
        "outputId": "01e87758-b752-447a-bbd0-e15ea30fb27f",
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras in /Users/kaiyuancui/opt/anaconda3/lib/python3.9/site-packages (2.15.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHWbo7W7TuUC",
        "outputId": "5cd4f2b1-ab01-424a-d499-cdd5f2945056"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/kaiyuancui/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/kaiyuancui/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /Users/kaiyuancui/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import string\n",
        "import contractions\n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXFRgZCnRO1V",
        "outputId": "a74cee0a-709c-4014-8056-aac9d978b6e7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "claim-1937    not scientific evidence pollutant higher conce...\n",
              "claim-126     el niño drove record high global temperature s...\n",
              "claim-2510                              pdo switched cool phase\n",
              "claim-2021    weather channel john coleman provided evidence...\n",
              "claim-2449    january capped month period global temperature...\n",
              "dtype: object"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "\n",
        "def preprocess_data(data: pd.Series, limit=10000) -> pd.Series:\n",
        "  preprocessed_data = {}\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  stop_words.remove('not')\n",
        "  count = 0\n",
        "  for id, text in data.items():\n",
        "    text = text.lower()\n",
        "    text = contractions.fix(text)\n",
        "    tokens = word_tokenize(text)\n",
        "    wnl = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [wnl.lemmatize(word) for word in tokens if word.isalpha() and word not in stop_words]\n",
        "    preprocessed_data[id] = \" \".join(lemmatized_tokens)\n",
        "    count += 1\n",
        "    if count >= limit:\n",
        "      break\n",
        "\n",
        "  return pd.Series(preprocessed_data)\n",
        "\n",
        "processed_evidence = preprocess_data(evidence)\n",
        "\n",
        "test_claims = test['claim_text']\n",
        "train_claims = train['claim_text']\n",
        "processed_test = preprocess_data(test_claims)\n",
        "processed_test.head()\n",
        "processed_train = preprocess_data(train_claims)\n",
        "processed_train.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "x_5xtX0qzryS"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "evidence-0    john bennet lawes english entrepreneur agricul...\n",
              "evidence-1    lindberg began professional career age eventua...\n",
              "evidence-2                boston lady cambridge vampire weekend\n",
              "evidence-3    gerald francis goyer born october professional...\n",
              "evidence-4    detected abnormality oxytocinergic function sc...\n",
              "dtype: object"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "processed_evidence = processed_evidence[processed_evidence.str.strip().str.len() > 0]\n",
        "processed_evidence.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnNiS5byTvJM"
      },
      "source": [
        "# Two steps for the this task\n",
        "# first. find all relavent evidence, either use contextual embedding or similarity scoring\n",
        "# second. classify the evidents into 4 classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FA2ao2l8hOg"
      },
      "source": [
        "# 2. Model Implementation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "MXbiFXUfvngL"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from keras.layers import Dense\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Embedding, Flatten, Dense\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "# from tensorflow.keras.models import Sequential\n",
        "# from tensorflow.keras.layers import Embedding, Flatten, Dense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "QIEqDDT78q39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['aa' 'aaa' 'aabis' ... '楊璟翊' '盧翰' '민주국민당']\n",
            "  (0, 24182)\t0.5894022428982739\n",
            "  (0, 9912)\t0.43153385604356104\n",
            "  (0, 9096)\t0.3217964341887153\n",
            "  (0, 4988)\t0.5097918621830975\n",
            "  (0, 4330)\t0.32084706535977053\n",
            "evidence-0       john bennet lawes english entrepreneur agricul...\n",
            "evidence-1       lindberg began professional career age eventua...\n",
            "evidence-2                   boston lady cambridge vampire weekend\n",
            "evidence-3       gerald francis goyer born october professional...\n",
            "evidence-4       detected abnormality oxytocinergic function sc...\n",
            "                                       ...                        \n",
            "evidence-9995    rising step load testing rsl testing testing s...\n",
            "evidence-9996               one single would big league hit season\n",
            "evidence-9997    stream tributary include panther creek plank b...\n",
            "evidence-9998    kerry glen simon june september american celeb...\n",
            "evidence-9999    survivor retreated across river joined spaniar...\n",
            "Length: 9994, dtype: object\n"
          ]
        }
      ],
      "source": [
        "#Vectorizing preprocessed text\n",
        "vectorizer = TfidfVectorizer()\n",
        "all_texts = pd.concat([processed_evidence, processed_train])\n",
        "vectorizer.fit(all_texts)\n",
        "evidence_tfidf = vectorizer.transform(processed_evidence)\n",
        "test_tfidf = vectorizer.transform(processed_test)\n",
        "train_tfidf = vectorizer.transform(processed_train)\n",
        "\n",
        "print(vectorizer.get_feature_names_out()) # why does this contain non-alphabetic?\n",
        "print(test_tfidf[0])\n",
        "#print(test_tfidf.shape)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "john bennet lawes english entrepreneur agricultural scientist\n",
            "[ 9.19078158e-02  6.90522912e-01  2.78793582e-01 -1.88396685e-02\n",
            " -2.28649519e-01 -4.43825321e-01 -2.57578073e-02 -7.96965988e-02\n",
            "  1.56434759e-01 -4.25711121e-01 -4.77746917e-01 -5.09624938e-01\n",
            " -1.12946350e-01 -5.53536547e-01  4.47556537e-01 -2.55390281e-01\n",
            " -8.45387330e-02 -2.80425862e-01  2.29172011e-01 -1.44044688e-01\n",
            "  1.98405743e-01  2.12396421e-01  3.82673217e-01  2.49713839e-01\n",
            " -4.64526597e-01  8.77822631e-01  4.16555944e-01  4.88641779e-01\n",
            "  7.62103965e-01 -6.33041772e-02  5.51566457e-02 -2.50505329e-01\n",
            " -5.33437444e-01 -2.24365794e-01 -6.79122785e-02  3.87498591e-01\n",
            "  1.91612665e-01 -3.02849983e-01  1.51370910e-01  5.73424382e-01\n",
            " -1.48332053e-01 -1.97941481e-01  7.50132249e-02  5.45069735e-01\n",
            "  3.11545775e-01 -3.15839024e-01 -1.87085857e-01 -8.14146393e-01\n",
            "  9.50082157e-01 -3.44185258e-01 -1.33396986e-01 -9.43473491e-01\n",
            " -6.64342791e-02 -1.11255514e-01  5.68910679e-01  4.18735663e-01\n",
            " -4.06766784e-01 -8.01613104e-01 -1.84273162e-01  1.84680995e-01\n",
            " -5.31504476e-01  2.02924726e-01  4.49059531e-02  2.23657573e-01\n",
            " -1.70834912e-01  3.40137599e-01  1.33019458e-01  6.21088318e-01\n",
            " -3.53148012e-01  8.18296346e-01  3.60952962e-01 -1.10272825e-01\n",
            "  7.04990995e-01 -4.84456777e-01  8.74053538e-02  4.72602237e-01\n",
            " -1.56976980e-01 -3.82620558e-01 -3.42737055e-01  1.33955272e-01\n",
            "  3.29134395e-01 -3.06517548e-03  2.87641138e-01 -5.25257604e-01\n",
            "  2.25251677e-01 -3.67227517e-01 -5.32765154e-01 -4.43593750e-01\n",
            "  2.78641627e-01  5.04054899e-01 -2.48731847e-02 -3.72609126e-01\n",
            "  9.52989133e-01 -9.62972658e-02 -4.30661528e-01 -1.57503536e+00\n",
            "  1.90260964e-01  1.86842004e-01 -3.16250371e-01 -8.75501464e-01\n",
            " -6.08768960e-02  7.15842297e-01 -1.49108489e-01  1.07539826e-01\n",
            "  3.54361180e-01 -5.48919824e-01  6.82221585e-02 -2.44426245e-01\n",
            "  7.18091176e-01 -5.82387088e-01 -6.29837050e-01  3.26866888e-02\n",
            " -1.28310836e-01  1.84650133e-02  2.00837329e-01 -8.49254355e-02\n",
            " -1.10766434e-01  3.82196438e-01  5.26335975e-01 -1.26303492e-01\n",
            " -8.58591174e-01  2.10093763e-01 -5.45184364e-02  5.18236690e-02\n",
            "  5.89135332e-01  1.60812057e-02 -3.32299258e-01  6.39512694e-01\n",
            "  4.24780326e-01 -1.45777302e-01  2.41114547e-01  9.35602784e-02\n",
            " -6.96427292e-04  3.77941937e-01 -5.26721112e-01  2.88689865e-02\n",
            " -4.98519517e-01 -6.07438887e-01 -1.86883858e-01 -5.15418527e-01\n",
            "  1.11484259e-01  2.40547430e-01 -2.75382499e-02 -7.18231967e-02\n",
            " -9.38533247e-03 -4.78613857e-02 -8.43767111e-01  6.85936739e-01\n",
            "  5.31092584e-02 -1.64897715e-01  8.23363231e-02  1.31988039e-01\n",
            "  4.68021824e-01 -9.94961146e-02  4.83377655e-02  5.14093775e-01\n",
            " -3.82322820e-01  6.04983386e-02 -1.02844265e-01  4.44772005e-01\n",
            "  4.62718325e-02  2.73048524e-01  1.80417299e-03 -3.25348259e-01\n",
            "  6.13319770e-01  5.09365896e-02  4.86133563e-01  3.85925569e-01\n",
            " -2.17803405e-01  3.27305930e-01 -1.22211018e-01  5.57852813e-01\n",
            " -9.86894237e-02 -5.31671508e-02  7.87138066e-01  3.62079329e-01\n",
            " -3.50019038e-02  2.05816522e-01 -4.08466489e-01 -2.62436868e-01\n",
            " -1.95253135e-01  7.57942226e-01 -3.64813864e-01  3.56644450e-01\n",
            " -1.03077836e-01 -5.63311704e-01 -2.73095134e-01  1.69426563e-01\n",
            "  6.39718341e-01 -6.35603882e-02 -4.39264162e-02  2.46287516e-01\n",
            " -2.32933263e-01  3.03533366e-01 -8.89632271e-03 -7.41224470e-02\n",
            " -2.12967703e-01  1.62355108e-01  1.52328786e-01  5.25768808e-01]\n"
          ]
        }
      ],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>claim_text</th>\n",
              "      <th>evidences</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>claim-2967</th>\n",
              "      <td>contribution waste heat global climate</td>\n",
              "      <td>[evidence-8950, evidence-4903, evidence-1294, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-979</th>\n",
              "      <td>warm weather worsened recent drought included ...</td>\n",
              "      <td>[evidence-2760, evidence-8828, evidence-5911, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-1609</th>\n",
              "      <td>greenland lost tiny fraction ice mass</td>\n",
              "      <td>[evidence-5928, evidence-4202, evidence-3680, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-1020</th>\n",
              "      <td>global reef crisis not necessarily mean extinc...</td>\n",
              "      <td>[evidence-3210, evidence-8721, evidence-2739, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-2599</th>\n",
              "      <td>small amount active substance cause large effect</td>\n",
              "      <td>[evidence-8207, evidence-8000, evidence-320, e...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   claim_text  \\\n",
              "claim-2967             contribution waste heat global climate   \n",
              "claim-979   warm weather worsened recent drought included ...   \n",
              "claim-1609              greenland lost tiny fraction ice mass   \n",
              "claim-1020  global reef crisis not necessarily mean extinc...   \n",
              "claim-2599   small amount active substance cause large effect   \n",
              "\n",
              "                                                    evidences  \n",
              "claim-2967  [evidence-8950, evidence-4903, evidence-1294, ...  \n",
              "claim-979   [evidence-2760, evidence-8828, evidence-5911, ...  \n",
              "claim-1609  [evidence-5928, evidence-4202, evidence-3680, ...  \n",
              "claim-1020  [evidence-3210, evidence-8721, evidence-2739, ...  \n",
              "claim-2599  [evidence-8207, evidence-8000, evidence-320, e...  "
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "similarity_matrix = cosine_similarity(test_tfidf, evidence_tfidf)\n",
        "\n",
        "def getTopN(similarity_matrix, test, evidence, n):\n",
        "  test = test.to_frame(name='claim_text')\n",
        "  top_indices = np.argsort(-similarity_matrix, axis = 1)[:, :n]\n",
        "  top_evidence = [[str(evidence.index[i]) for i in row] for row in top_indices]\n",
        "  test['evidences'] = top_evidence\n",
        "  return test\n",
        "\n",
        "test_with_evi = getTopN(similarity_matrix, processed_test, processed_evidence, 5)\n",
        "test_with_evi.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torchtext==0.4.0\n",
            "  Downloading torchtext-0.4.0-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /Users/kaiyuancui/opt/anaconda3/lib/python3.9/site-packages (from torchtext==0.4.0) (1.16.0)\n",
            "Requirement already satisfied: numpy in /Users/kaiyuancui/opt/anaconda3/lib/python3.9/site-packages (from torchtext==0.4.0) (1.24.4)\n",
            "Requirement already satisfied: torch in /Users/kaiyuancui/opt/anaconda3/lib/python3.9/site-packages (from torchtext==0.4.0) (2.2.1)\n",
            "Requirement already satisfied: requests in /Users/kaiyuancui/opt/anaconda3/lib/python3.9/site-packages (from torchtext==0.4.0) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /Users/kaiyuancui/opt/anaconda3/lib/python3.9/site-packages (from torchtext==0.4.0) (4.64.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/kaiyuancui/opt/anaconda3/lib/python3.9/site-packages (from requests->torchtext==0.4.0) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/kaiyuancui/opt/anaconda3/lib/python3.9/site-packages (from requests->torchtext==0.4.0) (3.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/kaiyuancui/opt/anaconda3/lib/python3.9/site-packages (from requests->torchtext==0.4.0) (2022.9.24)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kaiyuancui/opt/anaconda3/lib/python3.9/site-packages (from requests->torchtext==0.4.0) (1.26.11)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/kaiyuancui/opt/anaconda3/lib/python3.9/site-packages (from torch->torchtext==0.4.0) (4.10.0)\n",
            "Requirement already satisfied: filelock in /Users/kaiyuancui/opt/anaconda3/lib/python3.9/site-packages (from torch->torchtext==0.4.0) (3.6.0)\n",
            "Requirement already satisfied: networkx in /Users/kaiyuancui/opt/anaconda3/lib/python3.9/site-packages (from torch->torchtext==0.4.0) (2.8.4)\n",
            "Requirement already satisfied: sympy in /Users/kaiyuancui/opt/anaconda3/lib/python3.9/site-packages (from torch->torchtext==0.4.0) (1.10.1)\n",
            "Requirement already satisfied: jinja2 in /Users/kaiyuancui/opt/anaconda3/lib/python3.9/site-packages (from torch->torchtext==0.4.0) (2.11.3)\n",
            "Requirement already satisfied: fsspec in /Users/kaiyuancui/opt/anaconda3/lib/python3.9/site-packages (from torch->torchtext==0.4.0) (2022.7.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /Users/kaiyuancui/opt/anaconda3/lib/python3.9/site-packages (from jinja2->torch->torchtext==0.4.0) (2.0.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /Users/kaiyuancui/opt/anaconda3/lib/python3.9/site-packages (from sympy->torch->torchtext==0.4.0) (1.2.1)\n",
            "Installing collected packages: torchtext\n",
            "Successfully installed torchtext-0.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip3 install torchtext==0.4.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "VSq_5IHKuTUu",
        "outputId": "e53b3ea8-1625-46f6-9e8d-f963c83e9eea"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "only integer scalar arrays can be converted to a scalar index",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Input \u001b[0;32mIn [83]\u001b[0m, in \u001b[0;36m<cell line: 69>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m     original_data \u001b[38;5;241m=\u001b[39m [TEXT\u001b[38;5;241m.\u001b[39mvocab\u001b[38;5;241m.\u001b[39mitos[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m data_reshaped]\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m original_data\n\u001b[0;32m---> 69\u001b[0m reversed_data \u001b[38;5;241m=\u001b[39m \u001b[43mreverse_batchify\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(reversed_data[\u001b[38;5;241m0\u001b[39m])\n",
            "Input \u001b[0;32mIn [83]\u001b[0m, in \u001b[0;36mreverse_batchify\u001b[0;34m(batched_data)\u001b[0m\n\u001b[1;32m     62\u001b[0m data_reshaped \u001b[38;5;241m=\u001b[39m data_array\u001b[38;5;241m.\u001b[39mreshape(original_shape)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Convert the numerical indices back to tokens using the vocabulary\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m original_data \u001b[38;5;241m=\u001b[39m [TEXT\u001b[38;5;241m.\u001b[39mvocab\u001b[38;5;241m.\u001b[39mitos[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m data_reshaped]\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m original_data\n",
            "Input \u001b[0;32mIn [83]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     62\u001b[0m data_reshaped \u001b[38;5;241m=\u001b[39m data_array\u001b[38;5;241m.\u001b[39mreshape(original_shape)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Convert the numerical indices back to tokens using the vocabulary\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m original_data \u001b[38;5;241m=\u001b[39m [\u001b[43mTEXT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitos\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m data_reshaped]\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m original_data\n",
            "\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
          ]
        }
      ],
      "source": [
        "\n",
        "# using a classification model to test relevance of each evidence\n",
        "\n",
        "# assuming train data text formatted as follows:\n",
        "# trainX = [claim_text + SEPARATION_TOKEN + evidence_text, .....]\n",
        "# trainY = [RELEVANT, NOT_RELEVANT, ...... ]\n",
        "\n",
        "# WORKSHOP 8 ------\n",
        "import torch\n",
        "import torchtext\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "# Define tokenizer\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "# Define Field for text data\n",
        "TEXT = torchtext.data.Field(tokenize=tokenizer,\n",
        "                            init_token='<sos>',\n",
        "                            eos_token='<eos>',\n",
        "                            lower=True)\n",
        "\n",
        "# Define Field for label data\n",
        "LABEL = torchtext.data.LabelField(dtype=torch.float)\n",
        "\n",
        "# Load IMDb dataset\n",
        "train_txt, test_txt = torchtext.datasets.IMDB.splits(TEXT, LABEL)\n",
        "\n",
        "# Split train_txt into train and validation sets\n",
        "train_txt, valid_txt = train_txt.split(split_ratio=0.8)\n",
        "\n",
        "# Build vocabulary\n",
        "TEXT.build_vocab(train_txt)\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Batchify function\n",
        "def batchify(data, bsz):\n",
        "    data = TEXT.numericalize([data.examples[0].text])\n",
        "    # Divide the dataset into bsz parts.\n",
        "    nbatch = data.size(0) // bsz\n",
        "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "    data = data.narrow(0, 0, nbatch * bsz)\n",
        "    # Evenly divide the data across the bsz batches.\n",
        "    data = data.view(bsz, -1).t().contiguous()\n",
        "    return data.to(device)\n",
        "\n",
        "# Batch sizes\n",
        "batch_size = 20\n",
        "eval_batch_size = 10\n",
        "\n",
        "# Process the datasets\n",
        "train_data = batchify(train_txt, batch_size)\n",
        "val_data = batchify(valid_txt, eval_batch_size)\n",
        "test_data = batchify(test_txt, eval_batch_size)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['how', 'can', 'such', 'good', 'actors', 'like', 'jean', 'rochefort', 'and', 'carole', 'bouquet', 'could', 'have', 'been', 'involved', 'in', 'such', 'a', '.', '.', '.', 'a', '.', '.', '.', 'well', ',', 'such', 'a', 'thing', '?', 'i', 'can', \"'\", 't', 'get', 'it', '.', 'it', 'was', 'awful', ',', 'very', 'baldy', 'played', '(', 'but', 'some', 'of', 'the', 'few', 'leading', 'roles', ')', ',', 'the', 'jokes', 'are', 'dumb', 'and', 'absolutely', 'not', 'funny', '.', '.', '.', 'i', 'won', \"'\", 't', 'talk', 'more', 'about', 'this', 'movie', ',', 'except', 'for', 'one', 'little', 'piece', 'of', 'advice', 'do', 'not', 'go', 'see', 'it', ',', 'it', 'will', 'be', 'a', 'waste', 'of', 'time', 'and', 'money', '.']\n",
            "neg\n",
            "['wow', ',', 'finally', 'jim', 'carrey', 'has', 'returned', 'from', 'the', 'died', '.', 'this', 'movie', 'had', 'me', 'laughing', 'and', 'crying', '.', 'it', 'also', 'sends', 'a', 'message', 'that', 'we', 'should', 'all', 'know', 'and', 'learn', 'from', '.', 'jeniffer', 'aniston', 'was', 'great', ',', 'she', 'will', 'finally', 'have', 'a', 'hit', 'movie', 'under', 'her', 'belt', '.', 'if', 'you', 'liked', 'liar', 'liar', 'you', 'will', 'love', 'this', 'movie', '.', 'i', 'give', 'it', '9/10', '.']\n",
            "pos\n",
            "<torchtext.data.dataset.Dataset object at 0x34abd3fd0>\n",
            "tensor([[   96,  9776,     5,   147,    13,    27,     6,     5,    52,     9],\n",
            "        [   60, 37708,     5,     8,     5,    24,     4,     5,    16,  1997],\n",
            "        [  147,   105,     5,   163,    13,    58,   632,     5,    23,    93],\n",
            "        [   59,    35,     8,    57,    19,     9,    33,    15,     6,    31],\n",
            "        [  164,    86,     5,    15,   386,     4,   964,   374,   540,   150],\n",
            "        [   47,   575,     5,    60,     6,   181,     7,    11,    22,    77],\n",
            "        [ 1881,    14,     5,    11,    64,   969,   436,    29,    37,    13],\n",
            "        [29759,   147,    82,    29,     0,   556,    31,   732,   123,     6],\n",
            "        [    7,     8,     6,    85,   261,    26,   165,    61,   420,    13]])\n",
            "<torchtext.data.field.Field object at 0x1529feca0>\n"
          ]
        }
      ],
      "source": [
        "print(valid_txt[0].text)\n",
        "print(valid_txt[0].label)\n",
        "\n",
        "print(valid_txt[1].text)\n",
        "print(valid_txt[1].label)\n",
        "\n",
        "print(valid_txt)\n",
        "\n",
        "# ours can be:\n",
        "# text =  claim_tokens + [SEP_TOKEN] + evidence_tokens\n",
        "# label <- one of {'rel', 'irr'} (relevant, irrelevant)\n",
        "\n",
        "# and use the same batchify method ?\n",
        "\n",
        "print(val_data)\n",
        "\n",
        "print(TEXT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['how', 'carole', '.', 'such', 'it', '(', ',', '.', 'about', 'of'], ['can', 'bouquet', '.', 'a', '.', 'but', 'the', '.', 'this', 'advice'], ['such', 'could', '.', 'thing', 'it', 'some', 'jokes', '.', 'movie', 'do'], ['good', 'have', 'a', '?', 'was', 'of', 'are', 'i', ',', 'not'], ['actors', 'been', '.', 'i', 'awful', 'the', 'dumb', 'won', 'except', 'go'], ['like', 'involved', '.', 'can', ',', 'few', 'and', \"'\", 'for', 'see'], ['jean', 'in', '.', \"'\", 'very', 'leading', 'absolutely', 't', 'one', 'it'], ['rochefort', 'such', 'well', 't', '<unk>', 'roles', 'not', 'talk', 'little', ','], ['and', 'a', ',', 'get', 'played', ')', 'funny', 'more', 'piece', 'it']]\n"
          ]
        }
      ],
      "source": [
        "def reverse_batchify(batched_data):\n",
        "    # Convert the batched data tensor to a numpy array\n",
        "    data_array = batched_data.cpu().numpy()\n",
        "    \n",
        "    # Convert numerical indices back to tokens using the vocabulary\n",
        "    original_data = []\n",
        "    for row in data_array:\n",
        "        tokens = [TEXT.vocab.itos[idx] for idx in row]\n",
        "        original_data.append(tokens)\n",
        "    \n",
        "    return original_data\n",
        "\n",
        "\n",
        "reversed_data = reverse_batchify(val_data)\n",
        "print(reversed_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/kaiyuancui/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        }
      ],
      "source": [
        "bptt = 35\n",
        "def get_batch(source, i):\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].view(-1)\n",
        "    return data, target\n",
        "\n",
        "\n",
        "ntokens = len(TEXT.vocab.stoi) # the size of vocabulary\n",
        "emsize = 200 # embedding dimension\n",
        "nhid = 200 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
        "nlayers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
        "nhead = 2 # the number of heads in the multiheadattention models\n",
        "dropout = 0.2 # the dropout value\n",
        "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 5.0 # learning rate\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
        "\n",
        "import time\n",
        "def train():\n",
        "\n",
        "    model.train() # Turn on the train mode\n",
        "    total_loss = 0.\n",
        "    start_time = time.time()\n",
        "    ntokens = len(TEXT.vocab.stoi)\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output.view(-1, ntokens), targets)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        log_interval = 200\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            cur_loss = total_loss / log_interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
        "                  'lr {:02.2f} | ms/batch {:5.2f} | '\n",
        "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                    epoch, batch, len(train_data) // bptt, scheduler.get_lr()[0],\n",
        "                    elapsed * 1000 / log_interval,\n",
        "                    cur_loss, math.exp(cur_loss)))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "\n",
        "\n",
        "def evaluate(eval_model, data_source):\n",
        "    eval_model.eval() # Turn on the evaluation mode\n",
        "    total_loss = 0.\n",
        "    ntokens = len(TEXT.vocab.stoi)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, data_source.size(0) - 1, bptt):\n",
        "            data, targets = get_batch(data_source, i)\n",
        "            print(targets)\n",
        "            output = eval_model(data)\n",
        "            \n",
        "            output_flat = output.view(-1, ntokens)\n",
        "       \n",
        "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
        "    return total_loss / (len(data_source) - 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[ 0.2748, -0.5060,  0.5688,  ...,  1.6586, -1.2559,  0.3453],\n",
            "         [ 0.0743, -0.2062,  1.8589,  ...,  1.2166, -2.8926, -0.1619],\n",
            "         [-0.0331,  0.1290,  0.2629,  ...,  0.2275, -1.1322,  0.0752],\n",
            "         ...,\n",
            "         [-0.0331,  0.1290,  0.2629,  ...,  0.2275, -1.1322,  0.0752],\n",
            "         [-0.3395, -0.7340,  1.0964,  ...,  1.1425, -0.3426,  0.4701],\n",
            "         [ 0.4491, -0.5914,  0.5147,  ...,  0.9385, -1.7443, -0.4042]],\n",
            "\n",
            "        [[ 0.8524, -0.3086,  0.6977,  ...,  1.1315, -1.4706,  0.2938],\n",
            "         [-0.4178, -0.8994,  1.3850,  ...,  0.8975, -2.2735,  0.1196],\n",
            "         [-0.0124,  0.1729,  0.2242,  ...,  0.1382, -1.1179,  0.0198],\n",
            "         ...,\n",
            "         [-0.0124,  0.1729,  0.2242,  ...,  0.1382, -1.1179,  0.0198],\n",
            "         [-0.3050, -0.7221,  1.1860,  ...,  0.8514, -0.3012, -0.4128],\n",
            "         [ 1.7305, -0.4095,  1.6345,  ...,  0.5868, -1.7220, -1.2164]],\n",
            "\n",
            "        [[ 0.6288, -1.2293,  0.9387,  ...,  0.9099, -1.1320,  0.2057],\n",
            "         [ 0.6403, -1.0638,  1.2341,  ...,  0.2523, -1.2233, -0.6443],\n",
            "         [-0.0285,  0.2370,  0.1831,  ...,  0.0421, -1.0299, -0.0211],\n",
            "         ...,\n",
            "         [-0.0285,  0.2370,  0.1831,  ...,  0.0421, -1.0299, -0.0211],\n",
            "         [-0.1758, -1.1576,  1.6822,  ...,  0.4661, -0.4855, -0.1630],\n",
            "         [ 0.3078, -0.6587,  1.0149,  ...,  1.3179, -0.8757,  0.1243]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.0732, -0.8576,  0.8936,  ...,  0.1628, -1.0690, -0.4089],\n",
            "         [ 0.6553, -0.5473,  0.8167,  ...,  0.0220, -1.5489,  0.1613],\n",
            "         [-0.2380,  0.1195,  0.2118,  ...,  0.1046, -0.7760,  0.3197],\n",
            "         ...,\n",
            "         [ 0.0152, -0.0241,  1.5808,  ...,  0.7451, -0.5701,  0.5913],\n",
            "         [-0.2999, -0.6939,  1.3683,  ...,  0.8489, -1.0337, -0.3225],\n",
            "         [ 0.6839, -0.9537,  1.7773,  ...,  0.3421, -0.7904,  0.4009]],\n",
            "\n",
            "        [[-0.0731,  0.1318,  1.2491,  ...,  0.6501, -0.8649,  0.5392],\n",
            "         [-0.6085, -0.1767,  1.0733,  ...,  1.2012, -1.0247,  0.5214],\n",
            "         [-0.2748,  0.1093,  0.2500,  ...,  0.1336, -0.8472,  0.4169],\n",
            "         ...,\n",
            "         [ 0.8817, -0.7609,  1.4541,  ...,  0.4902, -1.1288,  0.5118],\n",
            "         [ 0.3209, -0.5298,  1.6527,  ...,  0.8827, -1.0034, -0.3222],\n",
            "         [ 0.4549, -0.6707,  0.5405,  ...,  0.9324, -1.1775, -0.3283]],\n",
            "\n",
            "        [[-0.6788,  0.1743,  0.9000,  ...,  0.2733, -1.2387,  0.4562],\n",
            "         [ 0.3208, -1.4239,  0.9750,  ...,  0.5454, -1.3951,  0.5123],\n",
            "         [ 0.5904, -0.0179,  1.4243,  ...,  0.7389, -0.9629,  0.9865],\n",
            "         ...,\n",
            "         [ 0.2811, -0.7264,  1.2585,  ...,  0.6787, -1.1382,  1.2730],\n",
            "         [ 0.0060, -1.1054,  0.6472,  ...,  0.6844, -0.6494,  0.3489],\n",
            "         [ 0.3804, -0.8759,  1.0259,  ...,  0.7885, -0.6715,  0.4173]]])\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time:  0.34s | valid loss 11.51 | valid ppl 100113.71\n",
            "-----------------------------------------------------------------------------------------\n",
            "tensor([[[ 0.1941,  0.3439, -1.0493,  ...,  1.8622, -0.6460,  1.1640],\n",
            "         [-0.1782,  0.1997,  0.6445,  ...,  1.3258, -2.5385,  0.5575],\n",
            "         [-0.3633,  0.8145, -1.2858,  ..., -0.0940, -0.4600,  0.3569],\n",
            "         ...,\n",
            "         [-0.3633,  0.8145, -1.2858,  ..., -0.0940, -0.4600,  0.3569],\n",
            "         [-0.9353, -0.4152, -0.5057,  ...,  0.9499,  0.7798,  1.4187],\n",
            "         [ 0.4958, -0.1761, -1.1093,  ...,  0.7558, -1.3581, -0.0917]],\n",
            "\n",
            "        [[ 1.1300,  0.5239, -0.7566,  ...,  0.7614, -0.9695,  0.8607],\n",
            "         [-0.8302, -0.5296,  0.0365,  ...,  0.7427, -1.5865,  0.5657],\n",
            "         [-0.3752,  0.8465, -1.3745,  ..., -0.1975, -0.4498,  0.2805],\n",
            "         ...,\n",
            "         [-0.3752,  0.8465, -1.3745,  ..., -0.1975, -0.4498,  0.2805],\n",
            "         [-0.8523, -0.4638, -0.1379,  ...,  0.6069,  1.0512,  0.0044],\n",
            "         [ 1.8761,  0.0551,  0.2422,  ...,  0.0644, -1.5031, -1.0153]],\n",
            "\n",
            "        [[ 0.8443, -0.9813, -0.5084,  ...,  0.6968, -0.3559,  0.9126],\n",
            "         [ 0.4676, -0.4833, -0.2125,  ...,  0.1470, -0.5117, -0.2351],\n",
            "         [-0.4212,  0.9055, -1.4491,  ..., -0.3204, -0.3567,  0.2021],\n",
            "         ...,\n",
            "         [-0.4212,  0.9055, -1.4491,  ..., -0.3204, -0.3567,  0.2021],\n",
            "         [-0.5102, -0.7532,  0.3992,  ...,  0.3218,  0.6554,  0.4600],\n",
            "         [ 0.2605, -0.0993, -0.5252,  ...,  1.2023,  0.2155,  0.6873]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.3949, -0.6664, -0.6851,  ..., -0.5231, -0.2080,  0.1537],\n",
            "         [ 0.5471, -0.0725, -0.7216,  ..., -0.4287, -1.0100,  0.7558],\n",
            "         [-0.6407,  0.7803, -1.4348,  ..., -0.1900, -0.0105,  0.5823],\n",
            "         ...,\n",
            "         [-0.1603,  0.5693,  0.3081,  ...,  0.4648,  0.3090,  1.2876],\n",
            "         [-0.6210, -0.3883,  0.0322,  ...,  0.9338, -0.0926,  0.2831],\n",
            "         [ 0.8664, -0.6101,  0.5506,  ..., -0.1706,  0.3552,  1.0964]],\n",
            "\n",
            "        [[-0.2831,  0.8736, -0.1231,  ...,  0.2754,  0.1284,  1.3040],\n",
            "         [-1.0613,  0.1259, -0.6434,  ...,  1.2531, -0.3228,  1.3359],\n",
            "         [-0.6785,  0.7860, -1.3591,  ..., -0.1685, -0.1113,  0.7528],\n",
            "         ...,\n",
            "         [ 0.5978, -0.6034, -0.0816,  ...,  0.1542, -0.3791,  1.2562],\n",
            "         [ 0.5632,  0.2191,  0.1339,  ...,  0.5940, -0.1300,  0.1785],\n",
            "         [ 0.5186, -0.2053, -1.2049,  ...,  0.6827, -0.3835,  0.0467]],\n",
            "\n",
            "        [[-1.3009,  0.8153, -0.5220,  ..., -0.0972, -0.6267,  1.3257],\n",
            "         [ 0.3393, -1.3014, -0.5650,  ...,  0.3500, -0.6874,  1.5093],\n",
            "         [ 0.3754,  0.6683,  0.1253,  ...,  0.5030, -0.3708,  1.9099],\n",
            "         ...,\n",
            "         [ 0.1786, -0.6005, -0.1747,  ...,  0.5545, -0.4634,  2.1470],\n",
            "         [-0.2284, -0.7645, -1.2523,  ...,  0.5164,  0.2464,  0.9990],\n",
            "         [ 0.5257, -0.5744, -0.6852,  ...,  0.8007,  0.4712,  0.9831]]])\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time:  0.18s | valid loss 11.68 | valid ppl 117661.33\n",
            "-----------------------------------------------------------------------------------------\n",
            "tensor([[[ 1.1249e-01, -4.4404e-01, -6.0576e-01,  ...,  1.4798e+00,\n",
            "          -9.9463e-01,  4.2771e-01],\n",
            "         [-4.0129e-01, -3.0495e-01,  1.0822e+00,  ...,  1.2864e+00,\n",
            "          -3.3607e+00, -2.6290e-01],\n",
            "         [-5.8310e-01,  1.9733e-01, -5.1662e-01,  ..., -8.7435e-02,\n",
            "          -1.0468e+00, -9.9202e-02],\n",
            "         ...,\n",
            "         [-5.8310e-01,  1.9733e-01, -5.1662e-01,  ..., -8.7435e-02,\n",
            "          -1.0468e+00, -9.9202e-02],\n",
            "         [-1.1940e+00, -1.0079e+00,  1.6567e-01,  ...,  1.0414e+00,\n",
            "           4.7186e-02,  6.4513e-01],\n",
            "         [ 1.5031e-01, -7.8374e-01, -8.0740e-01,  ...,  3.9731e-01,\n",
            "          -1.8888e+00, -6.4425e-01]],\n",
            "\n",
            "        [[ 8.1481e-01, -1.2152e-01, -4.9850e-01,  ...,  8.5457e-01,\n",
            "          -1.6230e+00,  1.2521e-01],\n",
            "         [-1.0037e+00, -1.2805e+00,  4.9797e-01,  ...,  7.6737e-01,\n",
            "          -2.4043e+00, -4.9534e-02],\n",
            "         [-5.7517e-01,  2.3248e-01, -5.5863e-01,  ..., -1.7302e-01,\n",
            "          -1.0512e+00, -1.4329e-01],\n",
            "         ...,\n",
            "         [-5.7517e-01,  2.3248e-01, -5.5863e-01,  ..., -1.7302e-01,\n",
            "          -1.0512e+00, -1.4329e-01],\n",
            "         [-9.2298e-01, -8.8217e-01,  3.3120e-01,  ...,  3.8548e-01,\n",
            "           2.9496e-01, -5.5015e-01],\n",
            "         [ 1.4340e+00, -5.1769e-01,  6.3946e-01,  ...,  1.8923e-01,\n",
            "          -1.8144e+00, -1.3827e+00]],\n",
            "\n",
            "        [[ 4.4743e-01, -1.4420e+00, -2.3964e-01,  ...,  5.8714e-01,\n",
            "          -1.1426e+00,  1.0011e-01],\n",
            "         [ 1.5582e-01, -1.2796e+00, -1.2985e-02,  ..., -7.3575e-02,\n",
            "          -1.2330e+00, -1.0750e+00],\n",
            "         [-5.9318e-01,  2.6566e-01, -5.9411e-01,  ..., -2.6189e-01,\n",
            "          -9.9054e-01, -1.7347e-01],\n",
            "         ...,\n",
            "         [-5.9318e-01,  2.6566e-01, -5.9411e-01,  ..., -2.6189e-01,\n",
            "          -9.9054e-01, -1.7347e-01],\n",
            "         [-8.2161e-01, -1.4155e+00,  9.1671e-01,  ...,  2.5413e-01,\n",
            "          -1.2720e-01, -2.5036e-01],\n",
            "         [-9.8651e-05, -7.8954e-01, -7.1192e-02,  ...,  9.8655e-01,\n",
            "          -6.7028e-01, -4.6494e-02]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-6.0765e-01, -9.8644e-01, -1.7151e-01,  ..., -4.6015e-01,\n",
            "          -9.7237e-01, -6.1025e-01],\n",
            "         [ 2.9016e-01, -5.6133e-01, -2.9703e-01,  ..., -3.3319e-01,\n",
            "          -1.6784e+00,  1.8085e-01],\n",
            "         [-7.6643e-01,  1.8036e-01, -5.8418e-01,  ..., -1.9449e-01,\n",
            "          -8.0772e-01,  8.0141e-02],\n",
            "         ...,\n",
            "         [-4.7781e-01,  3.8142e-02,  6.0232e-01,  ...,  2.0455e-01,\n",
            "          -5.1639e-01,  6.6750e-01],\n",
            "         [-8.1004e-01, -7.1948e-01,  3.1959e-01,  ...,  6.5910e-01,\n",
            "          -1.0104e+00, -4.7473e-01],\n",
            "         [ 3.1033e-01, -1.3714e+00,  9.5334e-01,  ..., -1.0744e-01,\n",
            "          -4.7274e-01,  4.0752e-01]],\n",
            "\n",
            "        [[-5.0899e-01,  3.0417e-01,  3.5575e-01,  ...,  4.2989e-01,\n",
            "          -6.4102e-01,  5.7755e-01],\n",
            "         [-1.1740e+00, -3.3545e-01, -1.4272e-01,  ...,  9.9017e-01,\n",
            "          -1.1362e+00,  3.8156e-01],\n",
            "         [-7.9001e-01,  1.8774e-01, -5.3710e-01,  ..., -1.8295e-01,\n",
            "          -8.5142e-01,  1.8021e-01],\n",
            "         ...,\n",
            "         [ 2.3112e-01, -7.5161e-01,  4.4916e-01,  ...,  1.7463e-01,\n",
            "          -1.1497e+00,  3.4063e-01],\n",
            "         [ 1.2461e-01, -5.2179e-01,  5.6937e-01,  ...,  4.2056e-01,\n",
            "          -7.7686e-01, -4.0379e-01],\n",
            "         [ 3.0869e-01, -8.0459e-01, -6.0374e-01,  ...,  6.5083e-01,\n",
            "          -1.2334e+00, -4.7808e-01]],\n",
            "\n",
            "        [[-1.2877e+00,  2.3909e-01,  9.3146e-03,  ..., -9.3089e-02,\n",
            "          -1.0496e+00,  4.3643e-01],\n",
            "         [-3.2389e-02, -1.6136e+00, -8.7454e-02,  ...,  2.5476e-01,\n",
            "          -1.4487e+00,  6.0120e-01],\n",
            "         [ 5.3759e-02,  8.3935e-02,  5.5470e-01,  ...,  2.8225e-01,\n",
            "          -9.0665e-01,  9.4256e-01],\n",
            "         ...,\n",
            "         [-2.1752e-01, -7.8240e-01,  2.3421e-01,  ...,  1.9451e-01,\n",
            "          -9.4194e-01,  1.3015e+00],\n",
            "         [-5.3985e-01, -1.2736e+00, -6.9862e-01,  ...,  4.1713e-01,\n",
            "          -5.7613e-01,  2.1196e-01],\n",
            "         [ 3.4240e-01, -1.0894e+00, -2.8964e-01,  ...,  3.8721e-01,\n",
            "          -3.8658e-01,  3.4486e-01]]])\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time:  0.16s | valid loss 10.33 | valid ppl 30688.17\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "best_val_loss = float(\"inf\")\n",
        "epochs = 3 # The number of epochs\n",
        "best_model = None\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train()\n",
        "    val_loss = evaluate(model, val_data)\n",
        "    print('-' * 89)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                     val_loss, math.exp(val_loss)))\n",
        "    print('-' * 89)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_model = model\n",
        "\n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([   30,  8357,    62,    63,   557,     5,    63,    10,   134,    50,\n",
            "           43,    21,    49,   105,     8,    15,    76,  1795,    12,    10,\n",
            "          777,    49, 15095,    85,  1186,    19,  3775,    45,     8,   176,\n",
            "           73,  1888,    19,    15,    92,  1560,     7,     4,   249,    40,\n",
            "            6,     5,  2621,     5,     4,   538,  7731,   745,    44,    63,\n",
            "          304,     8,    14,   247,  4183,     5, 19371,  1339,  6638,    54,\n",
            "        11287,  3178,     0,     5,    21,  1064,     8,     7,  8211,     8,\n",
            "          288,    10,     6,  6577,     8,    39,  1339,   360,     6,   317,\n",
            "            4, 37860,    63,    45,  3350,   612,     6,    10, 18966,   535,\n",
            "         1663,    21,   105,     4,  3609,  3191,    63,  2334,  8803,     5,\n",
            "            9,    49,    31,   325,     5,  1949,   225,    99,     6,    16,\n",
            "           43,   782,   566, 14833,    15,     7,    49,     6, 18931,    12,\n",
            "          325,   467,     4,     5,   801,    63,  3350,    21,    49,    43,\n",
            "          947,    47,   702,    10,  1526,    19,  3609,    67,   129,  3108,\n",
            "            6,     8,    21,  2197,   152,  1036,    85,   207,    10,    73,\n",
            "          635,    59,    49,    14,    14,    14,  3875,     9,   593,     5,\n",
            "          657,   328,   782,     8,   156,     8,     7,   746,   167,    80,\n",
            "            7,    10,     6,   270,  1560,  1033,   331,     5,     5,    33,\n",
            "          489,    85,     7,   168, 12862,  7209,     5,  5987,  5479,  4483,\n",
            "           78,   250,    19,    63,     6,     5,    14,    49,  6612,    17,\n",
            "         2009,    45,   935,   105,    24,    50,     8,   129,    12,  2270,\n",
            "            7,    13,    10,  2373,   166,    62,  3348,    19,   317,    12,\n",
            "           63,    39,   771,    55, 16015,    13,   342,    14,     6,     4,\n",
            "           19,     6,   155,     6,    14,   606,    63,  2258,     7,    37])\n",
            "=========================================================================================\n",
            "| End of training | test loss 10.14 | test ppl 25303.30\n",
            "=========================================================================================\n",
            "tensor([[[ 0.6643, -0.1033,  0.0308,  ...,  0.5299, -1.4911, -1.0017],\n",
            "         [-1.0247, -1.4095,  0.2525,  ...,  0.6111, -1.9695, -0.0342],\n",
            "         [-0.5831,  0.1973, -0.5166,  ..., -0.0874, -1.0468, -0.0992],\n",
            "         ...,\n",
            "         [-0.5831,  0.1973, -0.5166,  ..., -0.0874, -1.0468, -0.0992],\n",
            "         [-0.7907, -0.7789,  0.6694,  ...,  0.4820, -0.0446, -0.3495],\n",
            "         [ 1.0633, -0.5488,  0.3864,  ...,  0.2555, -1.5847, -1.2604]],\n",
            "\n",
            "        [[ 0.6609, -0.0311, -0.0411,  ...,  0.4265, -1.5139, -1.0615],\n",
            "         [-1.0452, -1.3687,  0.2474,  ...,  0.5196, -2.0246, -0.0912],\n",
            "         [-0.5752,  0.2325, -0.5586,  ..., -0.1730, -1.0512, -0.1433],\n",
            "         ...,\n",
            "         [-0.5752,  0.2325, -0.5586,  ..., -0.1730, -1.0512, -0.1433],\n",
            "         [-0.8377, -0.7388,  0.6795,  ...,  0.3659, -0.0059, -0.4181],\n",
            "         [ 1.0556, -0.4911,  0.3726,  ...,  0.1153, -1.5830, -1.3563]],\n",
            "\n",
            "        [[ 0.5693,  0.0630, -0.1122,  ...,  0.2787, -1.4137, -1.0826],\n",
            "         [-1.0859, -1.2752,  0.2319,  ...,  0.3928, -1.9652, -0.1460],\n",
            "         [-0.5932,  0.2657, -0.5941,  ..., -0.2619, -0.9905, -0.1735],\n",
            "         ...,\n",
            "         [-0.5932,  0.2657, -0.5941,  ..., -0.2619, -0.9905, -0.1735],\n",
            "         [-0.9250, -0.6963,  0.6775,  ...,  0.2091,  0.1639, -0.4285],\n",
            "         [ 0.9360, -0.4062,  0.3471,  ..., -0.0791, -1.4291, -1.4015]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.2333,  0.0177,  0.0226,  ...,  0.2853, -1.2753, -0.3861],\n",
            "         [-1.3389, -1.2447,  0.2860,  ...,  0.4011, -1.8565,  0.3951],\n",
            "         [-0.7833,  0.2771, -0.4725,  ..., -0.2240, -0.9252,  0.1974],\n",
            "         ...,\n",
            "         [-0.7833,  0.2771, -0.4725,  ..., -0.2240, -0.9252,  0.1974],\n",
            "         [-1.1016, -0.6815,  0.7504,  ...,  0.2031,  0.1159,  0.3449],\n",
            "         [ 0.5557, -0.3181,  0.5088,  ...,  0.0842, -1.3633, -0.5878]],\n",
            "\n",
            "        [[ 0.2584,  0.1860,  0.0318,  ...,  0.1985, -1.2484, -0.4127],\n",
            "         [-1.3649, -1.1134,  0.2858,  ...,  0.3298, -1.8275,  0.3936],\n",
            "         [-0.7922,  0.3672, -0.4893,  ..., -0.2696, -0.8894,  0.1794],\n",
            "         ...,\n",
            "         [-0.7922,  0.3672, -0.4893,  ..., -0.2696, -0.8894,  0.1794],\n",
            "         [-1.1213, -0.4863,  0.7567,  ...,  0.1241,  0.1579,  0.3288],\n",
            "         [ 0.5904, -0.1100,  0.5471,  ...,  0.0128, -1.3380, -0.6337]],\n",
            "\n",
            "        [[ 0.3191,  0.3341,  0.0750,  ...,  0.1117, -1.1136, -0.4678],\n",
            "         [-1.3338, -0.9726,  0.3199,  ...,  0.2410, -1.7160,  0.3610],\n",
            "         [-0.7648,  0.4521, -0.4746,  ..., -0.3092, -0.7887,  0.1574],\n",
            "         ...,\n",
            "         [-0.7648,  0.4521, -0.4746,  ..., -0.3092, -0.7887,  0.1574],\n",
            "         [-1.1112, -0.3168,  0.7738,  ...,  0.0361,  0.2981,  0.2833],\n",
            "         [ 0.6597,  0.0713,  0.6121,  ..., -0.1042, -1.1556, -0.7005]]],\n",
            "       grad_fn=<ViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "test_loss = evaluate(best_model, test_data)\n",
        "\n",
        "\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
        "    test_loss, math.exp(test_loss)))\n",
        "print('=' * 89)\n",
        "\n",
        "\n",
        "print(best_model(val_data[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cn5LPjkHAO_f",
        "outputId": "487a618f-ebc1-476c-bed4-b2f09571b922"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Global forcing from waste heat was 0.028 W/m2 in 2005.\n",
            "Thus, the waste heat engine may be one of the least expensive components of a complete waste heat recovery system.\n",
            "Only a tiny fraction of the original chemical energy is used for work:\n",
            "Land ice sheets in both Antarctica and Greenland have been losing mass since 2002 and have seen an acceleration of ice mass loss since 2009.\n"
          ]
        }
      ],
      "source": [
        "#print(train.head())\n",
        "#print(test_with_evi.head())\n",
        "# claim: contribution waste heat global climate\n",
        "print(evidence['evidence-308923'])\n",
        "print(evidence['evidence-213569'])\n",
        "\n",
        "#greenland lost tiny fraction ice mass\t\n",
        "print(evidence['evidence-962481'])\n",
        "print(evidence['evidence-1200633'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "pvjyFX7dD6ij"
      },
      "outputs": [],
      "source": [
        "from torch import nn, optim\n",
        "\n",
        "# define hypermeter\n",
        "sequence_len = 28\n",
        "input_len = 28\n",
        "hidden_size = 128\n",
        "num_layers = 2\n",
        "num_classes = 4\n",
        "num_epchos = 5\n",
        "learning_rate = 0.01\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "7nG03eBPFZ_K"
      },
      "outputs": [],
      "source": [
        "class LSTM(nn.Module):\n",
        "  def __init__(self, input_length, hidden_size, num_classes, num_layers):\n",
        "    super(LSTM, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "    self.lstm = nn.LSTM(input_len, hidden_size, num_layers, num_classes, batch_first=True)\n",
        "    self.output_layer = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "  def forward(self, X):\n",
        "    hidden_states = torch.zeros(self.num_layers, X.size(0), self.hidden_size)\n",
        "    cell_states = torch.zeros(self.num_layers, X.size(0), self.hidden_size)\n",
        "    out, _ = self.lstm(X, (hidden_states, cell_states))\n",
        "    out = self.output_layer(out[:, -1, :])\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_WMcOkxH9Sj",
        "outputId": "0316dddf-f21c-465d-a772-8b42cf9d365f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LSTM(\n",
            "  (lstm): LSTM(28, 128, num_layers=2, bias=4, batch_first=True)\n",
            "  (output_layer): Linear(in_features=128, out_features=4, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = LSTM(input_len, hidden_size, num_classes, num_layers)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "Oau1OuglI7Hh"
      },
      "outputs": [],
      "source": [
        "loss_func = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DFZXfcGJaPY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzGuzHPE87Ya"
      },
      "source": [
        "# 3.Testing and Evaluation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZVeNYIH9IaL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mefSOe8eTmGP"
      },
      "source": [
        "## Object Oriented Programming codes here\n",
        "\n",
        "*You can use multiple code snippets. Just add more if needed*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# model adpated from workshop 8\n",
        "class TransformerModel(nn.Module):\n",
        "\n",
        "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "        self.model_type = 'Transformer'\n",
        "        self.src_mask = None\n",
        "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
        "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.encoder = nn.Embedding(ntoken, ninp)\n",
        "        self.ninp = ninp\n",
        "        self.decoder = nn.Linear(ninp, ntoken)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def _generate_square_subsequent_mask(self, sz):\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src):\n",
        "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
        "            device = src.device\n",
        "            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
        "            self.src_mask = mask\n",
        "\n",
        "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
        "        src = self.pos_encoder(src)\n",
        "        output = self.transformer_encoder(src, self.src_mask)\n",
        "        output = self.decoder(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term) #0::2 means starting with index 0, step = 2\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
