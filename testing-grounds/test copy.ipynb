{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32yCsRUo8H33"
      },
      "source": [
        "# 2024 COMP90042 Project\n",
        "*Make sure you change the file name with your group id.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCybYoGz8YWQ"
      },
      "source": [
        "# Readme\n",
        "*If there is something to be noted for the marker, please mention here.*\n",
        "\n",
        "*If you are planning to implement a program with Object Oriented Programming style, please put those the bottom of this ipynb file*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6po98qVA8bJD"
      },
      "source": [
        "# 1.DataSet Processing\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvff21Hv8zjk",
        "outputId": "4aa2923a-2e47-4e05-c9cc-42fce5e6a04d"
      },
      "outputs": [],
      "source": [
        "LOCAL_DEV = True # to switch between developing locally and on colab\n",
        "\n",
        "if not LOCAL_DEV:\n",
        "    # TODO: need to upload data files on Google Drive?\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: contractions in /Users/kaiyuancui/opt/anaconda3/lib/python3.9/site-packages (0.1.73)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /Users/kaiyuancui/opt/anaconda3/lib/python3.9/site-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: pyahocorasick in /Users/kaiyuancui/opt/anaconda3/lib/python3.9/site-packages (from textsearch>=0.0.21->contractions) (2.1.0)\n",
            "Requirement already satisfied: anyascii in /Users/kaiyuancui/opt/anaconda3/lib/python3.9/site-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install contractions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "MH1Hdrb5NmHM"
      },
      "outputs": [],
      "source": [
        "#Imports\n",
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "2piZvV4OMSa3",
        "outputId": "b17e35ee-0c98-48b9-aa69-f6b63d336059"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>claim_text</th>\n",
              "      <th>claim_label</th>\n",
              "      <th>evidences</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>claim-1937</th>\n",
              "      <td>Not only is there no scientific evidence that ...</td>\n",
              "      <td>DISPUTED</td>\n",
              "      <td>[evidence-442946, evidence-1194317, evidence-1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-126</th>\n",
              "      <td>El Niño drove record highs in global temperatu...</td>\n",
              "      <td>REFUTES</td>\n",
              "      <td>[evidence-338219, evidence-1127398]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-2510</th>\n",
              "      <td>In 1946, PDO switched to a cool phase.</td>\n",
              "      <td>SUPPORTS</td>\n",
              "      <td>[evidence-530063, evidence-984887]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-2021</th>\n",
              "      <td>Weather Channel co-founder John Coleman provid...</td>\n",
              "      <td>DISPUTED</td>\n",
              "      <td>[evidence-1177431, evidence-782448, evidence-5...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-2449</th>\n",
              "      <td>\"January 2008 capped a 12 month period of glob...</td>\n",
              "      <td>NOT_ENOUGH_INFO</td>\n",
              "      <td>[evidence-1010750, evidence-91661, evidence-72...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   claim_text  \\\n",
              "claim-1937  Not only is there no scientific evidence that ...   \n",
              "claim-126   El Niño drove record highs in global temperatu...   \n",
              "claim-2510             In 1946, PDO switched to a cool phase.   \n",
              "claim-2021  Weather Channel co-founder John Coleman provid...   \n",
              "claim-2449  \"January 2008 capped a 12 month period of glob...   \n",
              "\n",
              "                claim_label                                          evidences  \n",
              "claim-1937         DISPUTED  [evidence-442946, evidence-1194317, evidence-1...  \n",
              "claim-126           REFUTES                [evidence-338219, evidence-1127398]  \n",
              "claim-2510         SUPPORTS                 [evidence-530063, evidence-984887]  \n",
              "claim-2021         DISPUTED  [evidence-1177431, evidence-782448, evidence-5...  \n",
              "claim-2449  NOT_ENOUGH_INFO  [evidence-1010750, evidence-91661, evidence-72...  "
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#visualising training data\n",
        "if LOCAL_DEV:\n",
        "    train = pd.read_json(\"../data/train-claims.json\") # for local dev\n",
        "    \n",
        "else:\n",
        "    train = pd.read_json(\"/content/drive/MyDrive/data/train-claims.json\") # on colab\n",
        "train = train.transpose()\n",
        "train.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>claim_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>claim-2967</th>\n",
              "      <td>The contribution of waste heat to the global c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-979</th>\n",
              "      <td>“Warm weather worsened the most recent five-ye...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-1609</th>\n",
              "      <td>Greenland has only lost a tiny fraction of its...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-1020</th>\n",
              "      <td>“The global reef crisis does not necessarily m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-2599</th>\n",
              "      <td>Small amounts of very active substances can ca...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   claim_text\n",
              "claim-2967  The contribution of waste heat to the global c...\n",
              "claim-979   “Warm weather worsened the most recent five-ye...\n",
              "claim-1609  Greenland has only lost a tiny fraction of its...\n",
              "claim-1020  “The global reef crisis does not necessarily m...\n",
              "claim-2599  Small amounts of very active substances can ca..."
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "if LOCAL_DEV:\n",
        "    test = pd.read_json(\"../data/test-claims-unlabelled.json\") # for local dev\n",
        "    \n",
        "else:\n",
        "    test = pd.read_json(\"/content/drive/MyDrive/data/test-claims-unlabelled.json\") # on colab\n",
        "test = test.transpose()\n",
        "test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ZKbGFcA2THHP"
      },
      "outputs": [],
      "source": [
        "#visualising evidence data\n",
        "if LOCAL_DEV:\n",
        "    evidence = pd.read_json(\"../data/evidence.json\",typ='series')\n",
        "else:\n",
        "    evidence = pd.read_json(\"/content/drive/MyDrive/data/evidence.json\",typ='series')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFNTiC0UMS45",
        "outputId": "a55e84f3-4f1c-4039-8fe0-cd2e8f2d13b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1208827\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "evidence-0    John Bennet Lawes, English entrepreneur and ag...\n",
              "evidence-1    Lindberg began his professional career at the ...\n",
              "evidence-2    ``Boston (Ladies of Cambridge)'' by Vampire We...\n",
              "evidence-3    Gerald Francis Goyer (born October 20, 1936) w...\n",
              "evidence-4    He detected abnormalities of oxytocinergic fun...\n",
              "dtype: object"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(len(evidence))\n",
        "evidence.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/kaiyuancui/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/kaiyuancui/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /Users/kaiyuancui/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import string\n",
        "import contractions\n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Xf73PDzRTrft"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "claim-2967               contribution waste heat global climate\n",
              "claim-979     warm weather worsened recent drought included ...\n",
              "claim-1609                greenland lost tiny fraction ice mass\n",
              "claim-1020    global reef crisis not necessarily mean extinc...\n",
              "claim-2599     small amount active substance cause large effect\n",
              "dtype: object"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def preprocess_data(data: pd.Series) -> pd.Series:\n",
        "  preprocessed_data = {}\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  stop_words.remove('not')\n",
        "  for id, text in data.items():\n",
        "    text = text.lower()\n",
        "    text = contractions.fix(text)\n",
        "    tokens = word_tokenize(text)\n",
        "    wnl = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [wnl.lemmatize(word) for word in tokens if word.isalpha() and word not in stop_words]\n",
        "    preprocessed_data[id] = \" \".join(lemmatized_tokens)\n",
        "\n",
        "  return pd.Series(preprocessed_data)\n",
        "\n",
        "processed_evidence = preprocess_data(evidence)\n",
        "\n",
        "test_claims = test['claim_text']\n",
        "processed_test = preprocess_data(test_claims)\n",
        "processed_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "processed_evidence = processed_evidence[processed_evidence.str.strip().str.len() > 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "yHWbo7W7TuUC"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>claim_text</th>\n",
              "      <th>evidence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>not scientific evidence pollutant higher conce...</td>\n",
              "      <td>high concentration time atmospheric concentrat...</td>\n",
              "      <td>related</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>not scientific evidence pollutant higher conce...</td>\n",
              "      <td>plant grow much percent faster concentration p...</td>\n",
              "      <td>related</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>not scientific evidence pollutant higher conce...</td>\n",
              "      <td>higher carbon dioxide concentration favourably...</td>\n",
              "      <td>related</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>not scientific evidence pollutant higher conce...</td>\n",
              "      <td>mine located state jharkhand</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>not scientific evidence pollutant higher conce...</td>\n",
              "      <td>also property brick dairy pyramidal roof domin...</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          claim_text  \\\n",
              "0  not scientific evidence pollutant higher conce...   \n",
              "1  not scientific evidence pollutant higher conce...   \n",
              "2  not scientific evidence pollutant higher conce...   \n",
              "3  not scientific evidence pollutant higher conce...   \n",
              "4  not scientific evidence pollutant higher conce...   \n",
              "\n",
              "                                            evidence      label  \n",
              "0  high concentration time atmospheric concentrat...    related  \n",
              "1  plant grow much percent faster concentration p...    related  \n",
              "2  higher carbon dioxide concentration favourably...    related  \n",
              "3                       mine located state jharkhand  unrelated  \n",
              "4  also property brick dairy pyramidal roof domin...  unrelated  "
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def prepareTrainData(n):\n",
        "    train_claims = preprocess_data(train['claim_text'])\n",
        "    processed_train_claim = preprocess_data(train_claims)\n",
        "    claim_lst = []\n",
        "    evidence_lst = []\n",
        "    label_lst = []\n",
        "    for i in range(len(train)):\n",
        "        train_claim = processed_train_claim[i]\n",
        "        evidences = train.iloc[i]['evidences']\n",
        "        for j in evidences:\n",
        "            if j in processed_evidence.index :\n",
        "                claim_lst.append(train_claim)\n",
        "                evidence_lst.append(processed_evidence[j])\n",
        "                label_lst.append('related')\n",
        "        filtered_evi = processed_evidence[~processed_evidence.index.isin(evidences)]\n",
        "        random_evidence = filtered_evi.sample(n)\n",
        "        for k in random_evidence:\n",
        "            claim_lst.append(train_claim)\n",
        "            evidence_lst.append(k)\n",
        "            label_lst.append('unrelated')\n",
        "    claim_evi_label = {'claim_text': claim_lst, 'evidence': evidence_lst, 'label': label_lst}\n",
        "    return pd.DataFrame(claim_evi_label)\n",
        "\n",
        "train_claims = train['claim_text']\n",
        "processed_train_claim = preprocess_data(train_claims)\n",
        "preparedTrain = prepareTrainData(10)\n",
        "preparedTrain.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnNiS5byTvJM"
      },
      "source": [
        "# Two steps for the this task\n",
        "# first. find all relavent evidence, either use contextual embedding or similarity scoring\n",
        "# second. classify the evidents into 4 classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FA2ao2l8hOg"
      },
      "source": [
        "# 2. Model Implementation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "QIEqDDT78q39"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "import torchtext\n",
        "from torchtext.data.utils import get_tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "TfidfVectorizer()"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Vectorizing preprocessed text\n",
        "vectorizer = TfidfVectorizer()\n",
        "all_texts = pd.concat([processed_evidence, processed_train_claim])\n",
        "vectorizer.fit(all_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>claim_text</th>\n",
              "      <th>evidence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>not scientific evidence pollutant higher conce...</td>\n",
              "      <td>high concentration time atmospheric concentrat...</td>\n",
              "      <td>related</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>not scientific evidence pollutant higher conce...</td>\n",
              "      <td>plant grow much percent faster concentration p...</td>\n",
              "      <td>related</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>not scientific evidence pollutant higher conce...</td>\n",
              "      <td>higher carbon dioxide concentration favourably...</td>\n",
              "      <td>related</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>not scientific evidence pollutant higher conce...</td>\n",
              "      <td>mine located state jharkhand</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>not scientific evidence pollutant higher conce...</td>\n",
              "      <td>also property brick dairy pyramidal roof domin...</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>not scientific evidence pollutant higher conce...</td>\n",
              "      <td>sponsored sigda association computing machiner...</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>not scientific evidence pollutant higher conce...</td>\n",
              "      <td>july state kansa franchise failure pay state tax</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>not scientific evidence pollutant higher conce...</td>\n",
              "      <td>named british explorer sir john franklin peris...</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>not scientific evidence pollutant higher conce...</td>\n",
              "      <td>silver medal member french team</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>not scientific evidence pollutant higher conce...</td>\n",
              "      <td>newport located</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          claim_text  \\\n",
              "0  not scientific evidence pollutant higher conce...   \n",
              "1  not scientific evidence pollutant higher conce...   \n",
              "2  not scientific evidence pollutant higher conce...   \n",
              "3  not scientific evidence pollutant higher conce...   \n",
              "4  not scientific evidence pollutant higher conce...   \n",
              "5  not scientific evidence pollutant higher conce...   \n",
              "6  not scientific evidence pollutant higher conce...   \n",
              "7  not scientific evidence pollutant higher conce...   \n",
              "8  not scientific evidence pollutant higher conce...   \n",
              "9  not scientific evidence pollutant higher conce...   \n",
              "\n",
              "                                            evidence      label  \n",
              "0  high concentration time atmospheric concentrat...    related  \n",
              "1  plant grow much percent faster concentration p...    related  \n",
              "2  higher carbon dioxide concentration favourably...    related  \n",
              "3                       mine located state jharkhand  unrelated  \n",
              "4  also property brick dairy pyramidal roof domin...  unrelated  \n",
              "5  sponsored sigda association computing machiner...  unrelated  \n",
              "6   july state kansa franchise failure pay state tax  unrelated  \n",
              "7  named british explorer sir john franklin peris...  unrelated  \n",
              "8                    silver medal member french team  unrelated  \n",
              "9                                    newport located  unrelated  "
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "preparedTrain.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0        [not, scientific, evidence, pollutant, higher,...\n",
            "1        [not, scientific, evidence, pollutant, higher,...\n",
            "2        [not, scientific, evidence, pollutant, higher,...\n",
            "3        [not, scientific, evidence, pollutant, higher,...\n",
            "4        [not, scientific, evidence, pollutant, higher,...\n",
            "                               ...                        \n",
            "16396    [sending, oscillating, microwave, antenna, ins...\n",
            "16397    [sending, oscillating, microwave, antenna, ins...\n",
            "16398    [sending, oscillating, microwave, antenna, ins...\n",
            "16399    [sending, oscillating, microwave, antenna, ins...\n",
            "16400    [sending, oscillating, microwave, antenna, ins...\n",
            "Length: 16401, dtype: object\n",
            "0        [high, concentration, time, atmospheric, conce...\n",
            "1        [plant, grow, much, percent, faster, concentra...\n",
            "2        [higher, carbon, dioxide, concentration, favou...\n",
            "3                        [mine, located, state, jharkhand]\n",
            "4        [also, property, brick, dairy, pyramidal, roof...\n",
            "                               ...                        \n",
            "16396            [company, founded, headquartered, munich]\n",
            "16397    [society, founder, included, kosta, shahov, ch...\n",
            "16398    [make, isotope, suitable, usage, radioisotope,...\n",
            "16399    [ibm, weighs, many, aspect, inventor, contribu...\n",
            "16400    [cantonment, general, hospital, yusra, general...\n",
            "Length: 16401, dtype: object\n",
            "0          related\n",
            "1          related\n",
            "2          related\n",
            "3        unrelated\n",
            "4        unrelated\n",
            "           ...    \n",
            "16396    unrelated\n",
            "16397    unrelated\n",
            "16398    unrelated\n",
            "16399    unrelated\n",
            "16400    unrelated\n",
            "Name: label, Length: 16401, dtype: object\n"
          ]
        }
      ],
      "source": [
        "def tokenize(data):\n",
        "    token_txt = []\n",
        "    for txt in data:\n",
        "        tokens = word_tokenize(txt)\n",
        "        token_txt.append(tokens)\n",
        "    return pd.Series(token_txt)\n",
        "        \n",
        "train_claim_txt = tokenize(preparedTrain['claim_text'])\n",
        "train_evidence_txt = tokenize(preparedTrain['evidence'])\n",
        "train_label = preparedTrain['label']\n",
        "\n",
        "print(train_claim_txt)\n",
        "print(train_evidence_txt)\n",
        "print(train_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "#TODO: vectorise and batchify data\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.memory_format memory_format, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Input \u001b[0;32mIn [67]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m nhead \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;66;03m# the number of heads in the multiheadattention models\u001b[39;00m\n\u001b[1;32m     15\u001b[0m dropout \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m \u001b[38;5;66;03m# the dropout value\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mTransformerClassificationModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mntokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memsize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnhead\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnhid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnlayers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n",
            "Input \u001b[0;32mIn [24]\u001b[0m, in \u001b[0;36mTransformerClassificationModel.__init__\u001b[0;34m(self, ntoken, ninp, nhead, nhid, nlayers, num_classes, dropout)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(ntoken, ninp)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mninp \u001b[38;5;241m=\u001b[39m ninp\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassification_head \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mninp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# added classification head\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_weights()\n",
            "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py:98\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_features \u001b[38;5;241m=\u001b[39m in_features\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_features \u001b[38;5;241m=\u001b[39m out_features\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m Parameter(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_features\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfactory_kwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bias:\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m=\u001b[39m Parameter(torch\u001b[38;5;241m.\u001b[39mempty(out_features, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs))\n",
            "\u001b[0;31mTypeError\u001b[0m: empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.memory_format memory_format, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"
          ]
        }
      ],
      "source": [
        "bptt = 35\n",
        "# TODO: modify this\n",
        "def get_batch(source, i):\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].view(-1)\n",
        "    return data, target\n",
        "\n",
        "\n",
        "ntokens = 0 # TODO: get num of vocabs\n",
        "emsize = 200 # embedding dimension\n",
        "nhid = 200 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
        "nlayers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
        "nhead = 2 # the number of heads in the multiheadattention models\n",
        "dropout = 0.2 # the dropout value\n",
        "model = TransformerClassificationModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 5.0 # learning rate\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
        "\n",
        "import time\n",
        "\n",
        "def train(train_data, model):\n",
        "\n",
        "    model.train() # Turn on the train mode\n",
        "    for batch in train_data:\n",
        "        inputs, labels = batch.text, batch.label\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def evaluate(eval_model, data_source):\n",
        "    return NotImplementedError\n",
        "    #TODO: implement this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time:  0.17s | valid loss 24.26 | valid ppl 34207299015.88\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time:  0.32s | valid loss 24.30 | valid ppl 35673099810.18\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time:  0.12s | valid loss 24.33 | valid ppl 36926992824.35\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "best_val_loss = float(\"inf\")\n",
        "epochs = 3 # The number of epochs\n",
        "best_model = None\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train()\n",
        "    val_loss = evaluate(model, val_data)\n",
        "    print('-' * 89)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                     val_loss, math.exp(val_loss)))\n",
        "    print('-' * 89)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_model = model\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=========================================================================================\n",
            "| End of training | test loss 24.55 | test ppl 45957962903.59\n",
            "=========================================================================================\n",
            "tensor([[[ 2.5528e+01,  4.8138e-01, -9.5534e-01,  ..., -8.1160e-01,\n",
            "          -1.5472e+00, -1.2553e-01],\n",
            "         [ 2.4752e+01, -5.8802e-01, -1.0957e+00,  ..., -6.6444e-01,\n",
            "          -4.8647e-01, -1.5684e-01],\n",
            "         [ 2.4639e+01, -4.5157e-01, -1.7419e+00,  ..., -6.2768e-01,\n",
            "          -1.0963e+00, -8.3802e-01],\n",
            "         ...,\n",
            "         [ 2.5265e+01,  2.4933e-01, -9.0049e-01,  ..., -8.4550e-01,\n",
            "          -2.0898e+00,  8.6930e-02],\n",
            "         [ 2.4738e+01, -1.5027e-01, -5.6473e-01,  ..., -6.1347e-01,\n",
            "          -1.5001e+00,  1.8570e-03],\n",
            "         [ 2.5788e+01, -2.9468e-01, -1.4228e+00,  ..., -2.7725e-01,\n",
            "          -1.2456e+00, -7.7090e-02]],\n",
            "\n",
            "        [[ 2.6233e+01,  5.0844e-01, -1.9147e+00,  ..., -8.2978e-01,\n",
            "          -1.0774e+00,  3.2543e-01],\n",
            "         [ 2.3195e+01,  1.4552e-01, -1.2921e+00,  ..., -3.7468e-01,\n",
            "          -1.2571e+00, -2.6772e-01],\n",
            "         [ 2.5951e+01, -1.2119e-01, -1.2862e+00,  ..., -9.3127e-01,\n",
            "          -7.7190e-01, -5.1595e-01],\n",
            "         ...,\n",
            "         [ 2.5483e+01,  2.5675e-01, -9.0389e-01,  ..., -3.9464e-01,\n",
            "          -1.4428e+00, -1.5663e-01],\n",
            "         [ 2.4634e+01, -4.5482e-02, -1.0881e+00,  ..., -3.7190e-01,\n",
            "          -1.1639e+00, -7.7284e-01],\n",
            "         [ 2.5767e+01,  3.4480e-01, -8.5995e-01,  ..., -1.2234e+00,\n",
            "          -4.2391e-01, -3.1967e-01]],\n",
            "\n",
            "        [[ 2.4848e+01,  3.9569e-01, -9.7287e-01,  ..., -7.3555e-01,\n",
            "          -1.4070e+00, -2.3174e-02],\n",
            "         [ 2.5126e+01, -5.1890e-02, -1.3187e+00,  ..., -7.3097e-01,\n",
            "          -1.3012e+00,  1.2924e-01],\n",
            "         [ 2.4996e+01,  2.1124e-01, -1.0907e+00,  ..., -7.9852e-01,\n",
            "          -1.3421e+00, -2.0890e-01],\n",
            "         ...,\n",
            "         [ 2.5427e+01,  4.1084e-01, -8.4842e-01,  ..., -6.2373e-01,\n",
            "          -1.6471e+00,  3.1525e-01],\n",
            "         [ 2.5118e+01,  2.6368e-01, -8.7681e-01,  ..., -9.8813e-01,\n",
            "          -1.8973e+00, -2.9467e-01],\n",
            "         [ 2.5775e+01,  4.3863e-01, -1.1651e+00,  ..., -1.0177e+00,\n",
            "          -9.7085e-01, -1.2382e-02]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 2.5766e+01,  6.0259e-01, -1.2204e+00,  ..., -3.2086e-01,\n",
            "          -8.0356e-01, -3.3619e-01],\n",
            "         [ 2.4426e+01,  3.6838e-01, -1.0906e+00,  ..., -5.3073e-01,\n",
            "          -2.1529e-01, -6.3200e-01],\n",
            "         [ 2.5514e+01,  4.2922e-01, -1.8967e+00,  ..., -5.6550e-01,\n",
            "          -1.0270e+00, -3.6943e-01],\n",
            "         ...,\n",
            "         [ 2.5356e+01,  1.0432e+00, -9.9344e-01,  ..., -3.5539e-01,\n",
            "          -8.9892e-01, -6.9772e-01],\n",
            "         [ 2.5469e+01,  8.3482e-01, -1.3789e+00,  ..., -6.7650e-01,\n",
            "          -1.0476e+00,  3.0387e-02],\n",
            "         [ 2.5784e+01,  2.0008e-01, -1.4451e+00,  ..., -7.0025e-01,\n",
            "          -6.8246e-01, -9.3276e-01]],\n",
            "\n",
            "        [[ 2.5607e+01,  2.5324e-01, -1.2250e+00,  ..., -8.8800e-01,\n",
            "          -8.8794e-01, -2.9003e-01],\n",
            "         [ 2.4339e+01, -2.0623e-01, -1.2854e+00,  ..., -5.8626e-01,\n",
            "          -6.0547e-01, -3.5449e-01],\n",
            "         [ 2.5358e+01,  8.2611e-01, -1.2856e+00,  ..., -4.8013e-01,\n",
            "          -1.1426e+00, -2.6568e-01],\n",
            "         ...,\n",
            "         [ 2.6140e+01,  7.1328e-01, -2.0277e+00,  ..., -1.0106e+00,\n",
            "          -8.6191e-01,  9.1806e-02],\n",
            "         [ 2.5372e+01,  5.7027e-01, -1.2030e+00,  ..., -8.7615e-01,\n",
            "          -5.0101e-01, -8.8587e-01],\n",
            "         [ 2.5469e+01,  6.2199e-01, -1.2688e+00,  ..., -4.3282e-01,\n",
            "          -4.4318e-01, -3.5595e-01]],\n",
            "\n",
            "        [[ 2.4781e+01,  4.5633e-02, -1.6101e+00,  ..., -6.5122e-01,\n",
            "          -7.0790e-01, -3.3126e-01],\n",
            "         [ 2.5459e+01,  3.0029e-01, -1.0558e+00,  ..., -1.1703e+00,\n",
            "          -3.9300e-01, -6.0333e-01],\n",
            "         [ 2.5620e+01,  4.9821e-01, -1.3108e+00,  ..., -8.1542e-01,\n",
            "          -1.4560e+00, -3.8620e-01],\n",
            "         ...,\n",
            "         [ 2.4689e+01,  5.7652e-01, -1.1943e+00,  ..., -9.1920e-01,\n",
            "          -1.2086e+00, -3.3002e-01],\n",
            "         [ 2.4775e+01,  5.9370e-01, -2.0045e+00,  ..., -9.8481e-01,\n",
            "          -5.3507e-01, -6.0070e-01],\n",
            "         [ 2.4588e+01,  7.8581e-01, -1.3791e+00,  ..., -8.0688e-01,\n",
            "          -6.0558e-01, -7.4402e-01]]], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "test_loss = evaluate(best_model, test_data)\n",
        "\n",
        "\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
        "    test_loss, math.exp(test_loss)))\n",
        "print('=' * 89)\n",
        "\n",
        "\n",
        "print(best_model(example))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzGuzHPE87Ya"
      },
      "source": [
        "# 3.Testing and Evaluation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZVeNYIH9IaL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mefSOe8eTmGP"
      },
      "source": [
        "## Object Oriented Programming codes here\n",
        "\n",
        "*You can use multiple code snippets. Just add more if needed*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# model adpated from workshop 8\n",
        "class TransformerClassificationModel(nn.Module):\n",
        "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, num_classes, dropout=0.5):\n",
        "        super(TransformerClassificationModel, self).__init__()\n",
        "        from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "        self.model_type = 'Transformer'\n",
        "        self.src_mask = None\n",
        "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
        "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.encoder = nn.Embedding(ntoken, ninp)\n",
        "        self.ninp = ninp\n",
        "        self.classification_head = nn.Linear(ninp, num_classes)  # added classification head\n",
        "        self.init_weights()\n",
        "\n",
        "    def _generate_square_subsequent_mask(self, sz):\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.classification_head.bias.data.zero_()\n",
        "        self.classification_head.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src):\n",
        "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
        "            device = src.device\n",
        "            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
        "            self.src_mask = mask\n",
        "\n",
        "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
        "        src = self.pos_encoder(src)\n",
        "        output = self.transformer_encoder(src, self.src_mask)\n",
        "        output = output.mean(dim=0)  # aggregate across all tokens\n",
        "        output = self.classification_head(output) \n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term) #0::2 means starting with index 0, step = 2\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
