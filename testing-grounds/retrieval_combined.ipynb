{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32yCsRUo8H33"
      },
      "source": [
        "# 2024 COMP90042 Project\n",
        "*Make sure you change the file name with your group id.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCybYoGz8YWQ"
      },
      "source": [
        "# Readme\n",
        "*If there is something to be noted for the marker, please mention here.*\n",
        "\n",
        "*If you are planning to implement a program with Object Oriented Programming style, please put those the bottom of this ipynb file*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6po98qVA8bJD"
      },
      "source": [
        "# 1.DataSet Processing\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /Users/kaiyuancui/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "#### word embedding pipeline\n",
        "\n",
        "from gensim.utils import deaccent\n",
        "from nltk import pos_tag\n",
        "from nltk import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "import re\n",
        "import time\n",
        "\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "import pickle\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "#\n",
        "LOAD_FILES = True\n",
        "\n",
        "#d_evidence = pd.read_json(\"data/evidence.json\", typ='series')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# contraction_dict from WS7\n",
        "contraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\",\n",
        "                    \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\",\n",
        "                    \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\",\n",
        "                    \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\",\n",
        "                    \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\",\n",
        "                    \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
        "                    \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\",\n",
        "                    \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\n",
        "                    \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
        "                    \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\",\n",
        "                    \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\",\n",
        "                    \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\n",
        "                    \"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
        "                    \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
        "                    \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\",\n",
        "                    \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\",\n",
        "                    \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\",\n",
        "                    \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\",\n",
        "                    \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\",\n",
        "                    \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\",\n",
        "                    \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "                    \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
        "\n",
        "\n",
        "# https://stackoverflow.com/a/46231553\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return None # for easy if-statement \n",
        "\n",
        "\n",
        "def sentence_preprocessing(sentence):\n",
        "\n",
        "    out_list = []\n",
        "    # Use gensim deaccent to match more characters to [a-z]\n",
        "    sentence = deaccent(sentence.lower())\n",
        "\n",
        "    for old, new in contraction_dict.items():\n",
        "        sentence.replace(old, new)\n",
        "\n",
        "    tokenized = word_tokenize(sentence)\n",
        "\n",
        "    # now remove all tokens that don't contain any alphanumeric characters\n",
        "    # then strip non alphanumeric characters afterwards\n",
        "    tokenized = [re.sub(r\"[^a-z0-9\\s]\", \"\", token) for token in tokenized if re.match(r\"[a-z0-9\\s]\", token)]\n",
        "\n",
        "    # now lemmatize with pos\n",
        "    tagged = pos_tag(tokenized)\n",
        "    for token, tag in tagged:\n",
        "        wntag = get_wordnet_pos(tag)\n",
        "\n",
        "        if wntag is None: # do not supply tag in case of None\n",
        "            lemma = lemmatizer.lemmatize(token) \n",
        "        else:\n",
        "            lemma = lemmatizer.lemmatize(token, pos=wntag) \n",
        "\n",
        "        out_list.append(lemma)\n",
        "    \n",
        "    return out_list\n",
        "\n",
        "\n",
        "def evidence_preprocessing(evidences):\n",
        "  t = time.time()\n",
        "  processed = []\n",
        "  for index, item in enumerate(evidences.items()):\n",
        "    id, evidence = item\n",
        "\n",
        "    row = []\n",
        "    \n",
        "    row.append(id)\n",
        "    row.append(evidence)\n",
        "\n",
        "    # break the text into sentences before tokenizing by each sentence\n",
        "    processed_sentences = [sentence_preprocessing(sentence) for sentence in sent_tokenize(evidence)]\n",
        "    row.append(processed_sentences)\n",
        "\n",
        "\n",
        "    # Appending an empty list to populate with embeddings later\n",
        "    row.append([])\n",
        "\n",
        "    processed.append(row)\n",
        "\n",
        "    if (index + 1) % 50000 == 0:\n",
        "        print(f\"{time.time() - t:.2f} - {index+1} rows processed\")\n",
        "\n",
        "  return pd.DataFrame(processed, columns = [\"id\", \"raw evidence\", \"processed evidence\", \"embeddings\"])\n",
        "\n",
        "\n",
        "# Evidence processing\n",
        "if not LOAD_FILES:\n",
        "    evidence = evidence_preprocessing(d_evidence)\n",
        "    with open(\"../pipeline/evidence_preprocessed_v3.pkl\", \"wb\") as f:\n",
        "        pickle.dump(evidence, f)\n",
        "else:\n",
        "    with open(\"../pipeline/evidence_preprocessed_v3.pkl\", \"rb\") as f:\n",
        "        evidence = pickle.load(f)\n",
        "    \n",
        "    evidence.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /Users/kaiyuancui/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "from gensim.utils import deaccent\n",
        "from nltk import pos_tag\n",
        "from nltk import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "import re\n",
        "import time\n",
        "\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "import pickle\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "#d_evidence = pd.read_json(\"data/evidence.json\", typ='series')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# contraction_dict from WS7\n",
        "contraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\",\n",
        "                    \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\",\n",
        "                    \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\",\n",
        "                    \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\",\n",
        "                    \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\",\n",
        "                    \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
        "                    \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\",\n",
        "                    \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\n",
        "                    \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
        "                    \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\",\n",
        "                    \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\",\n",
        "                    \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\n",
        "                    \"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
        "                    \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
        "                    \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\",\n",
        "                    \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\",\n",
        "                    \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\",\n",
        "                    \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\",\n",
        "                    \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\",\n",
        "                    \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\",\n",
        "                    \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "                    \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# https://stackoverflow.com/a/46231553\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return None # for easy if-statement \n",
        "    \n",
        "def sentence_preprocessing(sentence):\n",
        "\n",
        "    out_list = []\n",
        "    # Use gensim deaccent to match more characters to [a-z]\n",
        "    sentence = deaccent(sentence.lower())\n",
        "\n",
        "    for old, new in contraction_dict.items():\n",
        "        sentence.replace(old, new)\n",
        "\n",
        "    tokenized = word_tokenize(sentence)\n",
        "\n",
        "    # now remove all tokens that don't contain any alphanumeric characters\n",
        "    # then strip non alphanumeric characters afterwards\n",
        "    tokenized = [re.sub(r\"[^a-z0-9\\s]\", \"\", token) for token in tokenized if re.match(r\"[a-z0-9\\s]\", token)]\n",
        "\n",
        "    # now lemmatize with pos\n",
        "    tagged = pos_tag(tokenized)\n",
        "    for token, tag in tagged:\n",
        "        wntag = get_wordnet_pos(tag)\n",
        "\n",
        "        if wntag is None: # do not supply tag in case of None\n",
        "            lemma = lemmatizer.lemmatize(token) \n",
        "        else:\n",
        "            lemma = lemmatizer.lemmatize(token, pos=wntag) \n",
        "\n",
        "        out_list.append(lemma)\n",
        "    \n",
        "    return out_list\n",
        "\n",
        "\n",
        "# https://huggingface.co/learn/nlp-course/chapter6/6\n",
        "def encode_word(word):\n",
        "    tokens = []\n",
        "    while len(word) > 0:\n",
        "        i = len(word)\n",
        "        while i > 0 and word[:i] not in vocab:\n",
        "            i -= 1\n",
        "        if i == 0:\n",
        "            return [\"[UNK]\"]\n",
        "        tokens.append(word[:i])\n",
        "        word = word[i:]\n",
        "        if len(word) > 0:\n",
        "            word = f\"##{word}\"\n",
        "    return tokens\n",
        "\n",
        "# adapted from https://huggingface.co/learn/nlp-course/chapter6/6\n",
        "def tokenize(sentence):\n",
        "\n",
        "    # janky workaround for preprocessed sentences\n",
        "    if type(sentence) is not list:\n",
        "        sentence = sentence_preprocessing(sentence)\n",
        "        \n",
        "    encoded_words = [encode_word(word) for word in sentence]\n",
        "    return sum(encoded_words, [])\n",
        "\n",
        "\n",
        "\n",
        "with open(\"../pipeline/BPETokenizer_merge_rules_v1.5.pkl\", \"rb\") as f:\n",
        "    merge_rules = pickle.load(f)\n",
        "    \n",
        "\n",
        "# Reconstruct vocab from merge rules due to lack of foresight\n",
        "# This grabs all vocab of length 2 or above (if contains first letter)\n",
        "# or 4 or above (##__)\n",
        "vocab = [v for v in merge_rules.values()]\n",
        "\n",
        "# So iterate through merge rules again to find starting letters\n",
        "# and one letter suffixes\n",
        "for pair, merge in merge_rules.items():\n",
        "    if len(pair[0]) == 1 and pair[0] not in vocab:\n",
        "        vocab.append(pair[0])\n",
        "    if len(pair[1]) == 3 and pair[1] not in vocab:\n",
        "        vocab.append(pair[1])\n",
        "\n",
        "\n",
        "def processed_evidence_to_bpe(paragraph):\n",
        "    # 2d array -> paragraph\n",
        "    if type(paragraph[0]) is list:\n",
        "        return [tokenize(sentence) for sentence in paragraph]\n",
        "\n",
        "    # 1 sentence -> tokenize as is \n",
        "    else:\n",
        "        return tokenize(paragraph)\n",
        "\n",
        "\n",
        "counter = 0\n",
        "def processed_evidence_to_bpe(paragraph):\n",
        "    global counter\n",
        "    counter += 1\n",
        "    if counter % 1000 == 0:\n",
        "        print(f\"{counter} rows processed\")\n",
        "    #2d array -> paragraph\n",
        "    if type(paragraph[0]) is list:\n",
        "        return [tokenize(sentence) for sentence in paragraph]\n",
        "\n",
        "    # 1 sentence -> tokenize as is \n",
        "    else:\n",
        "        return tokenize(paragraph)\n",
        "\n",
        "\n",
        "# Save\n",
        "\n",
        "\"\"\"\n",
        "e[\"bpe evidence\"] = e[\"processed evidence\"].apply(processed_evidence_to_bpe)\n",
        "with open(\"BPETokenized_evidence_v3.pkl\", \"wb\") as f:\n",
        "    pickle.dump(e, f)\n",
        "\"\"\"\n",
        "\n",
        "# Load\n",
        "with open(\"../pipeline/BPETokenized_evidence_v3.pkl\", \"rb\") as f:\n",
        "    evidence = pickle.load(f)\n",
        "\n",
        "\"\"\"\n",
        "sentences = []\n",
        "\n",
        "for paragraph in evidence[\"bpe evidence\"]:\n",
        "    if type(paragraph[0]) is list:\n",
        "        for sentence in paragraph:\n",
        "            sentences.append(sentence)\n",
        "    else:\n",
        "        sentences.append(paragraph)\n",
        "\"\"\"\n",
        "\n",
        "# Now do word2vec\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "\n",
        "EMBEDDING_DIM = 200\n",
        "\"\"\"\n",
        "embedding_model = Word2Vec(sentences=sentences,\n",
        "                           vector_size=EMBEDDING_DIM,\n",
        "                           window=4,\n",
        "                           min_count=3,\n",
        "                           workers=10,\n",
        "                           negative=5\n",
        "                           )\n",
        "\n",
        "version = 3\n",
        "with open(f\"BPE Tokenizer to embedding/embeddings_BPE_v{version}.pkl\", \"wb\") as f:\n",
        "    pickle.dump(embedding_model, f)\n",
        "\"\"\"\n",
        "\n",
        "# Load embedding\n",
        "with open(\"../pipeline/embeddings_BPE_v3.pkl\", \"rb\") as f:\n",
        "    embedding_model = pickle.load(f)\n",
        "\n",
        "import numpy as np\n",
        "def sentence_embedding(sentence):\n",
        "\n",
        "  # Failsafe\n",
        "  if len(sentence) == 0:\n",
        "    return np.zeros(EMBEDDING_DIM)\n",
        "\n",
        "  if type(sentence[0]) is not list:\n",
        "      sentence = tokenize(sentence)\n",
        "\n",
        "  # Check again here as sentence list can be empty after tokenisation (e.g. sentence = '  ')\n",
        "  if len(sentence) == 0:\n",
        "    return np.zeros(EMBEDDING_DIM)\n",
        "\n",
        "\n",
        "  embedding = np.zeros(EMBEDDING_DIM)\n",
        "  for word in sentence:\n",
        "    word_embedding = np.zeros(EMBEDDING_DIM)\n",
        "\n",
        "    # get word vector for given word\n",
        "    # if not found, ignore (treat as having the zero vector)\n",
        "    try:\n",
        "      word_embedding = embedding_model.wv[str(word)]\n",
        "    except KeyError:\n",
        "      pass\n",
        "\n",
        "    embedding += word_embedding\n",
        "\n",
        "  return embedding / len(sentence)\n",
        "\n",
        "\n",
        "def paragraph_embedding(paragraph):\n",
        "    out = []\n",
        "\n",
        "    # One sentence\n",
        "    if type(paragraph[0]) is not list:\n",
        "        return [sentence_embedding(paragraph)]\n",
        "\n",
        "    else:\n",
        "        for sentence in paragraph:\n",
        "            out.append(sentence_embedding(sentence))\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Baseline retrieval: immediately use the raw embeddings to retrieve closest sentences\n",
        "# Train a cutoff distance threshold.\n",
        "\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "# Similarity based on cosine similarity ([0-1], higher the more similar)\n",
        "def similarity(text, evidence_ids):\n",
        "\n",
        "    # Seems stupid and retrieving everything from w2v is probably cleaner\n",
        "    # TODO: make this better\n",
        "    evidence_embeddings = [evidence.loc[evidence['id'] == id, 'embeddings'].values[0] for id in evidence_ids]\n",
        "    key_embedding = sentence_embedding(text)\n",
        "    \n",
        "    similarities = []\n",
        "    for evidence_embedding in evidence_embeddings:\n",
        "        similarities.append(1-cosine(key_embedding, evidence_embedding))\n",
        "\n",
        "    return similarities\n",
        "\n",
        "\n",
        "# Using 1 - fscore as the loss\n",
        "def retrieval_loss(prediction, target):\n",
        "    numerator = 0\n",
        "    denominator = 0\n",
        "    \n",
        "    for p in prediction:\n",
        "        if p in target:\n",
        "            denominator += 2\n",
        "            numerator += 2\n",
        "        else:\n",
        "            denominator += 1\n",
        "    \n",
        "    for t in target:\n",
        "        if t not in prediction:\n",
        "            denominator += 1\n",
        "    \n",
        "    return 1 - numerator/denominator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvff21Hv8zjk",
        "outputId": "4aa2923a-2e47-4e05-c9cc-42fce5e6a04d"
      },
      "outputs": [],
      "source": [
        "LOCAL_DEV = True # to switch between developing locally and on colab\n",
        "\n",
        "if not LOCAL_DEV:\n",
        "    # TODO: need to upload data files on Google Drive?\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: contractions in /Users/kaiyuancui/opt/anaconda3/lib/python3.9/site-packages (0.1.73)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /Users/kaiyuancui/opt/anaconda3/lib/python3.9/site-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: pyahocorasick in /Users/kaiyuancui/opt/anaconda3/lib/python3.9/site-packages (from textsearch>=0.0.21->contractions) (2.1.0)\n",
            "Requirement already satisfied: anyascii in /Users/kaiyuancui/opt/anaconda3/lib/python3.9/site-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install contractions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "MH1Hdrb5NmHM"
      },
      "outputs": [],
      "source": [
        "#Imports\n",
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "2piZvV4OMSa3",
        "outputId": "b17e35ee-0c98-48b9-aa69-f6b63d336059"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>claim_text</th>\n",
              "      <th>claim_label</th>\n",
              "      <th>evidences</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>claim-1937</th>\n",
              "      <td>Not only is there no scientific evidence that ...</td>\n",
              "      <td>DISPUTED</td>\n",
              "      <td>[evidence-442946, evidence-1194317, evidence-1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-126</th>\n",
              "      <td>El Niño drove record highs in global temperatu...</td>\n",
              "      <td>REFUTES</td>\n",
              "      <td>[evidence-338219, evidence-1127398]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-2510</th>\n",
              "      <td>In 1946, PDO switched to a cool phase.</td>\n",
              "      <td>SUPPORTS</td>\n",
              "      <td>[evidence-530063, evidence-984887]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-2021</th>\n",
              "      <td>Weather Channel co-founder John Coleman provid...</td>\n",
              "      <td>DISPUTED</td>\n",
              "      <td>[evidence-1177431, evidence-782448, evidence-5...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-2449</th>\n",
              "      <td>\"January 2008 capped a 12 month period of glob...</td>\n",
              "      <td>NOT_ENOUGH_INFO</td>\n",
              "      <td>[evidence-1010750, evidence-91661, evidence-72...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   claim_text  \\\n",
              "claim-1937  Not only is there no scientific evidence that ...   \n",
              "claim-126   El Niño drove record highs in global temperatu...   \n",
              "claim-2510             In 1946, PDO switched to a cool phase.   \n",
              "claim-2021  Weather Channel co-founder John Coleman provid...   \n",
              "claim-2449  \"January 2008 capped a 12 month period of glob...   \n",
              "\n",
              "                claim_label                                          evidences  \n",
              "claim-1937         DISPUTED  [evidence-442946, evidence-1194317, evidence-1...  \n",
              "claim-126           REFUTES                [evidence-338219, evidence-1127398]  \n",
              "claim-2510         SUPPORTS                 [evidence-530063, evidence-984887]  \n",
              "claim-2021         DISPUTED  [evidence-1177431, evidence-782448, evidence-5...  \n",
              "claim-2449  NOT_ENOUGH_INFO  [evidence-1010750, evidence-91661, evidence-72...  "
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#visualising training data\n",
        "if LOCAL_DEV:\n",
        "    train = pd.read_json(\"../data/train-claims.json\") # for local dev\n",
        "    \n",
        "else:\n",
        "    train = pd.read_json(\"/content/drive/MyDrive/data/train-claims.json\") # on colab\n",
        "train = train.transpose()\n",
        "train.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>claim_text</th>\n",
              "      <th>claim_label</th>\n",
              "      <th>evidences</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>claim-752</th>\n",
              "      <td>[South Australia] has the most expensive elect...</td>\n",
              "      <td>SUPPORTS</td>\n",
              "      <td>[evidence-67732, evidence-572512]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-375</th>\n",
              "      <td>when 3 per cent of total annual global emissio...</td>\n",
              "      <td>NOT_ENOUGH_INFO</td>\n",
              "      <td>[evidence-996421, evidence-1080858, evidence-2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-1266</th>\n",
              "      <td>This means that the world is now 1C warmer tha...</td>\n",
              "      <td>SUPPORTS</td>\n",
              "      <td>[evidence-889933, evidence-694262]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-871</th>\n",
              "      <td>“As it happens, Zika may also be a good model ...</td>\n",
              "      <td>NOT_ENOUGH_INFO</td>\n",
              "      <td>[evidence-422399, evidence-702226, evidence-28...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-2164</th>\n",
              "      <td>Greenland has only lost a tiny fraction of its...</td>\n",
              "      <td>REFUTES</td>\n",
              "      <td>[evidence-52981, evidence-264761, evidence-947...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   claim_text  \\\n",
              "claim-752   [South Australia] has the most expensive elect...   \n",
              "claim-375   when 3 per cent of total annual global emissio...   \n",
              "claim-1266  This means that the world is now 1C warmer tha...   \n",
              "claim-871   “As it happens, Zika may also be a good model ...   \n",
              "claim-2164  Greenland has only lost a tiny fraction of its...   \n",
              "\n",
              "                claim_label                                          evidences  \n",
              "claim-752          SUPPORTS                  [evidence-67732, evidence-572512]  \n",
              "claim-375   NOT_ENOUGH_INFO  [evidence-996421, evidence-1080858, evidence-2...  \n",
              "claim-1266         SUPPORTS                 [evidence-889933, evidence-694262]  \n",
              "claim-871   NOT_ENOUGH_INFO  [evidence-422399, evidence-702226, evidence-28...  \n",
              "claim-2164          REFUTES  [evidence-52981, evidence-264761, evidence-947...  "
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "if LOCAL_DEV:\n",
        "    test = pd.read_json(\"../data/test-claims-unlabelled.json\") # for local dev\n",
        "    dev_data = pd.read_json(\"../data/dev-claims.json\")\n",
        "else:\n",
        "    test = pd.read_json(\"/content/drive/MyDrive/data/test-claims-unlabelled.json\") # on colab\n",
        "test = test.transpose()\n",
        "dev_data = dev_data.transpose()\n",
        "test.head()\n",
        "\n",
        "dev_data.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ZKbGFcA2THHP"
      },
      "outputs": [],
      "source": [
        "#visualising evidence data\n",
        "if LOCAL_DEV:\n",
        "    evidence = pd.read_json(\"../data/evidence.json\",typ='series')\n",
        "else:\n",
        "    evidence = pd.read_json(\"/content/drive/MyDrive/data/evidence.json\",typ='series')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFNTiC0UMS45",
        "outputId": "a55e84f3-4f1c-4039-8fe0-cd2e8f2d13b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1208827\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "evidence-0    John Bennet Lawes, English entrepreneur and ag...\n",
              "evidence-1    Lindberg began his professional career at the ...\n",
              "evidence-2    ``Boston (Ladies of Cambridge)'' by Vampire We...\n",
              "evidence-3    Gerald Francis Goyer (born October 20, 1936) w...\n",
              "evidence-4    He detected abnormalities of oxytocinergic fun...\n",
              "dtype: object"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(len(evidence))\n",
        "evidence.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/kaiyuancui/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/kaiyuancui/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /Users/kaiyuancui/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import string\n",
        "import contractions\n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Xf73PDzRTrft"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1208827\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "claim-2967               contribution waste heat global climate\n",
              "claim-979     warm weather worsened recent drought included ...\n",
              "claim-1609                greenland lost tiny fraction ice mass\n",
              "claim-1020    global reef crisis necessarily mean extinction...\n",
              "claim-2599     small amount active substance cause large effect\n",
              "dtype: object"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def preprocess_data(data: pd.Series) -> pd.Series:\n",
        "  preprocessed_data = {}\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  for id, text in data.items():\n",
        "    text = text.lower()\n",
        "    text = contractions.fix(text)\n",
        "    tokens = word_tokenize(text)\n",
        "    wnl = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [wnl.lemmatize(word) for word in tokens if word.isalpha() and word not in stop_words]\n",
        "    preprocessed_data[id] = \" \".join(lemmatized_tokens)\n",
        "\n",
        "  return pd.Series(preprocessed_data)\n",
        "\n",
        "\n",
        "train_claims = train['claim_text']\n",
        "test_claims = test['claim_text']\n",
        "dev_claims = dev_data['claim_text']\n",
        "processed_evidence = preprocess_data(evidence)\n",
        "processed_test = preprocess_data(test_claims)\n",
        "processed_dev = preprocess_data(dev_claims)\n",
        "print(len(processed_evidence))\n",
        "processed_train_claim = preprocess_data(train_claims)\n",
        "processed_dev_claim = preprocess_data(dev_claims)\n",
        "processed_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "evidence-0    john bennet lawes english entrepreneur agricul...\n",
            "evidence-1    lindberg began professional career age eventua...\n",
            "evidence-2                boston lady cambridge vampire weekend\n",
            "evidence-3    gerald francis goyer born october professional...\n",
            "evidence-4    detected abnormality oxytocinergic function sc...\n",
            "dtype: object\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "1207971"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# remove empty\n",
        "processed_evidence = processed_evidence[processed_evidence.str.strip().str.len() > 0]\n",
        "print(processed_evidence.head())\n",
        "len(processed_evidence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# testing embeddings:\n",
        "# evidence_embeddings = np.array([sentence_embedding(' ') for sentence in processed_evidence[:1000]])\n",
        "# print(evidence_embeddings)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get all evidences that are likely to be relevant using a similarity score\n",
        "\n",
        "# Vectorizing preprocessed text\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "USE_EMBEDDING = False\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "all_texts = pd.concat([processed_evidence, processed_train_claim])\n",
        "vectorizer.fit(all_texts)\n",
        "\n",
        "if not USE_EMBEDDING:\n",
        "    evidence_tfidf = vectorizer.transform(processed_evidence)\n",
        "    test_tfidf = vectorizer.transform(processed_test)\n",
        "    train_tfidf = vectorizer.transform(processed_train_claim)\n",
        "    dev_tfidf = vectorizer.transform(processed_dev)\n",
        "\n",
        "    similarity_matrix = cosine_similarity(test_tfidf, evidence_tfidf)\n",
        "    dev_similarity_matrix = cosine_similarity(dev_tfidf, evidence_tfidf)\n",
        "    train_similarity_matrix = cosine_similarity(train_tfidf, evidence_tfidf)\n",
        "else:\n",
        "    # this takes ~ 1hr to run\n",
        "    evidence_embeddings = np.array([sentence_embedding(sentence) for sentence in processed_evidence])\n",
        "    test_embeddings = np.array([sentence_embedding(sentence) for sentence in processed_test])\n",
        "    train_embeddings = np.array([sentence_embedding(sentence) for sentence in processed_train_claim])\n",
        "    dev_embeddings = np.array([sentence_embedding(sentence) for sentence in processed_dev])\n",
        "\n",
        "    similarity_matrix = cosine_similarity(test_embeddings, evidence_embeddings)\n",
        "    dev_similarity_matrix = cosine_similarity(dev_embeddings, evidence_embeddings)\n",
        "    train_similarity_matrix = cosine_similarity(train_embeddings, evidence_embeddings)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# find the top n most similar evidence\n",
        "TOP_N_TRAIN = 10\n",
        "TOP_N_TEST = 10\n",
        "\n",
        "def getTopN(similarity_matrix, test, evidence, n):\n",
        "    test = test.to_frame(name='claim_text')\n",
        "    top_indices = np.argsort(-similarity_matrix, axis = 1)[:, :n]\n",
        "    top_evidence = [[str(evidence.index[i]) for i in row] for row in top_indices]\n",
        "    test['evidences'] = top_evidence\n",
        "\n",
        "    \n",
        "    return test\n",
        "\n",
        "test_with_evi = getTopN(similarity_matrix, processed_test, processed_evidence, TOP_N_TRAIN)\n",
        "test_with_evi.head()\n",
        "\n",
        "dev_with_evi = getTopN(dev_similarity_matrix, processed_dev, processed_evidence, TOP_N_TRAIN)\n",
        "train_with_evi = getTopN(train_similarity_matrix, processed_train_claim, processed_evidence, TOP_N_TRAIN)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get evidence with different values of N\n",
        "\n",
        "EVALUATE_TOP_N = False\n",
        "\n",
        "if EVALUATE_TOP_N:\n",
        "    TOP_N_VALUES = [5, 10, 20, 50, 100, 200]  # Values of N to iterate over\n",
        "    top_train_results = {n: getTopN(train_similarity_matrix, processed_train_claim, processed_evidence, n) for n in TOP_N_VALUES}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# check effect of N on the list of evidence found\n",
        "\n",
        "\n",
        "def evaluate_top_n(similar_claim_evidence, true_claim_evidence, top_n):\n",
        "    # Dictionary to store analysis results\n",
        "    analysis_results = {\n",
        "        'true_evidence_count': 0,\n",
        "        'true_evidence_found': 0,\n",
        "        'unrelated_evidence_found': 0,\n",
        "        'claims_with_all_evidence': 0\n",
        "    }\n",
        "\n",
        "    for index, row in true_claim_evidence.iterrows():\n",
        "        if index in similar_claim_evidence.index:\n",
        "            similar_claim_evidence_row = similar_claim_evidence.loc[index]\n",
        "        else:\n",
        "            print(f\"{index} NOT FOUND IN TOP N EVIDENCE!\")\n",
        "            print(similar_claim_evidence.index)\n",
        "            continue\n",
        "        \n",
        "        true_evidence_list = row['evidences']\n",
        "        similar_evidence_list = similar_claim_evidence_row['evidences']\n",
        "        false_evidence_list = list(set(similar_evidence_list) - set(true_evidence_list))\n",
        "\n",
        "        analysis_results['true_evidence_count'] += len(true_evidence_list)\n",
        "        analysis_results['unrelated_evidence_found'] += len(false_evidence_list)\n",
        "        analysis_results['true_evidence_found'] += len(similar_evidence_list) - len(false_evidence_list)\n",
        "        \n",
        "        if len(similar_evidence_list) - len(false_evidence_list) == len(true_evidence_list):\n",
        "            analysis_results['claims_with_all_evidence'] += 1\n",
        "\n",
        "    return analysis_results\n",
        "\n",
        "if EVALUATE_TOP_N:\n",
        "\n",
        "    evaluation_results = {n: evaluate_top_n(top_train_results[n], train, n) for n in TOP_N_VALUES}\n",
        "\n",
        "    print(evaluation_results)\n",
        "\n",
        "    evidence_found_percentage = [evaluation_results[n]['true_evidence_found'] / evaluation_results[n]['true_evidence_count'] *100 for n in TOP_N_VALUES]\n",
        "\n",
        "    related_ratio = [evaluation_results[n]['true_evidence_found'] / (evaluation_results[n]['unrelated_evidence_found'] + evaluation_results[n]['true_evidence_found'] )  * 100 for n in TOP_N_VALUES]\n",
        "\n",
        "    claims_with_all_evidence_percentage = [evaluation_results[n]['claims_with_all_evidence'] / len(processed_train_claim) *100 for n in TOP_N_VALUES]\n",
        "    print(len(processed_train_claim))\n",
        "    print(evaluation_results[20]['claims_with_all_evidence'])\n",
        "\n",
        "\n",
        "    plt.plot(TOP_N_VALUES, related_ratio, marker='o')\n",
        "    plt.title('% Evidence in Top-N Related to Claim')\n",
        "    plt.xlabel('Top N')\n",
        "    plt.ylabel('%')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    plt.plot(TOP_N_VALUES, evidence_found_percentage, marker='o')\n",
        "    plt.title('% of Related Evidence Found By Top-N')\n",
        "    plt.xlabel('Top N')\n",
        "    plt.ylabel('%')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(TOP_N_VALUES, claims_with_all_evidence_percentage, marker='o')\n",
        "    plt.title('% of Claims With All Related Evidence in Top-N')\n",
        "    plt.xlabel('Top N')\n",
        "    plt.ylabel('%')\n",
        "\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "yHWbo7W7TuUC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "evidence-248690may be empty/ not in english, skipping..\n",
            "true evidence count 4122\n",
            "true evidence found in top 10: 584\n",
            "unrelated evidence found in top 10: 11696\n",
            "# claims with all evidence in top 10: 58\n",
            "true evidence count 491\n",
            "true evidence found in top 10: 72\n",
            "unrelated evidence found in top 10: 1468\n",
            "# claims with all evidence in top 10: 7\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Not only is there no scientific evidence that ...</td>\n",
              "      <td>related</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Not only is there no scientific evidence that ...</td>\n",
              "      <td>related</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Not only is there no scientific evidence that ...</td>\n",
              "      <td>related</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Not only is there no scientific evidence that ...</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Not only is there no scientific evidence that ...</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12274</th>\n",
              "      <td>Sending oscillating microwaves from an antenna...</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12275</th>\n",
              "      <td>Sending oscillating microwaves from an antenna...</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12276</th>\n",
              "      <td>Sending oscillating microwaves from an antenna...</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12277</th>\n",
              "      <td>Sending oscillating microwaves from an antenna...</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12278</th>\n",
              "      <td>Sending oscillating microwaves from an antenna...</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>12279 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    text      label\n",
              "0      Not only is there no scientific evidence that ...    related\n",
              "1      Not only is there no scientific evidence that ...    related\n",
              "2      Not only is there no scientific evidence that ...    related\n",
              "3      Not only is there no scientific evidence that ...  unrelated\n",
              "4      Not only is there no scientific evidence that ...  unrelated\n",
              "...                                                  ...        ...\n",
              "12274  Sending oscillating microwaves from an antenna...  unrelated\n",
              "12275  Sending oscillating microwaves from an antenna...  unrelated\n",
              "12276  Sending oscillating microwaves from an antenna...  unrelated\n",
              "12277  Sending oscillating microwaves from an antenna...  unrelated\n",
              "12278  Sending oscillating microwaves from an antenna...  unrelated\n",
              "\n",
              "[12279 rows x 2 columns]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# format data for the transformer\n",
        "\n",
        "SPECIAL_TOKEN = ' <SPE_TOKEN> '\n",
        "\n",
        "def format_for_transformer(processed_evidence, similar_claim_evidence, true_claim_evidence, top_n):\n",
        "    text_lst = []\n",
        "    label_lst = []\n",
        "\n",
        "    # for analysing:\n",
        "    true_evidence_num = 0\n",
        "    found_evidence_num = 0\n",
        "    false_evidence_num = 0\n",
        "    claims_with_all_evidence = 0\n",
        "\n",
        "    for index, row in true_claim_evidence.iterrows():\n",
        "        #print(index)\n",
        "        if index in similar_claim_evidence.index:\n",
        "            similar_claim_evidence_row = similar_claim_evidence.loc[index]\n",
        "            \n",
        "        else:\n",
        "            print(index + \" NOT FOUND IN TOP N EVIDENCE!\")\n",
        "            print(similar_claim_evidence.index)\n",
        "            continue\n",
        "        claim_text = row['claim_text']\n",
        "        \n",
        "        true_evidence_list = row['evidences']\n",
        "        #print(len(true_evidence_list))\n",
        "        similar_evidence_list = similar_claim_evidence_row['evidences']\n",
        "        false_evidence_list =  list(set(similar_evidence_list) - set(true_evidence_list))\n",
        "\n",
        "\n",
        "        true_evidence_num += len(true_evidence_list)\n",
        "        false_evidence_num += len(false_evidence_list)\n",
        "        found_evidence_num += len(similar_evidence_list) - len(false_evidence_list)\n",
        "        \n",
        "        if(len(similar_evidence_list) - len(false_evidence_list) == len(true_evidence_list)):\n",
        "            claims_with_all_evidence +=1    \n",
        "\n",
        "       # print(len(false_evidence_list))\n",
        "        for evidence in true_evidence_list:\n",
        "            if evidence not in processed_evidence.index:\n",
        "                print(evidence + \"may be empty/ not in english, skipping..\")\n",
        "                continue\n",
        "            evidence_text = processed_evidence[evidence]\n",
        "            text = claim_text + SPECIAL_TOKEN + evidence_text\n",
        "            text_lst.append(text)\n",
        "            label_lst.append('related')\n",
        "        for evidence in false_evidence_list[:top_n-len(true_evidence_list)]:\n",
        "            if evidence not in processed_evidence.index:\n",
        "                print(evidence + \"may be empty/ not in english, skipping..\")\n",
        "                continue\n",
        "            evidence_text = processed_evidence[evidence]\n",
        "            text = claim_text + SPECIAL_TOKEN + evidence_text\n",
        "            text_lst.append(text)\n",
        "            label_lst.append('unrelated')\n",
        "\n",
        "\n",
        "    claim_evi_label = {'text': text_lst, 'label': label_lst}\n",
        "\n",
        "    print(f\"true evidence count {true_evidence_num}\")\n",
        "    print(f\"true evidence found in top {top_n}: {found_evidence_num}\")\n",
        "    print(f\"unrelated evidence found in top {top_n}: {false_evidence_num}\")\n",
        "    print(f\"# claims with all evidence in top {top_n}: {claims_with_all_evidence}\")\n",
        "    return pd.DataFrame(claim_evi_label)\n",
        "\n",
        "preparedTrain = format_for_transformer(processed_evidence, train_with_evi, train, TOP_N_TRAIN)\n",
        "preparedDev= format_for_transformer(processed_evidence, dev_with_evi, dev_data, TOP_N_TEST)\n",
        "# preparedTrain = prepareTrainData(10)\n",
        "# preparedDev = prepareDevData(10)\n",
        "preparedTrain\n",
        "#preparedDev.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": [
        "# need later versions for torchtext.transforms and special\n",
        "#pip install torchtext==0.18.0 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnNiS5byTvJM"
      },
      "source": [
        "# Two steps for the this task\n",
        "# first. find all relavent evidence, either use contextual embedding or similarity scoring\n",
        "# second. classify the evidents into 4 classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FA2ao2l8hOg"
      },
      "source": [
        "# 2. Model Implementation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "QIEqDDT78q39"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/kaiyuancui/opt/anaconda3/lib/python3.9/site-packages/torchtext/data/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/Users/kaiyuancui/opt/anaconda3/lib/python3.9/site-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/Users/kaiyuancui/opt/anaconda3/lib/python3.9/site-packages/torchtext/utils.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/Users/kaiyuancui/opt/anaconda3/lib/python3.9/site-packages/torchtext/transforms.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/Users/kaiyuancui/opt/anaconda3/lib/python3.9/site-packages/torchtext/functional.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "import torchtext\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "import torchtext.transforms\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "# No module named 'torchtext.transforms' ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# tokenize and vectorise training data for relevance classification\n",
        "\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "def yield_tokens(data_iter):\n",
        "    for text in data_iter:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "vocab = build_vocab_from_iterator(yield_tokens(preparedTrain['text']), specials=[\"<unk>\"])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])\n",
        "\n",
        "text_pipeline = lambda x: vocab(tokenizer(x))\n",
        "label_transform = torchtext.transforms.LabelToIndex({'related': 0, 'unrelated': 1})\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def collate_batch(batch):\n",
        "    text_list, label_list, offsets = [], [], [0]\n",
        "    for _text, _label in batch:\n",
        "        label_list.append(label_transform(_label))\n",
        "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
        "        text_list.append(processed_text)\n",
        "        offsets.append(processed_text.size(0))\n",
        "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
        "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "    # should stack instead of concat?\n",
        "    #text_list = torch.cat(text_list)\n",
        "   \n",
        "    # pad sequences to make them the same length \n",
        "    padded_sequences = pad_sequence(text_list, batch_first=True)\n",
        "\n",
        "    return padded_sequences.to(device), label_list.to(device), offsets.to(device)\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, dataframe):\n",
        "        self.dataframe = dataframe\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        row = self.dataframe.iloc[idx]\n",
        "        text = row['text'] \n",
        "        label = row['label'] \n",
        "        return text, label\n",
        "\n",
        "full_dataset = TextDataset(preparedTrain)\n",
        "dev_dataset = TextDataset(preparedDev)\n",
        "# train test split\n",
        "#train_size = int(0.8 * len(full_dataset))\n",
        "\n",
        "#val_size = len(full_dataset) - train_size \n",
        "# train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
        "# use dev data instead:\n",
        "\n",
        "train_dataloader = DataLoader(full_dataset, batch_size=20, shuffle=False, collate_fn=collate_batch)\n",
        "val_dataloader = DataLoader(dev_dataset, batch_size=20, shuffle=False, collate_fn=collate_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/kaiyuancui/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        }
      ],
      "source": [
        "# initialise the relevance classification model\n",
        "# NOTE: run the OOP code first!\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "ntokens = len(vocab) # TODO: verify correctness of this\n",
        "\n",
        "emsize = 200 # embedding dimension\n",
        "nhid = 200 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
        "nlayers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
        "nhead = 4 # the number of heads in the multiheadattention models\n",
        "dropout = 0.2 # the dropout value\n",
        "\n",
        "model = TransformerClassificationModel(ntokens, emsize, nhead, nhid, nlayers, 2, dropout).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 35.19s | valid loss 13.92 | valid ppl 1106923.12\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 35.23s | valid loss 12.19 | valid ppl 196549.18\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 35.96s | valid loss 13.95 | valid ppl 1147158.72\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# train model\n",
        "import copy\n",
        "import math\n",
        "import torch.nn as nn\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 1.0 # learning rate\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
        "\n",
        "import time\n",
        "\n",
        "def train_model(train_data_loader, curr_model):\n",
        "\n",
        "    curr_model.train() # Turn on the train mode\n",
        "    for inputs, labels, offsets in train_data_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = curr_model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        clipping_value = 1 \n",
        "        # training unstable? https://stackoverflow.com/questions/66625645/why-does-my-pytorch-nn-return-a-tensor-of-nan\n",
        "        torch.nn.utils.clip_grad_norm_(curr_model.parameters(), clipping_value) # https://stackoverflow.com/questions/54716377/how-to-do-gradient-clipping-in-pytorch\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "def evaluate(val_data_loader, eval_model):\n",
        "    eval_model.eval() # Turn on the evaluation mode\n",
        "    total_loss = 0.\n",
        "\n",
        "    with torch.no_grad():\n",
        "       \n",
        "        for inputs, labels, offsets in val_data_loader:\n",
        "          \n",
        "            output = eval_model(inputs)\n",
        "            # output_flat = output.view(-1, ntokens) do we need to do this?\n",
        "            total_loss += len(inputs) * criterion(output, labels).item()\n",
        "    return total_loss / (len(val_data_loader) - 1)\n",
        "\n",
        "\n",
        "\n",
        "def train_over_epochs(epochs, train_dataloader, val_dataloader, curr_model):\n",
        "    best_val_loss = float(\"inf\")\n",
        "    \n",
        "    best_model = None\n",
        "    #train(dataloader, model)\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        epoch_start_time = time.time()\n",
        "\n",
        "        train_model(train_dataloader, curr_model)\n",
        "        val_loss = evaluate(val_dataloader, curr_model) \n",
        "        print('-' * 89)\n",
        "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "            'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                        val_loss, math.exp(val_loss)))\n",
        "        print('-' * 89)\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_model = copy.deepcopy(curr_model)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    return best_model\n",
        "\n",
        "best_model = train_over_epochs(3, train_dataloader, val_dataloader, model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('[South Australia] has the most expensive electricity in the world. <SPE_TOKEN> citation needed south australia highest retail price electricity country', 'related')\n",
            "tensor([   0,    0,   30,    1,  121, 2029,  465,    8,    1,  102,    2,    3,\n",
            "        1879, 1093,  412,  376,  629, 6373,  536,  465,  339,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0])\n",
            "tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1])\n",
            "tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0])\n"
          ]
        }
      ],
      "source": [
        "# sanity check\n",
        "\n",
        "print(dev_dataset[0])\n",
        "val_batch = next(iter(val_dataloader))\n",
        "print(val_batch[0][0]) # first input\n",
        "print(val_batch[1]) # labels\n",
        "\n",
        "test_outputs = model(val_batch[0])\n",
        "predicted_labels = torch.argmax(test_outputs, dim=1)\n",
        "\n",
        "print(predicted_labels)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzGuzHPE87Ya"
      },
      "source": [
        "# 3.Testing and Evaluation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "6ZVeNYIH9IaL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=========================================================================================\n",
            "| End of training | test loss 12.19 | test ppl 196549.18\n",
            "=========================================================================================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     related       0.53      0.84      0.65       491\n",
            "   unrelated       0.90      0.65      0.76      1049\n",
            "\n",
            "    accuracy                           0.71      1540\n",
            "   macro avg       0.71      0.75      0.70      1540\n",
            "weighted avg       0.78      0.71      0.72      1540\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "def get_classification_report(val_data_loader, eval_model):\n",
        "    eval_model.eval()\n",
        "\n",
        "\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for inputs, labels, offsets in val_data_loader:\n",
        "        outputs = eval_model(inputs)\n",
        "        predicted_labels = torch.argmax(outputs, dim=1)\n",
        "        y_true += labels\n",
        "        y_pred += predicted_labels\n",
        "    \n",
        "\n",
        "    return classification_report(y_true, y_pred, target_names=['related', 'unrelated'])\n",
        "\n",
        "test_loss = evaluate(val_dataloader, best_model) \n",
        "test_report = get_classification_report(val_dataloader, best_model)\n",
        "\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
        "    test_loss, math.exp(test_loss)))\n",
        "print('=' * 89)\n",
        "\n",
        "print(test_report)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# format test data to be put into model\n",
        "def prepareTestData():\n",
        "    tfidf_claim = test_with_evi['claim_text']\n",
        "    tfidf_evi = test_with_evi['evidences']\n",
        "    text_lst = []\n",
        "    for i in range(len(tfidf_claim)):\n",
        "        test_claim = tfidf_claim[i]\n",
        "        evidences = tfidf_evi[i]\n",
        "        for j in evidences:\n",
        "            text = test_claim + SPECIAL_TOKEN + processed_evidence[j]\n",
        "            text_lst.append(text)\n",
        "    claim_evi = {'text': text_lst, 'label':'unrelated'} # still include the label field to avoid index error\n",
        "    return pd.DataFrame(claim_evi)\n",
        "\n",
        "preparedTest = prepareTestData()\n",
        "preparedTest.head()\n",
        "test_dataset = TextDataset(preparedTest)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=20, shuffle=False, collate_fn=collate_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Counter({1: 885, 0: 655})\n"
          ]
        }
      ],
      "source": [
        "# manually check the results since we don't have the labels:\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "# print(test_dataset[0])\n",
        "# test_batch = next(iter(test_dataloader))\n",
        "# print(test_batch[0][0]) # first input\n",
        "\n",
        "# test_outputs = best_model(test_batch[0])\n",
        "# predicted_labels = torch.argmax(test_outputs, dim=1)\n",
        "# print(predicted_labels[0])\n",
        "\n",
        "\n",
        "def get_all_predictions(model, test_loader):\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    for inputs, _, _ in test_loader:\n",
        "        outputs = model(inputs)\n",
        "        predicted_labels = torch.argmax(outputs, dim=1)\n",
        "        all_predictions.extend(predicted_labels.cpu().numpy())\n",
        "    return all_predictions\n",
        "\n",
        "# check class imbalance\n",
        "def count_labels(predictions):\n",
        "    label_counts = Counter(predictions)\n",
        "    return label_counts\n",
        "\n",
        "\n",
        "# get all predictions\n",
        "\n",
        "# changed to dev data\n",
        "all_predictions = get_all_predictions(best_model, val_dataloader)\n",
        "print(count_labels(all_predictions))\n",
        "#print(all_predictions)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1540\n",
            "1540\n",
            "1540\n"
          ]
        }
      ],
      "source": [
        "# update test_with_evi with model results\n",
        "\n",
        "def filter_relevant_evidences(claim_evidences, model_classifications, default=True, no_filter = False):\n",
        "    claims = claim_evidences['claim_text']\n",
        "    evidences = claim_evidences['evidences']\n",
        "    filtered_evidences = []\n",
        "\n",
        "\n",
        "    classifications_index = 0\n",
        "    for i in range(len(claims)):\n",
        "        claim = claims[i]\n",
        "        curr_evidences = evidences[i]\n",
        "        curr_filtered = []\n",
        "        for evidence in curr_evidences:\n",
        "            # 0 => related\n",
        "            \n",
        "            if no_filter or (classifications_index < len(model_classifications) and model_classifications[classifications_index] == 0):\n",
        "                curr_filtered.append(evidence)\n",
        "\n",
        "            classifications_index += 1\n",
        "\n",
        "        if default and len(curr_filtered) == 0:\n",
        "            # keep the most similar evidence if all are deemed unrelated\n",
        "            curr_filtered.append(curr_evidences[0]) \n",
        "\n",
        "        filtered_evidences.append(curr_filtered)\n",
        "    \n",
        "    print(classifications_index)\n",
        "    claim_evi = {'text': claims, 'evidences': filtered_evidences, 'claim_label': 'not there yet'} #still include the label field to avoid index error \n",
        "    return pd.DataFrame(claim_evi)\n",
        "\n",
        "#filtered_claim_evidences = filter_relevant_evidences(test_with_evi, all_predictions)\n",
        "# use dev instead\n",
        "print(len(all_predictions))\n",
        "filtered_claim_evidences = filter_relevant_evidences(dev_with_evi, all_predictions)\n",
        "filtered_claim_evidences.head()\n",
        "# just change the format here\n",
        "top_n_predictions = filter_relevant_evidences(dev_with_evi, all_predictions, no_filter=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>evidences</th>\n",
              "      <th>claim_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>claim-752</th>\n",
              "      <td>south australia expensive electricity world</td>\n",
              "      <td>[evidence-421845, evidence-995049, evidence-68...</td>\n",
              "      <td>not there yet</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-375</th>\n",
              "      <td>per cent total annual global emission carbon d...</td>\n",
              "      <td>[evidence-1140012, evidence-1011788, evidence-...</td>\n",
              "      <td>not there yet</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-1266</th>\n",
              "      <td>mean world warmer time</td>\n",
              "      <td>[evidence-694262, evidence-943875, evidence-40...</td>\n",
              "      <td>not there yet</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-871</th>\n",
              "      <td>happens zika may also good model second worryi...</td>\n",
              "      <td>[evidence-336512, evidence-472751, evidence-54...</td>\n",
              "      <td>not there yet</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-2164</th>\n",
              "      <td>greenland lost tiny fraction ice mass</td>\n",
              "      <td>[evidence-962481, evidence-1200633, evidence-7...</td>\n",
              "      <td>not there yet</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                         text  \\\n",
              "claim-752         south australia expensive electricity world   \n",
              "claim-375   per cent total annual global emission carbon d...   \n",
              "claim-1266                             mean world warmer time   \n",
              "claim-871   happens zika may also good model second worryi...   \n",
              "claim-2164              greenland lost tiny fraction ice mass   \n",
              "\n",
              "                                                    evidences    claim_label  \n",
              "claim-752   [evidence-421845, evidence-995049, evidence-68...  not there yet  \n",
              "claim-375   [evidence-1140012, evidence-1011788, evidence-...  not there yet  \n",
              "claim-1266  [evidence-694262, evidence-943875, evidence-40...  not there yet  \n",
              "claim-871   [evidence-336512, evidence-472751, evidence-54...  not there yet  \n",
              "claim-2164  [evidence-962481, evidence-1200633, evidence-7...  not there yet  "
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "top_n_predictions.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evidence Retrieval F-score (F)    = 0.08100016866250632\n",
            "Claim Classification Accuracy (A) = 0.0\n",
            "Harmonic Mean of F and A          = 0.0\n",
            "Evidence Retrieval F-score (F)    = 0.0694247311130428\n",
            "Claim Classification Accuracy (A) = 0.0\n",
            "Harmonic Mean of F and A          = 0.0\n"
          ]
        }
      ],
      "source": [
        "# adapted from eval.py\n",
        "def provided_eval(predictions, groundtruth):\n",
        "\n",
        "    try:\n",
        "        f, acc = [], []\n",
        "\n",
        "        #iterate through the groundtruth instances\n",
        "        for claim_id, claim in sorted(groundtruth.iterrows()):\n",
        "            \n",
        "            if claim_id in predictions.index and \\\n",
        "                \"claim_label\" in predictions.loc[claim_id] and \\\n",
        "                \"evidences\" in predictions.loc[claim_id]:\n",
        "                #print(claim_id)\n",
        "                #check claim level label\n",
        "                instance_correct = 0.0\n",
        "                if predictions.loc[claim_id][\"claim_label\"] == claim[\"claim_label\"]:\n",
        "                    instance_correct = 1.0\n",
        "                \n",
        "                #check retrieved evidences\n",
        "                evidence_correct = 0\n",
        "                evidence_recall = 0.0\n",
        "                evidence_precision = 0.0\n",
        "                evidence_fscore = 0.0\n",
        "              \n",
        "                if type(predictions.loc[claim_id][\"evidences\"]) == list and (len(predictions.loc[claim_id][\"evidences\"]) > 0):\n",
        "                    top_six_ev = set(predictions.loc[claim_id][\"evidences\"])\n",
        "                 \n",
        "                    for gr_ev in claim[\"evidences\"]:\n",
        "                        if gr_ev in top_six_ev:\n",
        "                            evidence_correct += 1\n",
        "                    if evidence_correct > 0:\n",
        "                        evidence_recall = float(evidence_correct) / len(claim[\"evidences\"])\n",
        "                        evidence_precision = \\\n",
        "                            float(evidence_correct) / len(predictions.loc[claim_id][\"evidences\"])\n",
        "                        evidence_fscore = (2*evidence_precision*evidence_recall)/(evidence_precision+evidence_recall)\n",
        "\n",
        "                \n",
        "                # print(\"groundtruth =\", claim)\n",
        "                # print(\"predictions =\", predictions.loc[claim_id])\n",
        "                # print(\"instance accuracy =\", instance_correct)\n",
        "                # print(\"evidence recall =\", evidence_recall)\n",
        "                # print(\"evidence precision =\", evidence_precision)\n",
        "                # print(\"evidence fscore =\", evidence_fscore, \"\\n\\n\")\n",
        "\n",
        "                #add the metric results\n",
        "                acc.append(instance_correct)\n",
        "                f.append(evidence_fscore)\n",
        "\n",
        "        #compute aggregate performance\n",
        "        mean_f = np.mean(f if len(f) > 0 else [0.0])\n",
        "        mean_acc = np.mean(acc if len(acc) > 0 else [0.0])\n",
        "        if mean_f == 0.0 and mean_acc == 0.0:\n",
        "            hmean = 0.0\n",
        "        else:\n",
        "            hmean = (2*mean_f*mean_acc)/(mean_f+mean_acc)\n",
        "\n",
        "        print(\"Evidence Retrieval F-score (F)    =\", mean_f)\n",
        "        print(\"Claim Classification Accuracy (A) =\", mean_acc)\n",
        "        print(\"Harmonic Mean of F and A          =\", hmean)\n",
        "                \n",
        "    except Exception as error:\n",
        "        print(\"Error:\", error)\n",
        "        raise SystemExit\n",
        "\n",
        "# print(\"claim-752\" in filtered_claim_evidences)\n",
        "# print(\"claim-752\" in dev_data)\n",
        "#print(dev_data.loc['claim-752']['claim_label'])\n",
        "provided_eval(filtered_claim_evidences, dev_data)\n",
        "provided_eval(top_n_predictions, dev_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/kaiyuancui/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 33.18s | valid loss 15.01 | valid ppl 3287894.51\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 32.43s | valid loss 15.01 | valid ppl 3287894.51\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 34.62s | valid loss 15.01 | valid ppl 3287894.51\n",
            "-----------------------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     related       0.53      0.84      0.65       491\n",
            "   unrelated       0.90      0.65      0.76      1049\n",
            "\n",
            "    accuracy                           0.71      1540\n",
            "   macro avg       0.71      0.75      0.70      1540\n",
            "weighted avg       0.78      0.71      0.72      1540\n",
            "\n",
            "1540\n",
            "Evidence Retrieval F-score (F)    = 0.056990520302208615\n",
            "Claim Classification Accuracy (A) = 0.0\n",
            "Harmonic Mean of F and A          = 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/kaiyuancui/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 35.58s | valid loss 11.76 | valid ppl 128189.72\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 29.44s | valid loss 11.76 | valid ppl 128189.72\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 28.82s | valid loss 11.76 | valid ppl 128189.72\n",
            "-----------------------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     related       0.53      0.84      0.65       491\n",
            "   unrelated       0.90      0.65      0.76      1049\n",
            "\n",
            "    accuracy                           0.71      1540\n",
            "   macro avg       0.71      0.75      0.70      1540\n",
            "weighted avg       0.78      0.71      0.72      1540\n",
            "\n",
            "1540\n",
            "Evidence Retrieval F-score (F)    = 0.07328385899814471\n",
            "Claim Classification Accuracy (A) = 0.0\n",
            "Harmonic Mean of F and A          = 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/kaiyuancui/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 32.65s | valid loss 11.80 | valid ppl 132807.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 33.78s | valid loss 11.80 | valid ppl 132807.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 34.71s | valid loss 11.80 | valid ppl 132807.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     related       0.53      0.84      0.65       491\n",
            "   unrelated       0.90      0.65      0.76      1049\n",
            "\n",
            "    accuracy                           0.71      1540\n",
            "   macro avg       0.71      0.75      0.70      1540\n",
            "weighted avg       0.78      0.71      0.72      1540\n",
            "\n",
            "1540\n",
            "Evidence Retrieval F-score (F)    = 0.06767676767676768\n",
            "Claim Classification Accuracy (A) = 0.0\n",
            "Harmonic Mean of F and A          = 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/kaiyuancui/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 41.15s | valid loss 13.27 | valid ppl 579615.73\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 42.20s | valid loss 13.27 | valid ppl 579615.73\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 37.79s | valid loss 13.27 | valid ppl 579615.73\n",
            "-----------------------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     related       0.53      0.84      0.65       491\n",
            "   unrelated       0.90      0.65      0.76      1049\n",
            "\n",
            "    accuracy                           0.71      1540\n",
            "   macro avg       0.71      0.75      0.70      1540\n",
            "weighted avg       0.78      0.71      0.72      1540\n",
            "\n",
            "1540\n",
            "Evidence Retrieval F-score (F)    = 0.03438559997001556\n",
            "Claim Classification Accuracy (A) = 0.0\n",
            "Harmonic Mean of F and A          = 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/kaiyuancui/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 55.38s | valid loss 14.98 | valid ppl 3197241.95\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# experiement with different params\n",
        "EXPERIMENT = True\n",
        "if EXPERIMENT:\n",
        "    emsize = 200 # embedding dimension\n",
        "    nhid = 200 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
        "    nlayers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
        "    nhead = 4 # the number of heads in the multiheadattention models\n",
        "    dropout = 0.2 # the dropout value\n",
        "\n",
        "    num_heads = [1, 2, 4, 8, 20]\n",
        "    num_encoders = [2, 4, 6, 8, 10]\n",
        "\n",
        "    heads_loss = []\n",
        "    encoders_loss = []\n",
        "    heads_reports = []\n",
        "    encoders_reports = []\n",
        "    for n in num_heads:\n",
        "\n",
        "        exp_model = TransformerClassificationModel(ntokens, emsize, n, nhid, nlayers, 2, dropout).to(device)\n",
        "        trained_model = train_over_epochs(3, train_dataloader, val_dataloader, exp_model)\n",
        "        exp_model_loss = evaluate(val_dataloader, trained_model) \n",
        "        heads_loss.append(exp_model_loss)\n",
        "        exp_model_report = get_classification_report(val_dataloader, trained_model)\n",
        "        print(exp_model_report)\n",
        "        exp_predictions = get_all_predictions(trained_model, val_dataloader)\n",
        "\n",
        "        filtered_claim_evidences = filter_relevant_evidences(dev_with_evi, exp_predictions)\n",
        "        provided_eval(filtered_claim_evidences, dev_data)\n",
        "        heads_reports.append(exp_model_report)\n",
        "\n",
        "    for n in num_encoders:\n",
        "\n",
        "        exp_model = TransformerClassificationModel(ntokens, emsize, nhead, nhid, n, 2, dropout).to(device)\n",
        "        trained_model = train_over_epochs(3, train_dataloader,val_dataloader, exp_model)\n",
        "        exp_model_loss = evaluate(val_dataloader, trained_model) \n",
        "        encoders_loss.append(exp_model_loss)\n",
        "        exp_model_report = get_classification_report(val_dataloader, trained_model)\n",
        "        print(exp_model_report)\n",
        "        exp_predictions = get_all_predictions(trained_model, val_dataloader)\n",
        "\n",
        "        filtered_claim_evidences = filter_relevant_evidences(dev_with_evi, exp_predictions)\n",
        "        provided_eval(filtered_claim_evidences, dev_data)\n",
        "        encoders_reports.append(exp_model_report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     related       0.58      0.77      0.66       491\n",
            "   unrelated       0.87      0.74      0.80      1049\n",
            "\n",
            "    accuracy                           0.75      1540\n",
            "   macro avg       0.73      0.76      0.73      1540\n",
            "weighted avg       0.78      0.75      0.76      1540\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     related       0.58      0.77      0.66       491\n",
            "   unrelated       0.87      0.74      0.80      1049\n",
            "\n",
            "    accuracy                           0.75      1540\n",
            "   macro avg       0.73      0.76      0.73      1540\n",
            "weighted avg       0.78      0.75      0.76      1540\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     related       0.58      0.77      0.66       491\n",
            "   unrelated       0.87      0.74      0.80      1049\n",
            "\n",
            "    accuracy                           0.75      1540\n",
            "   macro avg       0.73      0.76      0.73      1540\n",
            "weighted avg       0.78      0.75      0.76      1540\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     related       0.58      0.77      0.66       491\n",
            "   unrelated       0.87      0.74      0.80      1049\n",
            "\n",
            "    accuracy                           0.75      1540\n",
            "   macro avg       0.73      0.76      0.73      1540\n",
            "weighted avg       0.78      0.75      0.76      1540\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     related       0.58      0.77      0.66       491\n",
            "   unrelated       0.87      0.74      0.80      1049\n",
            "\n",
            "    accuracy                           0.75      1540\n",
            "   macro avg       0.73      0.76      0.73      1540\n",
            "weighted avg       0.78      0.75      0.76      1540\n",
            "\n",
            "====================================================================================================\n"
          ]
        }
      ],
      "source": [
        "for report in heads_reports:\n",
        "    print(report)\n",
        "print(\"=\"*100)\n",
        "for report in encoders_reports:\n",
        "    print(report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[20.417001859137887, 13.1146245018432, 14.166609786058727, 12.59639099240303, 22.982759036515887]\n",
            "[]\n"
          ]
        }
      ],
      "source": [
        "print(heads_loss)\n",
        "print(encoders_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mefSOe8eTmGP"
      },
      "source": [
        "## Object Oriented Programming codes here\n",
        "\n",
        "*You can use multiple code snippets. Just add more if needed*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# model adpated from workshop 8\n",
        "class TransformerClassificationModel(nn.Module):\n",
        "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, num_classes, dropout=0.5):\n",
        "        super(TransformerClassificationModel, self).__init__()\n",
        "        from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "        self.model_type = 'Transformer'\n",
        "        self.src_mask = None\n",
        "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
        "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.encoder = nn.Embedding(ntoken, ninp)\n",
        "        self.ninp = ninp\n",
        "        self.classification_head = nn.Linear(ninp, num_classes)  # added classification head\n",
        "        self.init_weights()\n",
        "\n",
        "    def _generate_square_subsequent_mask(self, sz):\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.classification_head.bias.data.zero_()\n",
        "        self.classification_head.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src):\n",
        "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
        "            device = src.device\n",
        "            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
        "            self.src_mask = mask\n",
        "\n",
        "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
        "        src = self.pos_encoder(src)\n",
        "        output = self.transformer_encoder(src, self.src_mask)\n",
        "        output = output.mean(dim=1)  \n",
        "        output = self.classification_head(output) \n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term) #0::2 means starting with index 0, step = 2\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
