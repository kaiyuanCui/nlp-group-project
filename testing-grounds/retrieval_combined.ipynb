{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32yCsRUo8H33"
      },
      "source": [
        "# 2024 COMP90042 Project\n",
        "*Make sure you change the file name with your group id.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCybYoGz8YWQ"
      },
      "source": [
        "# Readme\n",
        "*If there is something to be noted for the marker, please mention here.*\n",
        "\n",
        "*If you are planning to implement a program with Object Oriented Programming style, please put those the bottom of this ipynb file*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6po98qVA8bJD"
      },
      "source": [
        "# 1.DataSet Processing\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvff21Hv8zjk",
        "outputId": "4aa2923a-2e47-4e05-c9cc-42fce5e6a04d"
      },
      "outputs": [],
      "source": [
        "LOCAL_DEV = True # to switch between developing locally and on colab\n",
        "\n",
        "if not LOCAL_DEV:\n",
        "    # TODO: need to upload data files on Google Drive?\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: contractions in g:\\ana\\envs\\nlp\\lib\\site-packages (0.1.73)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in g:\\ana\\envs\\nlp\\lib\\site-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: anyascii in g:\\ana\\envs\\nlp\\lib\\site-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
            "Requirement already satisfied: pyahocorasick in g:\\ana\\envs\\nlp\\lib\\site-packages (from textsearch>=0.0.21->contractions) (2.1.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install contractions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "MH1Hdrb5NmHM"
      },
      "outputs": [],
      "source": [
        "#Imports\n",
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "2piZvV4OMSa3",
        "outputId": "b17e35ee-0c98-48b9-aa69-f6b63d336059"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>claim_text</th>\n",
              "      <th>claim_label</th>\n",
              "      <th>evidences</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>claim-1937</th>\n",
              "      <td>Not only is there no scientific evidence that ...</td>\n",
              "      <td>DISPUTED</td>\n",
              "      <td>[evidence-442946, evidence-1194317, evidence-1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-126</th>\n",
              "      <td>El Niño drove record highs in global temperatu...</td>\n",
              "      <td>REFUTES</td>\n",
              "      <td>[evidence-338219, evidence-1127398]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-2510</th>\n",
              "      <td>In 1946, PDO switched to a cool phase.</td>\n",
              "      <td>SUPPORTS</td>\n",
              "      <td>[evidence-530063, evidence-984887]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-2021</th>\n",
              "      <td>Weather Channel co-founder John Coleman provid...</td>\n",
              "      <td>DISPUTED</td>\n",
              "      <td>[evidence-1177431, evidence-782448, evidence-5...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-2449</th>\n",
              "      <td>\"January 2008 capped a 12 month period of glob...</td>\n",
              "      <td>NOT_ENOUGH_INFO</td>\n",
              "      <td>[evidence-1010750, evidence-91661, evidence-72...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   claim_text  \\\n",
              "claim-1937  Not only is there no scientific evidence that ...   \n",
              "claim-126   El Niño drove record highs in global temperatu...   \n",
              "claim-2510             In 1946, PDO switched to a cool phase.   \n",
              "claim-2021  Weather Channel co-founder John Coleman provid...   \n",
              "claim-2449  \"January 2008 capped a 12 month period of glob...   \n",
              "\n",
              "                claim_label                                          evidences  \n",
              "claim-1937         DISPUTED  [evidence-442946, evidence-1194317, evidence-1...  \n",
              "claim-126           REFUTES                [evidence-338219, evidence-1127398]  \n",
              "claim-2510         SUPPORTS                 [evidence-530063, evidence-984887]  \n",
              "claim-2021         DISPUTED  [evidence-1177431, evidence-782448, evidence-5...  \n",
              "claim-2449  NOT_ENOUGH_INFO  [evidence-1010750, evidence-91661, evidence-72...  "
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#visualising training data\n",
        "if LOCAL_DEV:\n",
        "    train = pd.read_json(\"../data/train-claims.json\") # for local dev\n",
        "    \n",
        "else:\n",
        "    train = pd.read_json(\"/content/drive/MyDrive/data/train-claims.json\") # on colab\n",
        "train = train.transpose()\n",
        "train.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>claim_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>claim-2967</th>\n",
              "      <td>The contribution of waste heat to the global c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-979</th>\n",
              "      <td>“Warm weather worsened the most recent five-ye...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-1609</th>\n",
              "      <td>Greenland has only lost a tiny fraction of its...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-1020</th>\n",
              "      <td>“The global reef crisis does not necessarily m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-2599</th>\n",
              "      <td>Small amounts of very active substances can ca...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   claim_text\n",
              "claim-2967  The contribution of waste heat to the global c...\n",
              "claim-979   “Warm weather worsened the most recent five-ye...\n",
              "claim-1609  Greenland has only lost a tiny fraction of its...\n",
              "claim-1020  “The global reef crisis does not necessarily m...\n",
              "claim-2599  Small amounts of very active substances can ca..."
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "if LOCAL_DEV:\n",
        "    test = pd.read_json(\"../data/test-claims-unlabelled.json\") # for local dev\n",
        "    \n",
        "else:\n",
        "    test = pd.read_json(\"/content/drive/MyDrive/data/test-claims-unlabelled.json\") # on colab\n",
        "test = test.transpose()\n",
        "test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "ZKbGFcA2THHP"
      },
      "outputs": [],
      "source": [
        "#visualising evidence data\n",
        "if LOCAL_DEV:\n",
        "    evidence = pd.read_json(\"../data/evidence.json\",typ='series')\n",
        "else:\n",
        "    evidence = pd.read_json(\"/content/drive/MyDrive/data/evidence.json\",typ='series')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFNTiC0UMS45",
        "outputId": "a55e84f3-4f1c-4039-8fe0-cd2e8f2d13b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1208827\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "evidence-0    John Bennet Lawes, English entrepreneur and ag...\n",
              "evidence-1    Lindberg began his professional career at the ...\n",
              "evidence-2    ``Boston (Ladies of Cambridge)'' by Vampire We...\n",
              "evidence-3    Gerald Francis Goyer (born October 20, 1936) w...\n",
              "evidence-4    He detected abnormalities of oxytocinergic fun...\n",
              "dtype: object"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(len(evidence))\n",
        "evidence.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Asura\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Asura\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\Asura\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import string\n",
        "import contractions\n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "Xf73PDzRTrft"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "claim-2967               contribution waste heat global climate\n",
              "claim-979     warm weather worsened recent drought included ...\n",
              "claim-1609                greenland lost tiny fraction ice mass\n",
              "claim-1020    global reef crisis not necessarily mean extinc...\n",
              "claim-2599     small amount active substance cause large effect\n",
              "dtype: object"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def preprocess_data(data: pd.Series) -> pd.Series:\n",
        "  preprocessed_data = {}\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  stop_words.remove('not')\n",
        "  for id, text in data.items():\n",
        "    text = text.lower()\n",
        "    text = contractions.fix(text)\n",
        "    tokens = word_tokenize(text)\n",
        "    wnl = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [wnl.lemmatize(word) for word in tokens if word.isalpha() and word not in stop_words]\n",
        "    preprocessed_data[id] = \" \".join(lemmatized_tokens)\n",
        "\n",
        "  return pd.Series(preprocessed_data)\n",
        "\n",
        "processed_evidence = preprocess_data(evidence)\n",
        "\n",
        "test_claims = test['claim_text']\n",
        "processed_test = preprocess_data(test_claims)\n",
        "processed_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "processed_evidence = processed_evidence[processed_evidence.str.strip().str.len() > 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "yHWbo7W7TuUC"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Asura\\AppData\\Local\\Temp\\ipykernel_26768\\2039351849.py:8: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  train_claim = processed_train_claim[i]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>not scientific evidence pollutant higher conce...</td>\n",
              "      <td>related</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>not scientific evidence pollutant higher conce...</td>\n",
              "      <td>related</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>not scientific evidence pollutant higher conce...</td>\n",
              "      <td>related</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>not scientific evidence pollutant higher conce...</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>not scientific evidence pollutant higher conce...</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text      label\n",
              "0  not scientific evidence pollutant higher conce...    related\n",
              "1  not scientific evidence pollutant higher conce...    related\n",
              "2  not scientific evidence pollutant higher conce...    related\n",
              "3  not scientific evidence pollutant higher conce...  unrelated\n",
              "4  not scientific evidence pollutant higher conce...  unrelated"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "SPECIAL_TOKEN = ' <SPE_TOKEN> '\n",
        "def prepareTrainData(n):\n",
        "    train_claims = preprocess_data(train['claim_text'])\n",
        "    processed_train_claim = preprocess_data(train_claims)\n",
        "    text_lst = []\n",
        "    label_lst = []\n",
        "    for i in range(len(train)):\n",
        "        train_claim = processed_train_claim[i]\n",
        "        evidences = train.iloc[i]['evidences']\n",
        "        for j in evidences:\n",
        "            if j in processed_evidence.index :\n",
        "                text = train_claim + SPECIAL_TOKEN + processed_evidence[j]\n",
        "                text_lst.append(text)\n",
        "                label_lst.append('related')\n",
        "        filtered_evi = processed_evidence[~processed_evidence.index.isin(evidences)]\n",
        "        random_evidence = filtered_evi.sample(n)\n",
        "        for k in random_evidence:\n",
        "            text = train_claim + SPECIAL_TOKEN + k\n",
        "            text_lst.append(text)\n",
        "            label_lst.append('unrelated')\n",
        "    claim_evi_label = {'text': text_lst, 'label': label_lst}\n",
        "    return pd.DataFrame(claim_evi_label)\n",
        "\n",
        "train_claims = train['claim_text']\n",
        "processed_train_claim = preprocess_data(train_claims)\n",
        "preparedTrain = prepareTrainData(10)\n",
        "preparedTrain.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>not scientific evidence pollutant higher conce...</td>\n",
              "      <td>related</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>not scientific evidence pollutant higher conce...</td>\n",
              "      <td>related</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>not scientific evidence pollutant higher conce...</td>\n",
              "      <td>related</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>not scientific evidence pollutant higher conce...</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>not scientific evidence pollutant higher conce...</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>not scientific evidence pollutant higher conce...</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>not scientific evidence pollutant higher conce...</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>not scientific evidence pollutant higher conce...</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>not scientific evidence pollutant higher conce...</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>not scientific evidence pollutant higher conce...</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text      label\n",
              "0  not scientific evidence pollutant higher conce...    related\n",
              "1  not scientific evidence pollutant higher conce...    related\n",
              "2  not scientific evidence pollutant higher conce...    related\n",
              "3  not scientific evidence pollutant higher conce...  unrelated\n",
              "4  not scientific evidence pollutant higher conce...  unrelated\n",
              "5  not scientific evidence pollutant higher conce...  unrelated\n",
              "6  not scientific evidence pollutant higher conce...  unrelated\n",
              "7  not scientific evidence pollutant higher conce...  unrelated\n",
              "8  not scientific evidence pollutant higher conce...  unrelated\n",
              "9  not scientific evidence pollutant higher conce...  unrelated"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "preparedTrain.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": [
        "# need later versions for torchtext.transforms and special\n",
        "#pip install torchtext==0.18.0 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnNiS5byTvJM"
      },
      "source": [
        "# Two steps for the this task\n",
        "# first. find all relavent evidence, either use contextual embedding or similarity scoring\n",
        "# second. classify the evidents into 4 classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FA2ao2l8hOg"
      },
      "source": [
        "# 2. Model Implementation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn in g:\\ana\\envs\\nlp\\lib\\site-packages (1.4.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in g:\\ana\\envs\\nlp\\lib\\site-packages (from scikit-learn) (1.24.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in g:\\ana\\envs\\nlp\\lib\\site-packages (from scikit-learn) (1.13.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in g:\\ana\\envs\\nlp\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in g:\\ana\\envs\\nlp\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install -U scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "QIEqDDT78q39"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "g:\\Ana\\envs\\nlp\\Lib\\site-packages\\torchtext\\data\\__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "g:\\Ana\\envs\\nlp\\Lib\\site-packages\\torchtext\\vocab\\__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "g:\\Ana\\envs\\nlp\\Lib\\site-packages\\torchtext\\utils.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "g:\\Ana\\envs\\nlp\\Lib\\site-packages\\torchtext\\transforms.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "g:\\Ana\\envs\\nlp\\Lib\\site-packages\\torchtext\\functional.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "import torchtext\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "import torchtext.transforms\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "# No module named 'torchtext.transforms' ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "# tokenize and vectorise training data for relevance classification\n",
        "\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "def yield_tokens(data_iter):\n",
        "    for text in data_iter:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "vocab = build_vocab_from_iterator(yield_tokens(preparedTrain['text']), specials=[\"<unk>\"])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])\n",
        "\n",
        "text_pipeline = lambda x: vocab(tokenizer(x))\n",
        "label_transform = torchtext.transforms.LabelToIndex({'related': 0, 'unrelated': 1})\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def collate_batch(batch):\n",
        "    text_list, label_list, offsets = [], [], [0]\n",
        "    for _text, _label in batch:\n",
        "        label_list.append(label_transform(_label))\n",
        "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
        "        text_list.append(processed_text)\n",
        "        offsets.append(processed_text.size(0))\n",
        "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
        "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "    # should stack instead of concat?\n",
        "    #text_list = torch.cat(text_list)\n",
        "   \n",
        "    # pad sequences to make them the same length \n",
        "    padded_sequences = pad_sequence(text_list, batch_first=True)\n",
        "\n",
        "    return padded_sequences.to(device), label_list.to(device), offsets.to(device)\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, dataframe):\n",
        "        self.dataframe = dataframe\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        row = self.dataframe.iloc[idx]\n",
        "        text = row['text'] \n",
        "        label = row['label'] \n",
        "        return text, label\n",
        "\n",
        "full_dataset = TextDataset(preparedTrain)\n",
        "# train test split\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=20, shuffle=False, collate_fn=collate_batch)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=20, shuffle=False, collate_fn=collate_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "g:\\Ana\\envs\\nlp\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        }
      ],
      "source": [
        "# initialise the relevance classification model\n",
        "# NOTE: run the OOP code first!\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "ntokens = len(vocab) # TODO: verify correctness of this\n",
        "\n",
        "emsize = 200 # embedding dimension\n",
        "nhid = 200 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
        "nlayers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
        "nhead = 2 # the number of heads in the multiheadattention models\n",
        "dropout = 0.2 # the dropout value\n",
        "\n",
        "model = TransformerClassificationModel(ntokens, emsize, nhead, nhid, nlayers, 2, dropout).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "g:\\Ana\\envs\\nlp\\Lib\\site-packages\\torch\\nn\\functional.py:5504: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
            "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time:  7.45s | valid loss 12.95 | valid ppl 418902.31\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time:  6.05s | valid loss  9.09 | valid ppl  8906.35\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time:  5.80s | valid loss  7.88 | valid ppl  2647.04\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# train model\n",
        "import math\n",
        "import torch.nn as nn\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 1.0 # learning rate\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "# TODO: figure out what this does and decide if it's needed in train\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
        "\n",
        "import time\n",
        "\n",
        "def train_model(train_data_loader, model):\n",
        "\n",
        "    model.train() # Turn on the train mode\n",
        "    for inputs, labels, offsets in train_data_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        clipping_value = 1 # arbitrary value of your choosing\n",
        "        # training unstable? https://stackoverflow.com/questions/66625645/why-does-my-pytorch-nn-return-a-tensor-of-nan\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clipping_value) # https://stackoverflow.com/questions/54716377/how-to-do-gradient-clipping-in-pytorch\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "def evaluate(val_data_loader, eval_model):\n",
        "    eval_model.eval() # Turn on the evaluation mode\n",
        "    total_loss = 0.\n",
        "\n",
        "    with torch.no_grad():\n",
        "       \n",
        "        for inputs, labels, offsets in val_data_loader:\n",
        "          \n",
        "            output = eval_model(inputs)\n",
        "            # output_flat = output.view(-1, ntokens) do we need to do this?\n",
        "            total_loss += len(inputs) * criterion(output, labels).item()\n",
        "    return total_loss / (len(val_data_loader) - 1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "best_val_loss = float(\"inf\")\n",
        "epochs = 3 # The number of epochs\n",
        "best_model = None\n",
        "#train(dataloader, model)\n",
        "for epoch in range(1, epochs + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    # TODO: PUT DATA HERE\n",
        "    train_model(train_dataloader, model)\n",
        "    val_loss = evaluate(val_dataloader, model) \n",
        "    print('-' * 89)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                     val_loss, math.exp(val_loss)))\n",
        "    print('-' * 89)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_model = model\n",
        "\n",
        "    scheduler.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('forget global warming activist would lead not even close hottest year record <SPE_TOKEN> global warming rise average temperature earth climate system', 'related')\n",
            "tensor([3328,    4,    2, 1918,   30,  242,    7,   94,  469,  710,    6,   20,\n",
            "           1,    4,    2,   19,   65,    5,   23,    3,   91,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0], device='cuda:0')\n",
            "tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1],\n",
            "       device='cuda:0')\n",
            "tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1],\n",
            "       device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "# sanity check\n",
        "\n",
        "print(val_dataset[0])\n",
        "val_batch = next(iter(val_dataloader))\n",
        "print(val_batch[0][0]) # first input\n",
        "print(val_batch[1]) # labels\n",
        "\n",
        "test_outputs = model(val_batch[0])\n",
        "predicted_labels = torch.argmax(test_outputs, dim=1)\n",
        "\n",
        "print(predicted_labels)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzGuzHPE87Ya"
      },
      "source": [
        "# 3.Testing and Evaluation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "6ZVeNYIH9IaL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=========================================================================================\n",
            "| End of training | test loss  7.88 | test ppl  2647.04 | test accuracy:  0.84\n",
            "=========================================================================================\n"
          ]
        }
      ],
      "source": [
        "def accuracy(val_data_loader, eval_model):\n",
        "    model.eval()\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for inputs, labels, offsets in val_data_loader:\n",
        "        outputs = model(inputs)\n",
        "        predicted_labels = torch.argmax(outputs, dim=1)\n",
        "        total_samples += labels.size(0)\n",
        "        total_correct += (predicted_labels == labels).sum().item()\n",
        "\n",
        "    accuracy = total_correct / total_samples\n",
        "    return accuracy\n",
        "\n",
        "test_loss = evaluate(val_dataloader, best_model) \n",
        "test_acc = accuracy(val_dataloader, best_model)\n",
        "\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.2f} | test ppl {:8.2f} | test accuracy: {:5.2f}'.format(\n",
        "    test_loss, math.exp(test_loss), test_acc))\n",
        "print('=' * 89)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>claim_text</th>\n",
              "      <th>evidences</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>claim-2967</th>\n",
              "      <td>contribution waste heat global climate</td>\n",
              "      <td>[evidence-308923, evidence-213569, evidence-63...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-979</th>\n",
              "      <td>warm weather worsened recent drought included ...</td>\n",
              "      <td>[evidence-178433, evidence-421870, evidence-43...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-1609</th>\n",
              "      <td>greenland lost tiny fraction ice mass</td>\n",
              "      <td>[evidence-962481, evidence-1200633, evidence-7...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-1020</th>\n",
              "      <td>global reef crisis not necessarily mean extinc...</td>\n",
              "      <td>[evidence-642301, evidence-161852, evidence-67...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-2599</th>\n",
              "      <td>small amount active substance cause large effect</td>\n",
              "      <td>[evidence-834109, evidence-1175545, evidence-8...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   claim_text  \\\n",
              "claim-2967             contribution waste heat global climate   \n",
              "claim-979   warm weather worsened recent drought included ...   \n",
              "claim-1609              greenland lost tiny fraction ice mass   \n",
              "claim-1020  global reef crisis not necessarily mean extinc...   \n",
              "claim-2599   small amount active substance cause large effect   \n",
              "\n",
              "                                                    evidences  \n",
              "claim-2967  [evidence-308923, evidence-213569, evidence-63...  \n",
              "claim-979   [evidence-178433, evidence-421870, evidence-43...  \n",
              "claim-1609  [evidence-962481, evidence-1200633, evidence-7...  \n",
              "claim-1020  [evidence-642301, evidence-161852, evidence-67...  \n",
              "claim-2599  [evidence-834109, evidence-1175545, evidence-8...  "
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Try to use the model on unseen test claims:\n",
        "# Get all evidences that are likely to be relevant using a similarity score\n",
        "\n",
        "# Vectorizing preprocessed text\n",
        "# TODO: replace Tfidf with contextual embedding\n",
        "vectorizer = TfidfVectorizer()\n",
        "all_texts = pd.concat([processed_evidence, processed_train_claim])\n",
        "vectorizer.fit(all_texts)\n",
        "\n",
        "evidence_tfidf = vectorizer.transform(processed_evidence)\n",
        "test_tfidf = vectorizer.transform(processed_test)\n",
        "\n",
        "similarity_matrix = cosine_similarity(test_tfidf, evidence_tfidf)\n",
        "\n",
        "def getTopN(similarity_matrix, test, evidence, n):\n",
        "  test = test.to_frame(name='claim_text')\n",
        "  top_indices = np.argsort(-similarity_matrix, axis = 1)[:, :n]\n",
        "  top_evidence = [[str(evidence.index[i]) for i in row] for row in top_indices]\n",
        "  test['evidences'] = top_evidence\n",
        "  return test\n",
        "\n",
        "test_with_evi = getTopN(similarity_matrix, processed_test, processed_evidence, 10)\n",
        "test_with_evi.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Asura\\AppData\\Local\\Temp\\ipykernel_26768\\418587428.py:7: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  test_claim = tfidf_claim[i]\n",
            "C:\\Users\\Asura\\AppData\\Local\\Temp\\ipykernel_26768\\418587428.py:8: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  evidences = tfidf_evi[i]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# format test data to be put into model\n",
        "def prepareTestData():\n",
        "    tfidf_claim = test_with_evi['claim_text']\n",
        "    tfidf_evi = test_with_evi['evidences']\n",
        "    text_lst = []\n",
        "    for i in range(len(tfidf_claim)):\n",
        "        test_claim = tfidf_claim[i]\n",
        "        evidences = tfidf_evi[i]\n",
        "        for j in evidences:\n",
        "            text = test_claim + SPECIAL_TOKEN + processed_evidence[j]\n",
        "            text_lst.append(text)\n",
        "    claim_evi = {'text': text_lst, 'label':'unrelated'} # still include the label field to avoid index error\n",
        "    return pd.DataFrame(claim_evi)\n",
        "\n",
        "preparedTest = prepareTestData()\n",
        "preparedTest.head()\n",
        "test_dataset = TextDataset(preparedTest)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=20, shuffle=False, collate_fn=collate_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('contribution waste heat global climate <SPE_TOKEN> global forcing waste heat', 'unrelated')\n",
            "tensor([1028, 1575,   50,    4,    3,    1,    4,  502, 1575,   50,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0], device='cuda:0')\n",
            "tensor([1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "Counter({1: 1028, 0: 502})\n"
          ]
        }
      ],
      "source": [
        "# manually check the results since we don't have the labels:\n",
        "from collections import Counter\n",
        "print(test_dataset[0])\n",
        "test_batch = next(iter(test_dataloader))\n",
        "print(test_batch[0][0]) # first input\n",
        "\n",
        "test_outputs = best_model(test_batch[0])\n",
        "predicted_labels = torch.argmax(test_outputs, dim=1)\n",
        "print(predicted_labels)\n",
        "\n",
        "\n",
        "def get_all_predictions(model, test_loader):\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    for inputs, _, _ in test_loader:\n",
        "        outputs = model(inputs)\n",
        "        predicted_labels = torch.argmax(outputs, dim=1)\n",
        "        all_predictions.extend(predicted_labels.cpu().numpy())\n",
        "    return all_predictions\n",
        "\n",
        "# check class imbalance\n",
        "def count_labels(predictions):\n",
        "    label_counts = Counter(predictions)\n",
        "    return label_counts\n",
        "\n",
        "\n",
        "# get all predictions\n",
        "\n",
        "all_predictions = get_all_predictions(best_model, test_dataloader)\n",
        "print(count_labels(all_predictions))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Asura\\AppData\\Local\\Temp\\ipykernel_26768\\494974565.py:10: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  claim = claims[i]\n",
            "C:\\Users\\Asura\\AppData\\Local\\Temp\\ipykernel_26768\\494974565.py:11: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  curr_evidences = evidences[i]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>claim-2967</th>\n",
              "      <td>contribution waste heat global climate</td>\n",
              "      <td>[evidence-213569, evidence-631339, evidence-19...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-979</th>\n",
              "      <td>warm weather worsened recent drought included ...</td>\n",
              "      <td>[evidence-178433, evidence-421870, evidence-43...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-1609</th>\n",
              "      <td>greenland lost tiny fraction ice mass</td>\n",
              "      <td>[evidence-1200633, evidence-726093, evidence-6...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-1020</th>\n",
              "      <td>global reef crisis not necessarily mean extinc...</td>\n",
              "      <td>[evidence-670186, evidence-542625, evidence-16...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-2599</th>\n",
              "      <td>small amount active substance cause large effect</td>\n",
              "      <td>[evidence-860747, evidence-1045673]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                         text  \\\n",
              "claim-2967             contribution waste heat global climate   \n",
              "claim-979   warm weather worsened recent drought included ...   \n",
              "claim-1609              greenland lost tiny fraction ice mass   \n",
              "claim-1020  global reef crisis not necessarily mean extinc...   \n",
              "claim-2599   small amount active substance cause large effect   \n",
              "\n",
              "                                                        label  \n",
              "claim-2967  [evidence-213569, evidence-631339, evidence-19...  \n",
              "claim-979   [evidence-178433, evidence-421870, evidence-43...  \n",
              "claim-1609  [evidence-1200633, evidence-726093, evidence-6...  \n",
              "claim-1020  [evidence-670186, evidence-542625, evidence-16...  \n",
              "claim-2599                [evidence-860747, evidence-1045673]  "
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# update test_with_evi with model results\n",
        "\n",
        "def filter_relevant_evidences(claim_evidences, model_classifications, default=True):\n",
        "    claims = claim_evidences['claim_text']\n",
        "    evidences = claim_evidences['evidences']\n",
        "    filtered_evidences = []\n",
        "\n",
        "    classifications_index = 0\n",
        "    for i in range(len(claims)):\n",
        "        claim = claims[i]\n",
        "        curr_evidences = evidences[i]\n",
        "        curr_filtered = []\n",
        "        for evidence in curr_evidences:\n",
        "            # 0 => related\n",
        "            if model_classifications[classifications_index] == 0:\n",
        "                curr_filtered.append(evidence)\n",
        "\n",
        "            classifications_index += 1\n",
        "\n",
        "        if default and len(curr_filtered) == 0:\n",
        "            # keep the most similar evidence if all are deemed unrelated\n",
        "            curr_filtered.append(curr_evidences[0]) \n",
        "\n",
        "        filtered_evidences.append(curr_filtered)\n",
        "\n",
        "    claim_evi = {'text': claims, 'label': filtered_evidences} # still include the label field to avoid index error\n",
        "    return pd.DataFrame(claim_evi)\n",
        "\n",
        "filtered_claim_evidences = filter_relevant_evidences(test_with_evi, all_predictions)\n",
        "filtered_claim_evidences.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "claim-2967    [evidence-213569, evidence-631339, evidence-19...\n",
            "claim-979     [evidence-178433, evidence-421870, evidence-43...\n",
            "claim-1609    [evidence-1200633, evidence-726093, evidence-6...\n",
            "claim-1020    [evidence-670186, evidence-542625, evidence-16...\n",
            "claim-2599                  [evidence-860747, evidence-1045673]\n",
            "                                    ...                        \n",
            "claim-293                                    [evidence-1112386]\n",
            "claim-910                                     [evidence-197221]\n",
            "claim-2815    [evidence-786294, evidence-700183, evidence-12...\n",
            "claim-1652    [evidence-386828, evidence-442810, evidence-35...\n",
            "claim-1212                                    [evidence-162296]\n",
            "Name: label, Length: 153, dtype: object\n"
          ]
        }
      ],
      "source": [
        "print(filtered_claim_evidences[\"label\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The contribution of waste heat to the global climate is 0.028 W/m2.\n",
            "=========================================================================================\n",
            "“Warm weather worsened the most recent five-year drought, which included the driest four-year period on record in terms of statewide precipitation.\n",
            "Another form of severe weather is drought, which is a prolonged period of persistently dry weather (that is, absence of precipitation).\n",
            "This is worsened by extreme weather events caused by climate change.\n",
            "=========================================================================================\n",
            "Greenland has only lost a tiny fraction of its ice mass.\n",
            "With widespread degradation of highly biodiverse habitats such as coral reefs and rainforests, as well as other areas, the vast majority of these extinctions are thought to be \"undocumented\", as the species are undiscovered at the time of their extinction, or no one has yet discovered their extinction.\n",
            "The Greenland ice sheet (, ) is a vast body of ice covering , roughly 80% of the surface of Greenland.\n",
            "=========================================================================================\n",
            "“The global reef crisis does not necessarily mean extinction for coral species.\n",
            "With widespread degradation of highly biodiverse habitats such as coral reefs and rainforests, as well as other areas, the vast majority of these extinctions are thought to be \"undocumented\", as the species are undiscovered at the time of their extinction, or no one has yet discovered their extinction.\n",
            "More detailed methods for determining the health of coral reefs that take into account long-term changes to the coral ecosystems and better-informed conservation policies are necessary to protect coral reefs in the years to come.\n",
            "=========================================================================================\n",
            "Small amounts of very active substances can cause large effects.\n",
            "They are loosely divided into causes, effects and mitigation, noting that effects are interconnected and can cause new effects.\n"
          ]
        }
      ],
      "source": [
        "# manually check some of them to ensure they are somewhat correct\n",
        "\n",
        "# need to keep at least one for this one\n",
        "print(test_claims['claim-2967'])\n",
        "print('=' * 89)\n",
        "\n",
        "print(test_claims['claim-979'])\n",
        "print(evidence['evidence-178433'])\n",
        "print(evidence['evidence-421870'])\n",
        "print('=' * 89)\n",
        "\n",
        "print(test_claims['claim-1609'])\n",
        "print(evidence['evidence-382341'])\n",
        "print(evidence['evidence-726093'])\n",
        "\n",
        "print('=' * 89)\n",
        "\n",
        "print(test_claims['claim-1020'])\n",
        "print(evidence['evidence-382341'])\n",
        "print(evidence['evidence-542625'])\n",
        "print('=' * 89)\n",
        "print(test_claims['claim-2599'])\n",
        "print(evidence['evidence-860747'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Asura\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\Asura\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Asura\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>claim_text</th>\n",
              "      <th>claim_label</th>\n",
              "      <th>evidences</th>\n",
              "      <th>pro_claim_text</th>\n",
              "      <th>pro_evidences</th>\n",
              "      <th>evidence_ids</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>claim-1937</th>\n",
              "      <td>Not only is there no scientific evidence that ...</td>\n",
              "      <td>DISPUTED</td>\n",
              "      <td>[evidence-442946, evidence-1194317, evidence-1...</td>\n",
              "      <td>scientific evidence pollutant higher concentra...</td>\n",
              "      <td>[high concentration time atmospheric concentra...</td>\n",
              "      <td>[442946, 1194317, 12171]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-126</th>\n",
              "      <td>El Niño drove record highs in global temperatu...</td>\n",
              "      <td>REFUTES</td>\n",
              "      <td>[evidence-338219, evidence-1127398]</td>\n",
              "      <td>el niño drove record high global temperature s...</td>\n",
              "      <td>[climate change due natural force human activi...</td>\n",
              "      <td>[338219, 1127398]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-2510</th>\n",
              "      <td>In 1946, PDO switched to a cool phase.</td>\n",
              "      <td>SUPPORTS</td>\n",
              "      <td>[evidence-530063, evidence-984887]</td>\n",
              "      <td>pdo switched cool phase</td>\n",
              "      <td>[evidence reversal prevailing polarity meaning...</td>\n",
              "      <td>[530063, 984887]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-2021</th>\n",
              "      <td>Weather Channel co-founder John Coleman provid...</td>\n",
              "      <td>DISPUTED</td>\n",
              "      <td>[evidence-1177431, evidence-782448, evidence-5...</td>\n",
              "      <td>weather channel cofounder john coleman provide...</td>\n",
              "      <td>[convincing scientific evidence human release ...</td>\n",
              "      <td>[1177431, 782448, 540069, 352655, 1007867]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-2449</th>\n",
              "      <td>\"January 2008 capped a 12 month period of glob...</td>\n",
              "      <td>NOT_ENOUGH_INFO</td>\n",
              "      <td>[evidence-1010750, evidence-91661, evidence-72...</td>\n",
              "      <td>january capped month period global temperature...</td>\n",
              "      <td>[average temperature c f, iranian persian cale...</td>\n",
              "      <td>[1010750, 91661, 722725, 554161, 430839]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   claim_text  \\\n",
              "claim-1937  Not only is there no scientific evidence that ...   \n",
              "claim-126   El Niño drove record highs in global temperatu...   \n",
              "claim-2510             In 1946, PDO switched to a cool phase.   \n",
              "claim-2021  Weather Channel co-founder John Coleman provid...   \n",
              "claim-2449  \"January 2008 capped a 12 month period of glob...   \n",
              "\n",
              "                claim_label  \\\n",
              "claim-1937         DISPUTED   \n",
              "claim-126           REFUTES   \n",
              "claim-2510         SUPPORTS   \n",
              "claim-2021         DISPUTED   \n",
              "claim-2449  NOT_ENOUGH_INFO   \n",
              "\n",
              "                                                    evidences  \\\n",
              "claim-1937  [evidence-442946, evidence-1194317, evidence-1...   \n",
              "claim-126                 [evidence-338219, evidence-1127398]   \n",
              "claim-2510                 [evidence-530063, evidence-984887]   \n",
              "claim-2021  [evidence-1177431, evidence-782448, evidence-5...   \n",
              "claim-2449  [evidence-1010750, evidence-91661, evidence-72...   \n",
              "\n",
              "                                               pro_claim_text  \\\n",
              "claim-1937  scientific evidence pollutant higher concentra...   \n",
              "claim-126   el niño drove record high global temperature s...   \n",
              "claim-2510                            pdo switched cool phase   \n",
              "claim-2021  weather channel cofounder john coleman provide...   \n",
              "claim-2449  january capped month period global temperature...   \n",
              "\n",
              "                                                pro_evidences  \\\n",
              "claim-1937  [high concentration time atmospheric concentra...   \n",
              "claim-126   [climate change due natural force human activi...   \n",
              "claim-2510  [evidence reversal prevailing polarity meaning...   \n",
              "claim-2021  [convincing scientific evidence human release ...   \n",
              "claim-2449  [average temperature c f, iranian persian cale...   \n",
              "\n",
              "                                          evidence_ids  \n",
              "claim-1937                    [442946, 1194317, 12171]  \n",
              "claim-126                            [338219, 1127398]  \n",
              "claim-2510                            [530063, 984887]  \n",
              "claim-2021  [1177431, 782448, 540069, 352655, 1007867]  \n",
              "claim-2449    [1010750, 91661, 722725, 554161, 430839]  "
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stopwords = set(stopwords.words('english'))\n",
        "wnl = WordNetLemmatizer()\n",
        "evidence = pd.read_json(\"../data/evidence.json\",typ='series')\n",
        "evidence_pd = pd.DataFrame(evidence).reset_index()\n",
        "evidence_pd.columns = ['claim_id', 'claims']\n",
        "evidence_pd[\"pro_evidence\"] = evidence_pd[\"claims\"].apply(str.lower)\n",
        "evidence_pd[\"pro_evidence\"] = evidence_pd[\"pro_evidence\"].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
        "evidence_pd[\"pro_evidence\"] = evidence_pd[\"pro_evidence\"].apply(lambda x: ' '.join(wnl.lemmatize(word) for word in x.split( ) if word not in stopwords and word.isalpha()))\n",
        "\n",
        "train_data = pd.read_json(\"../data/train-claims.json\")\n",
        "train_data = train_data.transpose()\n",
        "train_data[\"pro_claim_text\"] = train_data['claim_text'].apply(str.lower)\n",
        "train_data[\"pro_claim_text\"] = train_data[\"pro_claim_text\"].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
        "train_data[\"pro_claim_text\"] = train_data[\"pro_claim_text\"].apply(lambda x: ' '.join(wnl.lemmatize(word) for word in x.split( ) if word not in stopwords and word.isalpha()))\n",
        "\n",
        "train_data[\"pro_evidences\"] = train_data[\"evidences\"].apply(lambda x:  [evidence_pd.iloc[int(re.sub(\"[^0-9]\", \"\", i))][\"pro_evidence\"] for i in x])\n",
        "train_data[\"evidence_ids\"] = train_data[\"evidences\"].apply(lambda x:  [int(re.sub(\"[^0-9]\", \"\", i)) for i in x])\n",
        "train_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "id2label = {0:\"SUPPORTS\", 1:\"NOT_ENOUGH_INFO\", 2:\"REFUTES\", 3:\"DISPUTED\"}\n",
        "label2id = {\"SUPPORTS\":0, \"NOT_ENOUGH_INFO\":1, \"REFUTES\":2, \"DISPUTED\":3}\n",
        "id2token = {0:\"<pad>\", 1:\"<cls>\", 2:\"<sep>\", 3:\"<unk>\"}\n",
        "token2id = {\"<pad>\": 0, \"<cls>\": 1, \"<sep>\": 2, \"<unk>\": 3}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "train_words = (train_data['pro_claim_text'].apply(lambda x : x.split())).tolist()\n",
        "evidence_words = (evidence_pd[\"pro_evidence\"].apply(lambda x : x.split())).tolist()\n",
        "train_words1 = [x for xs in train_words for x in xs ]\n",
        "evidence_words1 = [x for xs in evidence_words for x in xs ]\n",
        "vocab = list(set(train_words1 + evidence_words1))\n",
        "word_counts = Counter(train_words1 + evidence_words1)\n",
        "idx = 4\n",
        "for i, j in word_counts.items():\n",
        "  if j > 5:\n",
        "    id2token[idx] = i\n",
        "    token2id[i] = idx\n",
        "    idx +=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "idx = 4\n",
        "for i, j in word_counts.items():\n",
        "  if j > 5:\n",
        "    id2token[idx] = i\n",
        "    token2id[i] = idx\n",
        "    idx +=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "def wordtointvec(inputs, token2id):\n",
        "  lst = []\n",
        "  for i in inputs:\n",
        "\n",
        "    sent = []\n",
        "    for w in i.split(\" \"):\n",
        "      sent.append(token2id.get(w, token2id[\"<unk>\"]))\n",
        "    lst.append(sent)\n",
        "\n",
        "  return lst\n",
        "def wordtointvec4evi(inputs, token2id):\n",
        "  lst = []\n",
        "  for x in inputs:\n",
        "    sent = []\n",
        "    for i in x:\n",
        "      insent = []\n",
        "      for w in i.split(\" \"):\n",
        "        insent.append(token2id.get(w, token2id[\"<unk>\"]))\n",
        "      sent.append(insent)\n",
        "\n",
        "    lst.append(sent)\n",
        "\n",
        "  return lst\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "def addpadding(l, inputs, token2id):\n",
        "  lst = []\n",
        "  for i in inputs:\n",
        "    if len(i) < l:\n",
        "      lst.append([token2id[\"<cls>\"]]+ i + [token2id[\"<sep>\"]] + (l-len(i)) * [token2id[\"<pad>\"]])\n",
        "    else:\n",
        "\n",
        "      lst.append([token2id[\"<cls>\"]]+ i[:l] + [token2id[\"<sep>\"]] )\n",
        "  return lst\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_vec = wordtointvec(train_data['pro_claim_text'],token2id)\n",
        "train_evi_vec = wordtointvec4evi(train_data['pro_evidences'],token2id)\n",
        "#evi_vec = wordtointvec(evidence_pd[\"pro_evidence\"],token2id)\n",
        "#dev_vec = wordtointvec(dev['pro_claim_text'],token2id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 20, 8, 52, 110, 8, 851, 278, 279, 1843, 14, 15, 2421, 8, 765, 7, 1001, 3226, 2957, 19214, 3, 8271, 38097, 154, 2, 13, 237, 95, 299, 1791, 8, 765, 4353, 103, 22355, 170, 784, 10925, 122, 121, 7018, 20717, 2, 7, 278, 279, 8, 22501, 894, 13, 2116, 6789, 371, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ],
      "source": [
        "train_in = addpadding(34, train_vec, token2id)\n",
        "#train_evi_in = addpadding(100, train_evi_vec, token2id)\n",
        "#evi_in = addpadding(100, evi_vec, token2id)\n",
        "#dev_in =addpadding(34, dev_vec, token2id)\n",
        "max_len = 280\n",
        "for i in range(len(train_in)):\n",
        "  for j in train_evi_vec[i]:\n",
        "    train_in[i].append(token2id[\"<sep>\"])\n",
        "    train_in[i].extend(j)\n",
        "  train_in[i].append(token2id[\"<sep>\"])\n",
        "  if len(train_in[i]) < max_len:\n",
        "\n",
        "    train_in[i].extend([token2id[\"<pad>\"]] * (max_len - len(train_in[i])))\n",
        "\n",
        "print(train_in[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "# max_len = 550\n",
        "# train_in = [] \n",
        "# for i in range(len(processed_train_claim)):\n",
        "#   train_in.append(processed_train_claim[i])\n",
        "\n",
        "# for i in range(len(processed_train_claim)):\n",
        "#   for j in train_data[\"pro_evidences\"][i]:\n",
        "#     train_in[i] += \"<sep>\"\n",
        "#     train_in[i] += j\n",
        "#   train_in[i]+= \"<sep>\"\n",
        "#   if len(train_in[i]) < max_len:\n",
        "#     for j in range(max_len - len(train_in[i])):\n",
        "#       train_in[i] += \" \"\n",
        "#       train_in[i] += \"<pad>\"\n",
        "# train_in = vectorizer.transform(train_in).toarray()\n",
        "# print(train_in[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "class TrainDataset(Dataset):\n",
        "  def __init__(self, text_data, label_data):\n",
        "    self.text_data = text_data\n",
        "    self.label_data = [label2id[i] for i in label_data]\n",
        "    \n",
        "\n",
        "  def __getitem__ (self, index):\n",
        "    return [self.text_data[index], self.label_data[index]]\n",
        "  def __len__(self):\n",
        "    return len(self.text_data)\n",
        "\n",
        "  def collate_fn(self, data):\n",
        "    q = []\n",
        "    #evi = []\n",
        "    labels = []\n",
        "    for sen, lab in data:\n",
        "      q.append(sen)\n",
        "      labels.append(lab)\n",
        "    batch_encoding = {}\n",
        "    batch_encoding[\"queries\"] =  torch.LongTensor(q)\n",
        "\n",
        "    batch_encoding[\"labels\"] = torch.LongTensor(labels)\n",
        "    return batch_encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset = TrainDataset(train_in, train_data[\"claim_label\"])\n",
        "dataloader = DataLoader(train_dataset, batch_size = 20, shuffle = True, num_workers = 0,collate_fn=train_dataset.collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AttentionBlock(nn.Module):\n",
        "  def __init__(self, embed_dim, n_heads, drop_prob):\n",
        "    super(AttentionBlock , self).__init__()\n",
        "    self.att = nn.MultiheadAttention(embed_dim, n_heads)\n",
        "    self.ffn = nn.Sequential(\n",
        "        nn.Linear(embed_dim, 4*embed_dim),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4*embed_dim, embed_dim)\n",
        "    )\n",
        "    self.ln1 = nn.LayerNorm(embed_dim, eps = 1e-6)\n",
        "    self.ln2 = nn.LayerNorm(embed_dim, eps = 1e-6)\n",
        "    self.dropout1 = nn.Dropout(drop_prob)\n",
        "    self.dropout2 = nn.Dropout(drop_prob)\n",
        "  def forward(self, inputs):\n",
        "    attn_out, _ = self.att(inputs, inputs,inputs)\n",
        "    attn_out = self.dropout1(attn_out)\n",
        "    out1 = self.ln1(inputs + attn_out)\n",
        "    ffn_output = self.ffn(out1)\n",
        "    ffn_output = self.dropout2(ffn_output)\n",
        "    return self.ln2(out1 + ffn_output)\n",
        "class ClassificationModel(nn.Module):\n",
        "  def __init__(self, embed_dim, wordlist_size, block_size, drop_prob, n_classes, n_heads):\n",
        "    super(ClassificationModel , self).__init__()\n",
        "    self.token_embed = nn.Embedding(wordlist_size, embed_dim)\n",
        "    self.pos_embed = nn.Embedding(block_size, embed_dim)\n",
        "    self.AttentionBlock = AttentionBlock(embed_dim, n_heads, drop_prob)\n",
        "    self.ln1 = nn.LayerNorm(embed_dim)\n",
        "    self.ln2 = nn.LayerNorm(embed_dim)\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Linear(embed_dim, embed_dim),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(drop_prob),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(embed_dim, n_classes),\n",
        "        nn.Softmax(dim = 1)\n",
        "    )\n",
        "  def forward(self, token):\n",
        "    r,c = token.shape\n",
        "    emb = self.token_embed(token)\n",
        "    emb = emb+ self.pos_embed(torch.arange(c, device = device))\n",
        "    out = self.AttentionBlock(emb)\n",
        "    out = out.mean(dim = 1)\n",
        "    out = self.classifier(out)\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "classification_model = ClassificationModel(embed_dim = 512, wordlist_size = len(id2token), block_size = 280, drop_prob = 0.1, n_classes = 4, n_heads = 4)\n",
        "classification_model.to(device)\n",
        "lr = 0.0001\n",
        "optimizer = torch.optim.Adam(classification_model.parameters(), lr=lr)\n",
        "loss_t = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def train():\n",
        "  classification_model.train()\n",
        "  total_loss = 0.\n",
        "  #start_time = time.time()\n",
        "  for (i, targets) in enumerate(dataloader):\n",
        "    optimizer.zero_grad()\n",
        "    out = classification_model(targets[\"queries\"].cuda())\n",
        "    loss = loss_t(out, targets[\"labels\"].cuda())\n",
        "    #loss = loss\n",
        "    #torch.nn.utils.clip_grad_norm_()\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    total_loss += loss.item()\n",
        "\n",
        "\n",
        "def evaluate(model, input, out):\n",
        "    lst = []\n",
        "    start = 0\n",
        "    batch_size = 50\n",
        "    in_len = len(input[0])\n",
        "    model.eval()\n",
        "    correct_count = 0\n",
        "    while start < len(out):\n",
        "        end = min(start+ batch_size, len(out))\n",
        "\n",
        "        model_input = torch.LongTensor(input[start:end]).view(-1, in_len).cuda()\n",
        "\n",
        "        model_output = model(model_input)\n",
        "        model_output = torch.argmax(model_output, 1).tolist()\n",
        "\n",
        "        for i, j in zip(model_output, out[start: end]):\n",
        "            if i == j:\n",
        "                correct_count += 1\n",
        "\n",
        "        start = end\n",
        "    lst = correct_count / len(out)\n",
        "    print(\"\\n\")\n",
        "    print(\"Classification Accuracy: %.3f\" % lst)\n",
        "    print(\"\\n\")\n",
        "\n",
        "    model.train()\n",
        "    return lst\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict( input, model):\n",
        "    lst = []\n",
        "    start = 0\n",
        "    batch_size = 50\n",
        "    in_len = len(input[0])\n",
        "    model.eval()\n",
        "\n",
        "    \n",
        "    \n",
        "    while start < len(input):\n",
        "        end = min(start+ batch_size, len(input))\n",
        "        \n",
        "        model_input= torch.LongTensor(input[start:end]).view(-1, in_len).cuda()\n",
        "        \n",
        "        model_output = model(model_input)\n",
        "        model_output = torch.argmax(model_output, 1).tolist()\n",
        "        lst.extend(model_output)\n",
        "        \n",
        "        start = end\n",
        "\n",
        "    return lst"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'dev_in' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[41], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      4\u001b[0m train()\n\u001b[1;32m----> 5\u001b[0m f_score \u001b[38;5;241m=\u001b[39m evaluate(classification_model, \u001b[43mdev_in\u001b[49m, dev_label)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfscore\u001b[39m\u001b[38;5;124m\"\u001b[39m, f_score)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'dev_in' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "epochs = 3\n",
        "for epoch in range(epochs):\n",
        "  loss = 0\n",
        "  train()\n",
        "  f_score = evaluate(classification_model, dev_in, dev_label)\n",
        "  print(\"fscore\", f_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [],
      "source": [
        "filtered_claim_evidences[\"pro_evidences\"] = filtered_claim_evidences[\"label\"].apply(lambda x:  [evidence_pd.iloc[int(re.sub(\"[^0-9]\", \"\", i))][\"pro_evidence\"] for i in x])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>pro_evidences</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>claim-2967</th>\n",
              "      <td>contribution waste heat global climate</td>\n",
              "      <td>[evidence-213569, evidence-631339, evidence-19...</td>\n",
              "      <td>[thus waste heat engine may one least expensiv...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-979</th>\n",
              "      <td>warm weather worsened recent drought included ...</td>\n",
              "      <td>[evidence-178433, evidence-421870, evidence-43...</td>\n",
              "      <td>[another form severe weather drought prolonged...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-1609</th>\n",
              "      <td>greenland lost tiny fraction ice mass</td>\n",
              "      <td>[evidence-1200633, evidence-726093, evidence-6...</td>\n",
              "      <td>[land ice sheet antarctica greenland losing ma...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-1020</th>\n",
              "      <td>global reef crisis not necessarily mean extinc...</td>\n",
              "      <td>[evidence-670186, evidence-542625, evidence-16...</td>\n",
              "      <td>[bleaching coral great barrier reef killed per...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-2599</th>\n",
              "      <td>small amount active substance cause large effect</td>\n",
              "      <td>[evidence-860747, evidence-1045673]</td>\n",
              "      <td>[loosely divided cause effect mitigation notin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-293</th>\n",
              "      <td>measuring equipment get old need replacing oft...</td>\n",
              "      <td>[evidence-1112386]</td>\n",
              "      <td>[opened replacing old georgvonneumayerstation]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-910</th>\n",
              "      <td>cement iron steel petroleum refining industry ...</td>\n",
              "      <td>[evidence-197221]</td>\n",
              "      <td>[specifically reduction iron steel production ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-2815</th>\n",
              "      <td>new study surface warming solar cycle found ti...</td>\n",
              "      <td>[evidence-786294, evidence-700183, evidence-12...</td>\n",
              "      <td>[solar cycle solar cycle since extensive recor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-1652</th>\n",
              "      <td>strong effect observed many different measurement</td>\n",
              "      <td>[evidence-386828, evidence-442810, evidence-35...</td>\n",
              "      <td>[major aspect climate change demonstrated dire...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-1212</th>\n",
              "      <td>technical lingo social cost carbon would negative</td>\n",
              "      <td>[evidence-162296]</td>\n",
              "      <td>[lingo livery stable house oregon trail exhibi...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>153 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                         text  \\\n",
              "claim-2967             contribution waste heat global climate   \n",
              "claim-979   warm weather worsened recent drought included ...   \n",
              "claim-1609              greenland lost tiny fraction ice mass   \n",
              "claim-1020  global reef crisis not necessarily mean extinc...   \n",
              "claim-2599   small amount active substance cause large effect   \n",
              "...                                                       ...   \n",
              "claim-293   measuring equipment get old need replacing oft...   \n",
              "claim-910   cement iron steel petroleum refining industry ...   \n",
              "claim-2815  new study surface warming solar cycle found ti...   \n",
              "claim-1652  strong effect observed many different measurement   \n",
              "claim-1212  technical lingo social cost carbon would negative   \n",
              "\n",
              "                                                        label  \\\n",
              "claim-2967  [evidence-213569, evidence-631339, evidence-19...   \n",
              "claim-979   [evidence-178433, evidence-421870, evidence-43...   \n",
              "claim-1609  [evidence-1200633, evidence-726093, evidence-6...   \n",
              "claim-1020  [evidence-670186, evidence-542625, evidence-16...   \n",
              "claim-2599                [evidence-860747, evidence-1045673]   \n",
              "...                                                       ...   \n",
              "claim-293                                  [evidence-1112386]   \n",
              "claim-910                                   [evidence-197221]   \n",
              "claim-2815  [evidence-786294, evidence-700183, evidence-12...   \n",
              "claim-1652  [evidence-386828, evidence-442810, evidence-35...   \n",
              "claim-1212                                  [evidence-162296]   \n",
              "\n",
              "                                                pro_evidences  \n",
              "claim-2967  [thus waste heat engine may one least expensiv...  \n",
              "claim-979   [another form severe weather drought prolonged...  \n",
              "claim-1609  [land ice sheet antarctica greenland losing ma...  \n",
              "claim-1020  [bleaching coral great barrier reef killed per...  \n",
              "claim-2599  [loosely divided cause effect mitigation notin...  \n",
              "...                                                       ...  \n",
              "claim-293      [opened replacing old georgvonneumayerstation]  \n",
              "claim-910   [specifically reduction iron steel production ...  \n",
              "claim-2815  [solar cycle solar cycle since extensive recor...  \n",
              "claim-1652  [major aspect climate change demonstrated dire...  \n",
              "claim-1212  [lingo livery stable house oregon trail exhibi...  \n",
              "\n",
              "[153 rows x 3 columns]"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "filtered_claim_evidences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 2800, 1935, 713, 21, 121, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1828, 1935, 713, 3294, 25, 888, 1250, 2312, 2473, 4731, 1935, 713, 1079, 150, 2, 4253, 4708, 3, 4387, 1768, 904, 78752, 713, 593, 1935, 713, 149, 713, 2, 126, 51, 455, 403, 2816, 1615, 6365, 1935, 713, 381, 1017, 22580, 1738, 1206, 573, 713, 1935, 186, 2, 610, 455, 158, 3789, 11059, 40, 121, 122, 227, 48, 1935, 713, 5989, 2, 4642, 1935, 713, 3294, 46774, 715, 4239, 3294, 2536, 902, 885, 4239, 2878, 1935, 713, 2, 339, 78752, 1935, 713, 1206, 371, 843, 729, 2, 121, 122, 898, 21, 551, 121, 52, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ],
      "source": [
        "filtered_claim_evidences\n",
        "test_vec = wordtointvec(filtered_claim_evidences[\"text\"],token2id)\n",
        "test_in = addpadding(34, test_vec, token2id)\n",
        "test_evi_vec = wordtointvec4evi(filtered_claim_evidences['pro_evidences'],token2id)\n",
        "max_len = 280\n",
        "for i in range(len(test_in)):\n",
        "  for j in test_evi_vec[i]:\n",
        "    test_in[i].append(token2id[\"<sep>\"])\n",
        "    test_in[i].extend(j)\n",
        "  test_in[i].append(token2id[\"<sep>\"])\n",
        "  if len(test_in[i]) < max_len:\n",
        "\n",
        "    test_in[i].extend([token2id[\"<pad>\"]] * (max_len - len(test_in[i])))\n",
        "\n",
        "print(test_in[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_classes = predict(test_in, classification_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mefSOe8eTmGP"
      },
      "source": [
        "## Object Oriented Programming codes here\n",
        "\n",
        "*You can use multiple code snippets. Just add more if needed*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# model adpated from workshop 8\n",
        "class TransformerClassificationModel(nn.Module):\n",
        "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, num_classes, dropout=0.5):\n",
        "        super(TransformerClassificationModel, self).__init__()\n",
        "        from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "        self.model_type = 'Transformer'\n",
        "        self.src_mask = None\n",
        "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
        "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.encoder = nn.Embedding(ntoken, ninp)\n",
        "        self.ninp = ninp\n",
        "        self.classification_head = nn.Linear(ninp, num_classes)  # added classification head\n",
        "        self.init_weights()\n",
        "\n",
        "    def _generate_square_subsequent_mask(self, sz):\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.classification_head.bias.data.zero_()\n",
        "        self.classification_head.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src):\n",
        "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
        "            device = src.device\n",
        "            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
        "            self.src_mask = mask\n",
        "\n",
        "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
        "        src = self.pos_encoder(src)\n",
        "        output = self.transformer_encoder(src, self.src_mask)\n",
        "        # print(\"ENCODER OUTPUT\")\n",
        "        # print(len(output))\n",
        "        output = output.mean(dim=1)  # aggregate across all tokens TODO: check if dim is correct\n",
        "        # print(\"AGGREGATE\")\n",
        "        # print(len(output))\n",
        "        output = self.classification_head(output) \n",
        "        # print(\"FINAL\")\n",
        "        # print(len(output))\n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term) #0::2 means starting with index 0, step = 2\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
