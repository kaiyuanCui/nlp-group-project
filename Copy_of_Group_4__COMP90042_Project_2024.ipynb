{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32yCsRUo8H33"
      },
      "source": [
        "# 2024 COMP90042 Project\n",
        "*Make sure you change the file name with your group id.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCybYoGz8YWQ"
      },
      "source": [
        "# Readme\n",
        "*If there is something to be noted for the marker, please mention here.*\n",
        "\n",
        "*If you are planning to implement a program with Object Oriented Programming style, please put those the bottom of this ipynb file*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6po98qVA8bJD"
      },
      "source": [
        "# 1.DataSet Processing\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjGn8oHh9lew"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvff21Hv8zjk",
        "outputId": "b23d6816-9b04-47d7-ef12-c60b59783bc6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pandas\n",
            "  Downloading pandas-2.2.2-cp311-cp311-win_amd64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: numpy>=1.23.2 in g:\\ana\\envs\\nlp\\lib\\site-packages (from pandas) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in g:\\ana\\envs\\nlp\\lib\\site-packages (from pandas) (2.9.0)\n",
            "Collecting pytz>=2020.1 (from pandas)\n",
            "  Downloading pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas)\n",
            "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: six>=1.5 in g:\\ana\\envs\\nlp\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Downloading pandas-2.2.2-cp311-cp311-win_amd64.whl (11.6 MB)\n",
            "   ---------------------------------------- 0.0/11.6 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.1/11.6 MB 1.8 MB/s eta 0:00:07\n",
            "   - -------------------------------------- 0.4/11.6 MB 4.6 MB/s eta 0:00:03\n",
            "   -- ------------------------------------- 0.8/11.6 MB 6.0 MB/s eta 0:00:02\n",
            "   ---- ----------------------------------- 1.4/11.6 MB 7.3 MB/s eta 0:00:02\n",
            "   ------ --------------------------------- 1.8/11.6 MB 7.6 MB/s eta 0:00:02\n",
            "   -------- ------------------------------- 2.4/11.6 MB 8.6 MB/s eta 0:00:02\n",
            "   ---------- ----------------------------- 3.0/11.6 MB 9.2 MB/s eta 0:00:01\n",
            "   ------------ --------------------------- 3.6/11.6 MB 9.6 MB/s eta 0:00:01\n",
            "   -------------- ------------------------- 4.2/11.6 MB 9.9 MB/s eta 0:00:01\n",
            "   ---------------- ----------------------- 4.8/11.6 MB 10.2 MB/s eta 0:00:01\n",
            "   ------------------ --------------------- 5.3/11.6 MB 10.3 MB/s eta 0:00:01\n",
            "   -------------------- ------------------- 6.0/11.6 MB 10.7 MB/s eta 0:00:01\n",
            "   ---------------------- ----------------- 6.5/11.6 MB 10.7 MB/s eta 0:00:01\n",
            "   ------------------------ --------------- 7.1/11.6 MB 10.9 MB/s eta 0:00:01\n",
            "   -------------------------- ------------- 7.8/11.6 MB 11.0 MB/s eta 0:00:01\n",
            "   ---------------------------- ----------- 8.4/11.6 MB 11.1 MB/s eta 0:00:01\n",
            "   ------------------------------ --------- 9.0/11.6 MB 11.1 MB/s eta 0:00:01\n",
            "   -------------------------------- ------- 9.6/11.6 MB 11.2 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 10.2/11.6 MB 11.2 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 10.8/11.6 MB 12.1 MB/s eta 0:00:01\n",
            "   ---------------------------------------  11.4/11.6 MB 12.3 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 11.6/11.6 MB 12.8 MB/s eta 0:00:00\n",
            "Downloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
            "   ---------------------------------------- 0.0/505.5 kB ? eta -:--:--\n",
            "   --------------------------------------- 505.5/505.5 kB 10.5 MB/s eta 0:00:00\n",
            "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
            "Installing collected packages: pytz, tzdata, pandas\n",
            "Successfully installed pandas-2.2.2 pytz-2024.1 tzdata-2024.1\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MH1Hdrb5NmHM"
      },
      "outputs": [],
      "source": [
        "#Imports\n",
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "2piZvV4OMSa3",
        "outputId": "81a297a7-bae7-474e-9598-4f8d89ba98c2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>claim_text</th>\n",
              "      <th>claim_label</th>\n",
              "      <th>evidences</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>claim-1937</th>\n",
              "      <td>Not only is there no scientific evidence that ...</td>\n",
              "      <td>DISPUTED</td>\n",
              "      <td>[evidence-442946, evidence-1194317, evidence-1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-126</th>\n",
              "      <td>El Niño drove record highs in global temperatu...</td>\n",
              "      <td>REFUTES</td>\n",
              "      <td>[evidence-338219, evidence-1127398]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-2510</th>\n",
              "      <td>In 1946, PDO switched to a cool phase.</td>\n",
              "      <td>SUPPORTS</td>\n",
              "      <td>[evidence-530063, evidence-984887]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-2021</th>\n",
              "      <td>Weather Channel co-founder John Coleman provid...</td>\n",
              "      <td>DISPUTED</td>\n",
              "      <td>[evidence-1177431, evidence-782448, evidence-5...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-2449</th>\n",
              "      <td>\"January 2008 capped a 12 month period of glob...</td>\n",
              "      <td>NOT_ENOUGH_INFO</td>\n",
              "      <td>[evidence-1010750, evidence-91661, evidence-72...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   claim_text  \\\n",
              "claim-1937  Not only is there no scientific evidence that ...   \n",
              "claim-126   El Niño drove record highs in global temperatu...   \n",
              "claim-2510             In 1946, PDO switched to a cool phase.   \n",
              "claim-2021  Weather Channel co-founder John Coleman provid...   \n",
              "claim-2449  \"January 2008 capped a 12 month period of glob...   \n",
              "\n",
              "                claim_label                                          evidences  \n",
              "claim-1937         DISPUTED  [evidence-442946, evidence-1194317, evidence-1...  \n",
              "claim-126           REFUTES                [evidence-338219, evidence-1127398]  \n",
              "claim-2510         SUPPORTS                 [evidence-530063, evidence-984887]  \n",
              "claim-2021         DISPUTED  [evidence-1177431, evidence-782448, evidence-5...  \n",
              "claim-2449  NOT_ENOUGH_INFO  [evidence-1010750, evidence-91661, evidence-72...  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#visualising training data\n",
        "# TODO: put data in drive?\n",
        "train_data = pd.read_json(\"data/train-claims.json\")\n",
        "train_data = train_data.transpose()\n",
        "train_data.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ZKbGFcA2THHP"
      },
      "outputs": [],
      "source": [
        "#visualising evidence data\n",
        "evidence = pd.read_json(\"data/evidence.json\",typ='series')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFNTiC0UMS45",
        "outputId": "18754417-4f86-44d0-dce8-ad236d2d2940"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1208827\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "evidence-0    John Bennet Lawes, English entrepreneur and ag...\n",
              "evidence-1    Lindberg began his professional career at the ...\n",
              "evidence-2    ``Boston (Ladies of Cambridge)'' by Vampire We...\n",
              "evidence-3    Gerald Francis Goyer (born October 20, 1936) w...\n",
              "evidence-4    He detected abnormalities of oxytocinergic fun...\n",
              "dtype: object"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(len(evidence))\n",
        "evidence.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOBi2NyJELHS",
        "outputId": "e608224b-9c6e-446b-ded7-f19f9c049a9b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNy_AXaNjpA7",
        "outputId": "7ab3f523-4903-4392-e86d-78fe0349c413"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Asura\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\Asura\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Asura\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stopwords = set(stopwords.words('english'))\n",
        "wnl = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "MHB_RHiLejWy"
      },
      "outputs": [],
      "source": [
        "evidence_pd = pd.DataFrame(evidence).reset_index()\n",
        "evidence_pd.columns = ['claim_id', 'claims']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "svKVIgmuc9c1",
        "outputId": "2fe0b65d-1b56-4ca9-b09c-0efb4b03075c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>claim_id</th>\n",
              "      <th>claims</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>evidence-0</td>\n",
              "      <td>John Bennet Lawes, English entrepreneur and ag...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>evidence-1</td>\n",
              "      <td>Lindberg began his professional career at the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>evidence-2</td>\n",
              "      <td>``Boston (Ladies of Cambridge)'' by Vampire We...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>evidence-3</td>\n",
              "      <td>Gerald Francis Goyer (born October 20, 1936) w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>evidence-4</td>\n",
              "      <td>He detected abnormalities of oxytocinergic fun...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     claim_id                                             claims\n",
              "0  evidence-0  John Bennet Lawes, English entrepreneur and ag...\n",
              "1  evidence-1  Lindberg began his professional career at the ...\n",
              "2  evidence-2  ``Boston (Ladies of Cambridge)'' by Vampire We...\n",
              "3  evidence-3  Gerald Francis Goyer (born October 20, 1936) w...\n",
              "4  evidence-4  He detected abnormalities of oxytocinergic fun..."
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "evidence_pd.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "iNhVKCykc0M1"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "evidence_pd[\"pro_evidence\"] = evidence_pd[\"claims\"].apply(str.lower)\n",
        "evidence_pd[\"pro_evidence\"] = evidence_pd[\"pro_evidence\"].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
        "evidence_pd[\"pro_evidence\"] = evidence_pd[\"pro_evidence\"].apply(lambda x: ' '.join(wnl.lemmatize(word) for word in x.split( ) if word not in stopwords and word.isalpha()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "rGj0wwBZjseo",
        "outputId": "e3f52a64-197e-4dcf-c874-c54057bcb26e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>claim_id</th>\n",
              "      <th>claims</th>\n",
              "      <th>pro_evidence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>evidence-0</td>\n",
              "      <td>John Bennet Lawes, English entrepreneur and ag...</td>\n",
              "      <td>john bennet lawes english entrepreneur agricul...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>evidence-1</td>\n",
              "      <td>Lindberg began his professional career at the ...</td>\n",
              "      <td>lindberg began professional career age eventua...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>evidence-2</td>\n",
              "      <td>``Boston (Ladies of Cambridge)'' by Vampire We...</td>\n",
              "      <td>boston lady cambridge vampire weekend</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>evidence-3</td>\n",
              "      <td>Gerald Francis Goyer (born October 20, 1936) w...</td>\n",
              "      <td>gerald francis goyer born october professional...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>evidence-4</td>\n",
              "      <td>He detected abnormalities of oxytocinergic fun...</td>\n",
              "      <td>detected abnormality oxytocinergic function sc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1208822</th>\n",
              "      <td>evidence-1208822</td>\n",
              "      <td>Also on the property is a contributing garage ...</td>\n",
              "      <td>also property contributing garage apartment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1208823</th>\n",
              "      <td>evidence-1208823</td>\n",
              "      <td>| class = ``fn org'' | Fyrde | | | | 6110 | | ...</td>\n",
              "      <td>class fn org fyrde volda</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1208824</th>\n",
              "      <td>evidence-1208824</td>\n",
              "      <td>Dragon Storm (game), a role-playing game and c...</td>\n",
              "      <td>dragon storm game roleplaying game collectible...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1208825</th>\n",
              "      <td>evidence-1208825</td>\n",
              "      <td>It states that the Zeriuani ``which is so grea...</td>\n",
              "      <td>state zeriuani great realm tradition relates t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1208826</th>\n",
              "      <td>evidence-1208826</td>\n",
              "      <td>The storyline revolves around a giant plesiosa...</td>\n",
              "      <td>storyline revolves around giant plesiosaur aki...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1208827 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                 claim_id                                             claims  \\\n",
              "0              evidence-0  John Bennet Lawes, English entrepreneur and ag...   \n",
              "1              evidence-1  Lindberg began his professional career at the ...   \n",
              "2              evidence-2  ``Boston (Ladies of Cambridge)'' by Vampire We...   \n",
              "3              evidence-3  Gerald Francis Goyer (born October 20, 1936) w...   \n",
              "4              evidence-4  He detected abnormalities of oxytocinergic fun...   \n",
              "...                   ...                                                ...   \n",
              "1208822  evidence-1208822  Also on the property is a contributing garage ...   \n",
              "1208823  evidence-1208823  | class = ``fn org'' | Fyrde | | | | 6110 | | ...   \n",
              "1208824  evidence-1208824  Dragon Storm (game), a role-playing game and c...   \n",
              "1208825  evidence-1208825  It states that the Zeriuani ``which is so grea...   \n",
              "1208826  evidence-1208826  The storyline revolves around a giant plesiosa...   \n",
              "\n",
              "                                              pro_evidence  \n",
              "0        john bennet lawes english entrepreneur agricul...  \n",
              "1        lindberg began professional career age eventua...  \n",
              "2                    boston lady cambridge vampire weekend  \n",
              "3        gerald francis goyer born october professional...  \n",
              "4        detected abnormality oxytocinergic function sc...  \n",
              "...                                                    ...  \n",
              "1208822        also property contributing garage apartment  \n",
              "1208823                           class fn org fyrde volda  \n",
              "1208824  dragon storm game roleplaying game collectible...  \n",
              "1208825  state zeriuani great realm tradition relates t...  \n",
              "1208826  storyline revolves around giant plesiosaur aki...  \n",
              "\n",
              "[1208827 rows x 3 columns]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evidence_pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "yv9bdKQ9iVJ_",
        "outputId": "ff842241-8f39-43c1-e6ec-031317004aee"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'also property contributing garage apartment'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "i = \"evidence-1208822\"\n",
        "evidence_pd.iloc[int(re.sub(\"[^0-9]\", \"\", i))][\"pro_evidence\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "rw7mHUGHb3f_",
        "outputId": "754d4ee9-47cd-4894-a3bd-fd3966724650"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>claim_text</th>\n",
              "      <th>claim_label</th>\n",
              "      <th>evidences</th>\n",
              "      <th>pro_claim_text</th>\n",
              "      <th>pro_evidences</th>\n",
              "      <th>evidence_ids</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>claim-1937</th>\n",
              "      <td>Not only is there no scientific evidence that ...</td>\n",
              "      <td>DISPUTED</td>\n",
              "      <td>[evidence-442946, evidence-1194317, evidence-1...</td>\n",
              "      <td>scientific evidence pollutant higher concentra...</td>\n",
              "      <td>[high concentration time atmospheric concentra...</td>\n",
              "      <td>[442946, 1194317, 12171]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-126</th>\n",
              "      <td>El Niño drove record highs in global temperatu...</td>\n",
              "      <td>REFUTES</td>\n",
              "      <td>[evidence-338219, evidence-1127398]</td>\n",
              "      <td>el niño drove record high global temperature s...</td>\n",
              "      <td>[climate change due natural force human activi...</td>\n",
              "      <td>[338219, 1127398]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-2510</th>\n",
              "      <td>In 1946, PDO switched to a cool phase.</td>\n",
              "      <td>SUPPORTS</td>\n",
              "      <td>[evidence-530063, evidence-984887]</td>\n",
              "      <td>pdo switched cool phase</td>\n",
              "      <td>[evidence reversal prevailing polarity meaning...</td>\n",
              "      <td>[530063, 984887]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-2021</th>\n",
              "      <td>Weather Channel co-founder John Coleman provid...</td>\n",
              "      <td>DISPUTED</td>\n",
              "      <td>[evidence-1177431, evidence-782448, evidence-5...</td>\n",
              "      <td>weather channel cofounder john coleman provide...</td>\n",
              "      <td>[convincing scientific evidence human release ...</td>\n",
              "      <td>[1177431, 782448, 540069, 352655, 1007867]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-2449</th>\n",
              "      <td>\"January 2008 capped a 12 month period of glob...</td>\n",
              "      <td>NOT_ENOUGH_INFO</td>\n",
              "      <td>[evidence-1010750, evidence-91661, evidence-72...</td>\n",
              "      <td>january capped month period global temperature...</td>\n",
              "      <td>[average temperature c f, iranian persian cale...</td>\n",
              "      <td>[1010750, 91661, 722725, 554161, 430839]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   claim_text  \\\n",
              "claim-1937  Not only is there no scientific evidence that ...   \n",
              "claim-126   El Niño drove record highs in global temperatu...   \n",
              "claim-2510             In 1946, PDO switched to a cool phase.   \n",
              "claim-2021  Weather Channel co-founder John Coleman provid...   \n",
              "claim-2449  \"January 2008 capped a 12 month period of glob...   \n",
              "\n",
              "                claim_label  \\\n",
              "claim-1937         DISPUTED   \n",
              "claim-126           REFUTES   \n",
              "claim-2510         SUPPORTS   \n",
              "claim-2021         DISPUTED   \n",
              "claim-2449  NOT_ENOUGH_INFO   \n",
              "\n",
              "                                                    evidences  \\\n",
              "claim-1937  [evidence-442946, evidence-1194317, evidence-1...   \n",
              "claim-126                 [evidence-338219, evidence-1127398]   \n",
              "claim-2510                 [evidence-530063, evidence-984887]   \n",
              "claim-2021  [evidence-1177431, evidence-782448, evidence-5...   \n",
              "claim-2449  [evidence-1010750, evidence-91661, evidence-72...   \n",
              "\n",
              "                                               pro_claim_text  \\\n",
              "claim-1937  scientific evidence pollutant higher concentra...   \n",
              "claim-126   el niño drove record high global temperature s...   \n",
              "claim-2510                            pdo switched cool phase   \n",
              "claim-2021  weather channel cofounder john coleman provide...   \n",
              "claim-2449  january capped month period global temperature...   \n",
              "\n",
              "                                                pro_evidences  \\\n",
              "claim-1937  [high concentration time atmospheric concentra...   \n",
              "claim-126   [climate change due natural force human activi...   \n",
              "claim-2510  [evidence reversal prevailing polarity meaning...   \n",
              "claim-2021  [convincing scientific evidence human release ...   \n",
              "claim-2449  [average temperature c f, iranian persian cale...   \n",
              "\n",
              "                                          evidence_ids  \n",
              "claim-1937                    [442946, 1194317, 12171]  \n",
              "claim-126                            [338219, 1127398]  \n",
              "claim-2510                            [530063, 984887]  \n",
              "claim-2021  [1177431, 782448, 540069, 352655, 1007867]  \n",
              "claim-2449    [1010750, 91661, 722725, 554161, 430839]  "
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "train_data[\"pro_claim_text\"] = train_data['claim_text'].apply(str.lower)\n",
        "train_data[\"pro_claim_text\"] = train_data[\"pro_claim_text\"].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
        "train_data[\"pro_claim_text\"] = train_data[\"pro_claim_text\"].apply(lambda x: ' '.join(wnl.lemmatize(word) for word in x.split( ) if word not in stopwords and word.isalpha()))\n",
        "\n",
        "train_data[\"pro_evidences\"] = train_data[\"evidences\"].apply(lambda x:  [evidence_pd.iloc[int(re.sub(\"[^0-9]\", \"\", i))][\"pro_evidence\"] for i in x])\n",
        "train_data[\"evidence_ids\"] = train_data[\"evidences\"].apply(lambda x:  [int(re.sub(\"[^0-9]\", \"\", i)) for i in x])\n",
        "train_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Gg2dQrwHczvz",
        "outputId": "d6cb52e9-4053-4ed2-b5b9-ce3b4c9229d5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Asura\\AppData\\Local\\Temp\\ipykernel_5928\\3889600003.py:1: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  train_data[\"pro_evidences\"][0][1]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'plant grow much percent faster concentration ppm co compared ambient condition though assumes change climate limitation nutrient'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data[\"pro_evidences\"][0][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "yKRt6fuencO_",
        "outputId": "486e580c-ab5b-44e4-aa9b-dd3100e9fc37"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>claim_text</th>\n",
              "      <th>claim_label</th>\n",
              "      <th>evidences</th>\n",
              "      <th>pro_claim_text</th>\n",
              "      <th>pro_evidences</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>claim-752</th>\n",
              "      <td>[South Australia] has the most expensive elect...</td>\n",
              "      <td>SUPPORTS</td>\n",
              "      <td>[evidence-67732, evidence-572512]</td>\n",
              "      <td>south australia expensive electricity world</td>\n",
              "      <td>[citation needed south australia highest retai...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-375</th>\n",
              "      <td>when 3 per cent of total annual global emissio...</td>\n",
              "      <td>NOT_ENOUGH_INFO</td>\n",
              "      <td>[evidence-996421, evidence-1080858, evidence-2...</td>\n",
              "      <td>per cent total annual global emission carbon d...</td>\n",
              "      <td>[unep green economy report state aagricultural...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-1266</th>\n",
              "      <td>This means that the world is now 1C warmer tha...</td>\n",
              "      <td>SUPPORTS</td>\n",
              "      <td>[evidence-889933, evidence-694262]</td>\n",
              "      <td>mean world warmer preindustrial time</td>\n",
              "      <td>[multiple independently produced instrumental ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-871</th>\n",
              "      <td>“As it happens, Zika may also be a good model ...</td>\n",
              "      <td>NOT_ENOUGH_INFO</td>\n",
              "      <td>[evidence-422399, evidence-702226, evidence-28...</td>\n",
              "      <td>happens zika may also good model second worryi...</td>\n",
              "      <td>[genetic disorder result deleterious mutation ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-2164</th>\n",
              "      <td>Greenland has only lost a tiny fraction of its...</td>\n",
              "      <td>REFUTES</td>\n",
              "      <td>[evidence-52981, evidence-264761, evidence-947...</td>\n",
              "      <td>greenland lost tiny fraction ice mass</td>\n",
              "      <td>[iceberg calving happened average greenland lo...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   claim_text  \\\n",
              "claim-752   [South Australia] has the most expensive elect...   \n",
              "claim-375   when 3 per cent of total annual global emissio...   \n",
              "claim-1266  This means that the world is now 1C warmer tha...   \n",
              "claim-871   “As it happens, Zika may also be a good model ...   \n",
              "claim-2164  Greenland has only lost a tiny fraction of its...   \n",
              "\n",
              "                claim_label  \\\n",
              "claim-752          SUPPORTS   \n",
              "claim-375   NOT_ENOUGH_INFO   \n",
              "claim-1266         SUPPORTS   \n",
              "claim-871   NOT_ENOUGH_INFO   \n",
              "claim-2164          REFUTES   \n",
              "\n",
              "                                                    evidences  \\\n",
              "claim-752                   [evidence-67732, evidence-572512]   \n",
              "claim-375   [evidence-996421, evidence-1080858, evidence-2...   \n",
              "claim-1266                 [evidence-889933, evidence-694262]   \n",
              "claim-871   [evidence-422399, evidence-702226, evidence-28...   \n",
              "claim-2164  [evidence-52981, evidence-264761, evidence-947...   \n",
              "\n",
              "                                               pro_claim_text  \\\n",
              "claim-752         south australia expensive electricity world   \n",
              "claim-375   per cent total annual global emission carbon d...   \n",
              "claim-1266               mean world warmer preindustrial time   \n",
              "claim-871   happens zika may also good model second worryi...   \n",
              "claim-2164              greenland lost tiny fraction ice mass   \n",
              "\n",
              "                                                pro_evidences  \n",
              "claim-752   [citation needed south australia highest retai...  \n",
              "claim-375   [unep green economy report state aagricultural...  \n",
              "claim-1266  [multiple independently produced instrumental ...  \n",
              "claim-871   [genetic disorder result deleterious mutation ...  \n",
              "claim-2164  [iceberg calving happened average greenland lo...  "
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dev = pd.read_json(\"data/dev-claims.json\")\n",
        "dev = dev.transpose()\n",
        "dev.head()\n",
        "dev[\"pro_claim_text\"] = dev['claim_text'].apply(str.lower)\n",
        "dev[\"pro_claim_text\"] = dev[\"pro_claim_text\"].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
        "dev[\"pro_claim_text\"] = dev[\"pro_claim_text\"].apply(lambda x: ' '.join(wnl.lemmatize(word) for word in x.split( ) if word not in stopwords and word.isalpha()))\n",
        "\n",
        "dev[\"pro_evidences\"] = dev[\"evidences\"].apply(lambda x:  [evidence_pd.iloc[int(re.sub(\"[^0-9]\", \"\", i))][\"pro_evidence\"] for i in x])\n",
        "dev.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "pVG5-Ft_jpbu",
        "outputId": "70de8477-c31d-46d2-9d97-a4a68eb6ada1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>claim_text</th>\n",
              "      <th>pro_claim_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>claim-2967</th>\n",
              "      <td>The contribution of waste heat to the global c...</td>\n",
              "      <td>contribution waste heat global climate</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-979</th>\n",
              "      <td>“Warm weather worsened the most recent five-ye...</td>\n",
              "      <td>warm weather worsened recent fiveyear drought ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-1609</th>\n",
              "      <td>Greenland has only lost a tiny fraction of its...</td>\n",
              "      <td>greenland lost tiny fraction ice mass</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-1020</th>\n",
              "      <td>“The global reef crisis does not necessarily m...</td>\n",
              "      <td>global reef crisis necessarily mean extinction...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claim-2599</th>\n",
              "      <td>Small amounts of very active substances can ca...</td>\n",
              "      <td>small amount active substance cause large effect</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   claim_text  \\\n",
              "claim-2967  The contribution of waste heat to the global c...   \n",
              "claim-979   “Warm weather worsened the most recent five-ye...   \n",
              "claim-1609  Greenland has only lost a tiny fraction of its...   \n",
              "claim-1020  “The global reef crisis does not necessarily m...   \n",
              "claim-2599  Small amounts of very active substances can ca...   \n",
              "\n",
              "                                               pro_claim_text  \n",
              "claim-2967             contribution waste heat global climate  \n",
              "claim-979   warm weather worsened recent fiveyear drought ...  \n",
              "claim-1609              greenland lost tiny fraction ice mass  \n",
              "claim-1020  global reef crisis necessarily mean extinction...  \n",
              "claim-2599   small amount active substance cause large effect  "
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test = pd.read_json(\"data/test-claims-unlabelled.json\")\n",
        "test = test.transpose()\n",
        "\n",
        "test[\"pro_claim_text\"] = test['claim_text'].apply(str.lower)\n",
        "test[\"pro_claim_text\"] = test[\"pro_claim_text\"].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
        "test[\"pro_claim_text\"] = test[\"pro_claim_text\"].apply(lambda x: ' '.join(wnl.lemmatize(word) for word in x.split( ) if word not in stopwords and word.isalpha()))\n",
        "test.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyigDaT5iTa9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JbbnS0a_ztmM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18d0dUtuxT7I"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "yHWbo7W7TuUC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import math\n",
        "import copy\n",
        "from collections import Counter\n",
        "import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "qa2y2ai5cu8j"
      },
      "outputs": [],
      "source": [
        "#words = train['pro_claim_text'] + evidence_pd[\"pro_evidence\"]\n",
        "train_words = (train_data['pro_claim_text'].apply(lambda x : x.split())).tolist()\n",
        "evidence_words = (evidence_pd[\"pro_evidence\"].apply(lambda x : x.split())).tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jhZ4JlriAKX",
        "outputId": "04790efd-3a7f-4f63-a8e0-28a5efefe025"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['john',\n",
              " 'bennet',\n",
              " 'lawes',\n",
              " 'english',\n",
              " 'entrepreneur',\n",
              " 'agricultural',\n",
              " 'scientist']"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evidence_words[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdAgFz9biQ80",
        "outputId": "f38894d9-3ff1-4a8a-f71c-3e055288727c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "14558"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdS8CX60igFn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "oKEPO4dLfqO1"
      },
      "outputs": [],
      "source": [
        "train_words1 = [x for xs in train_words for x in xs ]\n",
        "evidence_words1 = [x for xs in evidence_words for x in xs ]\n",
        "vocab = list(set(train_words1 + evidence_words1))\n",
        "word_counts = Counter(train_words1 + evidence_words1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIW6Duagih2j",
        "outputId": "035e9270-51bf-4d5c-ce19-3f62b25f440b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "637640"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "len(word_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "7RkfoPIwnph4"
      },
      "outputs": [],
      "source": [
        "\n",
        "id2label = {0:\"SUPPORTS\", 1:\"NOT_ENOUGH_INFO\", 2:\"REFUTES\", 3:\"DISPUTED\"}\n",
        "label2id = {\"SUPPORTS\":0, \"NOT_ENOUGH_INFO\":1, \"REFUTES\":2, \"DISPUTED\":3}\n",
        "id2token = {0:\"<pad>\", 1:\"<cls>\", 2:\"<sep>\", 3:\"<unk>\"}\n",
        "token2id = {\"<pad>\": 0, \"<cls>\": 1, \"<sep>\": 2, \"<unk>\": 3}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQgMEg_th2s8",
        "outputId": "405a58b0-bb90-401a-8158-efe81d664b23"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "8298"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_counts[\"plant\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "QGWoMYcyiaHU"
      },
      "outputs": [],
      "source": [
        "idx = 4\n",
        "for i, j in word_counts.items():\n",
        "  if j > 5:\n",
        "    id2token[idx] = i\n",
        "    token2id[i] = idx\n",
        "    idx +=1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "ekfQ6OhtTx6S"
      },
      "outputs": [],
      "source": [
        "def wordtointvec(inputs, token2id):\n",
        "  lst = []\n",
        "  for i in inputs:\n",
        "\n",
        "    sent = []\n",
        "    for w in i.split(\" \"):\n",
        "      sent.append(token2id.get(w, token2id[\"<unk>\"]))\n",
        "    lst.append(sent)\n",
        "\n",
        "  return lst\n",
        "def wordtointvec4evi(inputs, token2id):\n",
        "  lst = []\n",
        "  for x in inputs:\n",
        "    sent = []\n",
        "    for i in x:\n",
        "      insent = []\n",
        "      for w in i.split(\" \"):\n",
        "        insent.append(token2id.get(w, token2id[\"<unk>\"]))\n",
        "      sent.append(insent)\n",
        "\n",
        "    lst.append(sent)\n",
        "\n",
        "  return lst\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACkbbtraw4ab",
        "outputId": "31cb731c-0a4e-49cb-aa27-52cbf66aaa5f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "claim-1937    scientific evidence pollutant higher concentra...\n",
              "claim-126     el niño drove record high global temperature s...\n",
              "claim-2510                              pdo switched cool phase\n",
              "claim-2021    weather channel cofounder john coleman provide...\n",
              "claim-2449    january capped month period global temperature...\n",
              "                                    ...                        \n",
              "claim-1504    climate scientist say aspect case hurricane ha...\n",
              "claim-243     assessment report ipcc estimated human emissio...\n",
              "claim-2302    since mid global temperature warming around de...\n",
              "claim-502     abnormal temperature spike february earlier mo...\n",
              "claim-3093    sending oscillating microwave antenna inside v...\n",
              "Name: pro_claim_text, Length: 1228, dtype: object"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data['pro_claim_text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "0mOIxXxtrBgi"
      },
      "outputs": [],
      "source": [
        "train_vec = wordtointvec(train_data['pro_claim_text'],token2id)\n",
        "train_evi_vec = wordtointvec4evi(train_data['pro_evidences'],token2id)\n",
        "#evi_vec = wordtointvec(evidence_pd[\"pro_evidence\"],token2id)\n",
        "dev_vec = wordtointvec(dev['pro_claim_text'],token2id)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFi3Zw2eRW-g",
        "outputId": "44cdd2ec-d95b-413a-a22e-5a6ccf0f3c26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[20, 8, 52, 110, 8, 851, 278, 279, 1843, 14, 15, 2421, 8, 765, 7, 1001, 3226, 2957, 19214, 3, 8271, 38097, 154], [13, 237, 95, 299, 1791, 8, 765, 4353, 103, 22355, 170, 784, 10925, 122, 121, 7018, 20717], [7, 278, 279, 8, 22501, 894, 13, 2116, 6789, 371]]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(train_evi_vec[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Qf7BxgiAwhje",
        "outputId": "0c2f0bd4-8e27-49f6-c3de-148edaee8b01"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Asura\\AppData\\Local\\Temp\\ipykernel_5928\\4177359292.py:1: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  train_data['pro_evidences'][0]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['high concentration time atmospheric concentration greater carbon dioxide toxic animal life raising concentration ppm higher several hour eliminate pest whitefly spider mite greenhouse',\n",
              " 'plant grow much percent faster concentration ppm co compared ambient condition though assumes change climate limitation nutrient',\n",
              " 'higher carbon dioxide concentration favourably affect plant growth demand water']"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data['pro_evidences'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mknTurTBupDv",
        "outputId": "60b90f2f-ad5b-4a0b-ff08-bcdcefaaf68c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "11.57328990228013"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sum([len(i) for i in train_vec])/ len(train_vec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VPJbCX5FpBf",
        "outputId": "4ef56a7f-3b29-4593-ee3f-79a31cfba20f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "11.57328990228013"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sum([len(i) for i in train_vec])/ len(train_vec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "jg9tHXyCrX3b"
      },
      "outputs": [],
      "source": [
        "def addpadding(l, inputs, token2id):\n",
        "  lst = []\n",
        "  for i in inputs:\n",
        "    if len(i) < l:\n",
        "      lst.append([token2id[\"<cls>\"]]+ i + [token2id[\"<sep>\"]] + (l-len(i)) * [token2id[\"<pad>\"]])\n",
        "    else:\n",
        "\n",
        "      lst.append([token2id[\"<cls>\"]]+ i[:l] + [token2id[\"<sep>\"]] )\n",
        "  return lst\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "M6KPwuQctRFW"
      },
      "outputs": [],
      "source": [
        "train_in = addpadding(34, train_vec, token2id)\n",
        "#train_evi_in = addpadding(100, train_evi_vec, token2id)\n",
        "#evi_in = addpadding(100, evi_vec, token2id)\n",
        "dev_in =addpadding(34, dev_vec, token2id)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "u9JByFEZrbjq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrCcjFWzQy19",
        "outputId": "db76fd21-c14b-4a28-c9d5-5d71ae85c04f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 20, 8, 52, 110, 8, 851, 278, 279, 1843, 14, 15, 2421, 8, 765, 7, 1001, 3226, 2957, 19214, 3, 8271, 38097, 154, 2, 13, 237, 95, 299, 1791, 8, 765, 4353, 103, 22355, 170, 784, 10925, 122, 121, 7018, 20717, 2, 7, 278, 279, 8, 22501, 894, 13, 2116, 6789, 371, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ],
      "source": [
        "max_len = 280\n",
        "for i in range(len(train_in)):\n",
        "  for j in train_evi_vec[i]:\n",
        "    train_in[i].append(token2id[\"<sep>\"])\n",
        "    train_in[i].extend(j)\n",
        "  train_in[i].append(token2id[\"<sep>\"])\n",
        "  if len(train_in[i]) < max_len:\n",
        "\n",
        "    train_in[i].extend([token2id[\"<pad>\"]] * (max_len - len(train_in[i])))\n",
        "\n",
        "print(train_in[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "AvFzPJYW5WV8"
      },
      "outputs": [],
      "source": [
        "dev_label = [label2id[i] for i in dev[\"claim_label\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYtkJV1o5lcs",
        "outputId": "e8c6ac20-67e6-4576-9e3c-3d970d897873"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(dev_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "lHCva-xnse6I"
      },
      "outputs": [],
      "source": [
        "for i in train_in:\n",
        "  if len(i)>280:\n",
        "    print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "yYdDLskVQe2q"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "class TrainDataset(Dataset):\n",
        "  def __init__(self, text_data, label_data):\n",
        "    self.text_data = text_data\n",
        "    self.label_data = [label2id[i] for i in label_data]\n",
        "    \n",
        "\n",
        "  def __getitem__ (self, index):\n",
        "    return [self.text_data[index], self.label_data[index]]\n",
        "  def __len__(self):\n",
        "    return len(self.text_data)\n",
        "\n",
        "  def collate_fn(self, data):\n",
        "    q = []\n",
        "    evi = []\n",
        "    labels = []\n",
        "    for sen, lab in data:\n",
        "      q.append(sen)\n",
        "      labels.append(lab)\n",
        "    batch_encoding = {}\n",
        "    batch_encoding[\"queries\"] =  torch.LongTensor(q)\n",
        "\n",
        "    batch_encoding[\"labels\"] = torch.LongTensor(labels)\n",
        "    return batch_encoding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "XSkwW5S7QnOb"
      },
      "outputs": [],
      "source": [
        "train_dataset = TrainDataset(train_in, train_data[\"claim_label\"])\n",
        "dataloader = DataLoader(train_dataset, batch_size = 20, shuffle = True, num_workers = 0,collate_fn=train_dataset.collate_fn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBpCCQm3QqId",
        "outputId": "dd909c8e-a34c-4026-b4af-1b19d2d98c95"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "GTaI4Yhk1neU",
        "outputId": "efedf25b-c574-4817-dff1-3be32a5e94d7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf2Ycr8tT2p2",
        "outputId": "4856b037-5328-4cd4-815a-4801df80dbe0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "62"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "a8u2febpZoY6",
        "outputId": "d06be6a5-12d2-4003-8c5a-6a76bbfb921b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[   1, 1802,  882,  ...,    0,    0,    0],\n",
            "        [   1,  371,  111,  ...,    0,    0,    0],\n",
            "        [   1,  121, 1452,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [   1,  840, 1318,  ...,    0,    0,    0],\n",
            "        [   1,  394,  789,  ...,    0,    0,    0],\n",
            "        [   1,  469, 1709,  ...,    0,    0,    0]])\n",
            "tensor([[  1, 160, 402,  ...,   0,   0,   0],\n",
            "        [  1, 318, 241,  ...,   0,   0,   0],\n",
            "        [  1, 406, 774,  ...,   0,   0,   0],\n",
            "        ...,\n",
            "        [  1, 954, 209,  ...,   0,   0,   0],\n",
            "        [  1, 209, 470,  ...,   0,   0,   0],\n",
            "        [  1, 192, 923,  ...,   0,   0,   0]])\n",
            "tensor([[   1, 1075, 1522,  ...,    0,    0,    0],\n",
            "        [   1,  369, 2367,  ...,    0,    0,    0],\n",
            "        [   1,  502,  503,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [   1, 2310,  543,  ...,    0,    0,    0],\n",
            "        [   1,  275,  312,  ...,    0,    0,    0],\n",
            "        [   1, 1018,  977,  ...,    0,    0,    0]])\n",
            "tensor([[   1,  371,  372,  ...,    0,    0,    0],\n",
            "        [   1, 1388,  721,  ...,    0,    0,    0],\n",
            "        [   1, 1018,  977,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [   1, 1018,  514,  ...,    0,    0,    0],\n",
            "        [   1, 2219,  366,  ...,    0,    0,    0],\n",
            "        [   1, 1540, 1541,  ...,    0,    0,    0]])\n",
            "tensor([[   1, 1219, 1054,  ...,    0,    0,    0],\n",
            "        [   1, 1912,  111,  ...,    0,    0,    0],\n",
            "        [   1,  228, 1064,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [   1,  318,  241,  ...,    0,    0,    0],\n",
            "        [   1,  509,  510,  ...,    0,    0,    0],\n",
            "        [   1,   41,   22,  ...,    0,    0,    0]])\n",
            "tensor([[   1,  628,  444,  ...,    0,    0,    0],\n",
            "        [   1,  396, 2945,  ...,    0,    0,    0],\n",
            "        [   1,  514,  753,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [   1,  297,   52,  ...,    0,    0,    0],\n",
            "        [   1,   63,  330,  ...,    0,    0,    0],\n",
            "        [   1,  168, 1015,  ...,    0,    0,    0]])\n",
            "tensor([[   1,  601,  659,  ...,    0,    0,    0],\n",
            "        [   1, 1381, 2060,  ...,    0,    0,    0],\n",
            "        [   1, 2535,    3,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [   1,    3, 1361,  ...,    0,    0,    0],\n",
            "        [   1,  606, 2625,  ...,    0,    0,    0],\n",
            "        [   1,  335,  691,  ...,    0,    0,    0]])\n",
            "tensor([[   1,  144,   41,  ...,    0,    0,    0],\n",
            "        [   1,  324,  626,  ...,    0,    0,    0],\n",
            "        [   1,  990,  991,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [   1, 1622, 1623,  ...,    0,    0,    0],\n",
            "        [   1,  325,  326,  ...,    0,    0,    0],\n",
            "        [   1,   21,   41,  ...,    0,    0,    0]])\n",
            "tensor([[   1, 1837, 1698,  ...,    0,    0,    0],\n",
            "        [   1,   22,  318,  ...,    0,    0,    0],\n",
            "        [   1,  840,  375,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [   1,  790,  204,  ...,    0,    0,    0],\n",
            "        [   1,  217,  218,  ...,    0,    0,    0],\n",
            "        [   1,  795,    3,  ...,    0,    0,    0]])\n",
            "tensor([[   1,   53,  696,  ...,    0,    0,    0],\n",
            "        [   1, 2678,  227,  ...,    0,    0,    0],\n",
            "        [   1, 1265,  467,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [   1,  121,  275,  ...,    0,    0,    0],\n",
            "        [   1,  968,  362,  ...,    0,    0,    0],\n",
            "        [   1,    3,  179,  ...,    0,    0,    0]])\n",
            "tensor([[   1,  303,    8,  ...,    0,    0,    0],\n",
            "        [   1,  270,  428,  ...,    0,    0,    0],\n",
            "        [   1,  954,  574,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [   1,  241,  111,  ...,    0,    0,    0],\n",
            "        [   1,  275,  623,  ...,    0,    0,    0],\n",
            "        [   1,  324, 2386,  ...,    0,    0,    0]])\n",
            "tensor([[   1,  516, 2949,  ...,    0,    0,    0],\n",
            "        [   1,  371, 1467,  ...,    0,    0,    0],\n",
            "        [   1,  493,  470,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [   1,  122,   75,  ...,    0,    0,    0],\n",
            "        [   1,  275,  115,  ...,    0,    0,    0],\n",
            "        [   1,  179, 1902,  ...,    0,    0,    0]])\n",
            "tensor([[   1, 2543,  299,  ...,    0,    0,    0],\n",
            "        [   1,  110, 1412,  ...,    0,    0,    0],\n",
            "        [   1,  149,  219,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [   1,  431,   99,  ...,    0,    0,    0],\n",
            "        [   1,  121,  122,  ...,    0,    0,    0],\n",
            "        [   1,  299,  572,  ...,    0,    0,    0]])\n",
            "tensor([[   1,  954,  209,  ...,    0,    0,    0],\n",
            "        [   1,  121, 2351,  ...,    0,    0,    0],\n",
            "        [   1, 1659, 2799,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [   1,  371,  111,  ...,    0,    0,    0],\n",
            "        [   1, 2448,  537,  ...,    0,    0,    0],\n",
            "        [   1, 2058, 1743,  ...,    0,    0,    0]])\n",
            "tensor([[   1,  188,  493,  ...,    0,    0,    0],\n",
            "        [   1, 1420, 1421,  ...,    0,    0,    0],\n",
            "        [   1, 1018,  275,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [   1,    3, 1108,  ...,    0,    0,    0],\n",
            "        [   1,  663,  227,  ...,    0,    0,    0],\n",
            "        [   1,  401,  664,  ...,    0,    0,    0]])\n",
            "tensor([[   1,  217, 1979,  ...,    0,    0,    0],\n",
            "        [   1,  275, 1636,  ...,    0,    0,    0],\n",
            "        [   1,  911,  912,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [   1, 2332,  474,  ...,    0,    0,    0],\n",
            "        [   1,  884,  983,  ...,    0,    0,    0],\n",
            "        [   1, 2677, 1696,  ...,    0,    0,    0]])\n",
            "tensor([[   1, 2289,  121,  ...,    0,    0,    0],\n",
            "        [   1,  646,   21,  ...,    0,    0,    0],\n",
            "        [   1,  245,   32,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [   1,  104, 1104,  ...,    0,    0,    0],\n",
            "        [   1, 2317, 2318,  ...,    0,    0,    0],\n",
            "        [   1,   13,  106,  ...,    0,    0,    0]])\n",
            "tensor([[   1, 3056,  322,  ...,    0,    0,    0],\n",
            "        [   1,  256,  875,  ...,    0,    0,    0],\n",
            "        [   1, 1130,  882,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [   1,  564, 1787,  ...,    0,    0,    0],\n",
            "        [   1,  722, 1898,  ...,    0,    0,    0],\n",
            "        [   1,  827,  241,  ...,    0,    0,    0]])\n",
            "tensor([[   1,   62,  204,  ...,    0,    0,    0],\n",
            "        [   1,  182,  241,  ...,    0,    0,    0],\n",
            "        [   1,  970,  109,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [   1,  167,  402,  ...,    0,    0,    0],\n",
            "        [   1, 1597,  338,  ...,    0,    0,    0],\n",
            "        [   1,  349,  592,  ...,    0,    0,    0]])\n",
            "tensor([[   1,  156,  157,  ...,    0,    0,    0],\n",
            "        [   1,  751,   91,  ...,    0,    0,    0],\n",
            "        [   1,  255, 1596,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [   1,   78,   79,  ...,    0,    0,    0],\n",
            "        [   1,  565, 1532,  ...,    0,    0,    0],\n",
            "        [   1,  173,   30,  ...,    0,    0,    0]])\n",
            "tensor([[   1,  121,  818,  ...,    0,    0,    0],\n",
            "        [   1, 2717, 1373,  ...,    0,    0,    0],\n",
            "        [   1,  554,  261,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [   1, 1645, 1216,  ...,    0,    0,    0],\n",
            "        [   1,  122,  406,  ...,    0,    0,    0],\n",
            "        [   1,   22,  375,  ...,    0,    0,    0]])\n",
            "tensor([[   1,  401,  110,  ...,    0,    0,    0],\n",
            "        [   1, 1429, 1291,  ...,    0,    0,    0],\n",
            "        [   1,  637, 1596,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [   1,  714,  345,  ...,    0,    0,    0],\n",
            "        [   1, 2319,    5,  ...,    0,    0,    0],\n",
            "        [   1,  141, 1297,  ...,    0,    0,    0]])\n",
            "tensor([[   1,   63, 1143,  ...,    0,    0,    0],\n",
            "        [   1,  121,  275,  ...,    0,    0,    0],\n",
            "        [   1,  179,  542,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [   1,  121,  179,  ...,    0,    0,    0],\n",
            "        [   1, 1481,   51,  ...,    0,    0,    0],\n",
            "        [   1, 1189,  312,  ...,    0,    0,    0]])\n",
            "tensor([[   1, 1332, 1333,  ...,    0,    0,    0],\n",
            "        [   1,   52,  121,  ...,    0,    0,    0],\n",
            "        [   1,  371,  372,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [   1, 3052,  470,  ...,    0,    0,    0],\n",
            "        [   1,  324, 2386,  ...,    0,    0,    0],\n",
            "        [   1,  114,  115,  ...,    0,    0,    0]])\n",
            "tensor([[   1,  778,  155,  ...,    0,    0,    0],\n",
            "        [   1,  318,    3,  ...,    0,    0,    0],\n",
            "        [   1,  781,  473,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [   1,  344,  192,  ...,    0,    0,    0],\n",
            "        [   1,    3, 1275,  ...,    0,    0,    0],\n",
            "        [   1,   52,  258,  ...,    0,    0,    0]])\n",
            "tensor([[  1,   5, 413,  ...,   0,   0,   0],\n",
            "        [  1,  77, 121,  ...,   0,   0,   0],\n",
            "        [  1,  21,  41,  ...,   0,   0,   0],\n",
            "        ...,\n",
            "        [  1, 121, 122,  ...,   0,   0,   0],\n",
            "        [  1, 795,   3,  ...,   0,   0,   0],\n",
            "        [  1, 226, 227,  ...,   0,   0,   0]])\n",
            "tensor([[   1,   54,  194,  ...,    0,    0,    0],\n",
            "        [   1,  470,  182,  ...,    0,    0,    0],\n",
            "        [   1,  329,  330,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [   1,   57,  549,  ...,    0,    0,    0],\n",
            "        [   1,  167,   91,  ...,    0,    0,    0],\n",
            "        [   1,  487, 1494,  ...,    0,    0,    0]])\n",
            "tensor([[   1,  661, 1259,  ...,    0,    0,    0],\n",
            "        [   1,  179,  276,  ...,    0,    0,    0],\n",
            "        [   1,   21,   41,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [   1, 2581, 2582,  ...,    0,    0,    0],\n",
            "        [   1, 1250, 1839,  ...,    0,    0,    0],\n",
            "        [   1,  278, 2218,  ...,    0,    0,    0]])\n",
            "tensor([[   1, 1468,  790,  ...,    0,    0,    0],\n",
            "        [   1,  415,  826,  ...,    0,    0,    0],\n",
            "        [   1,  371,  372,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [   1,   67,   41,  ...,    0,    0,    0],\n",
            "        [   1,  699,  700,  ...,    0,    0,    0],\n",
            "        [   1,  407,  414,  ...,    0,    0,    0]])\n",
            "tensor([[   1,   41,  121,  ...,    0,    0,    0],\n",
            "        [   1,  406,  574,  ...,    0,    0,    0],\n",
            "        [   1, 2794, 2795,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [   1, 2344, 2345,  ...,    0,    0,    0],\n",
            "        [   1,  334, 1690,  ...,    0,    0,    0],\n",
            "        [   1,  179,  349,  ...,    0,    0,    0]])\n",
            "tensor([[   1,  761,  762,  ...,    0,    0,    0],\n",
            "        [   1, 1217,  543,  ...,    0,    0,    0],\n",
            "        [   1,  543,  157,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [   1,    3, 1336,  ...,    0,    0,    0],\n",
            "        [   1, 2584, 1788,  ...,    0,    0,    0],\n",
            "        [   1, 1329,  217,  ...,    0,    0,    0]])\n",
            "tensor([[   1, 1311,  753,  ...,    0,    0,    0],\n",
            "        [   1,  579,   19,  ...,    0,    0,    0],\n",
            "        [   1,  664, 1865,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [   1,   71,  738,  ...,    0,    0,    0],\n",
            "        [   1,  188,  749,  ...,    0,    0,    0],\n",
            "        [   1,  324,   97,  ...,    0,    0,    0]])\n",
            "tensor([[   1,  565, 1114,  ...,    0,    0,    0],\n",
            "        [   1,   21,   41,  ...,    0,    0,    0],\n",
            "        [   1, 1444, 1564,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [   1,  706,  565,  ...,    0,    0,    0],\n",
            "        [   1,  369,   22,  ...,    0,    0,    0],\n",
            "        [   1,  335,  691,  ...,    0,    0,    0]])\n",
            "tensor([[   1, 1466,  406,  ...,    0,    0,    0],\n",
            "        [   1,   13,  106,  ...,    0,    0,    0],\n",
            "        [   1, 1170, 2021,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [   1,  882,  883,  ...,    0,    0,    0],\n",
            "        [   1, 1364,  121,  ...,    0,    0,    0],\n",
            "        [   1,  317,  487,  ...,    0,    0,    0]])\n",
            "tensor([[   1,  245,  572,  ...,    0,    0,    0],\n",
            "        [   1, 1547, 1548,  ...,    0,    0,    0],\n",
            "        [   1, 1066,  110,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [   1,   63,  187,  ...,    0,    0,    0],\n",
            "        [   1,  686,  455,  ...,    0,    0,    0],\n",
            "        [   1,   61,  402,  ...,    0,    0,    0]])\n",
            "tensor([[   1, 1342, 1343,  ...,    0,    0,    0],\n",
            "        [   1,  324, 1597,  ...,    0,    0,    0],\n",
            "        [   1,   21,   41,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [   1, 1512, 1513,  ...,    0,    0,    0],\n",
            "        [   1,    3, 1623,  ...,    0,    0,    0],\n",
            "        [   1,  762,  114,  ...,    0,    0,    0]])\n",
            "tensor([[   1,  270,  462,  ...,    0,    0,    0],\n",
            "        [   1, 1361, 1362,  ...,    0,    0,    0],\n",
            "        [   1,  474,  227,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [   1, 1450,   21,  ...,    0,    0,    0],\n",
            "        [   1,    3,  217,  ...,    0,    0,    0],\n",
            "        [   1,  487,  104,  ...,    0,    0,    0]])\n",
            "tensor([[   1,  275,  771,  ...,    0,    0,    0],\n",
            "        [   1, 3004,  227,  ...,    0,    0,    0],\n",
            "        [   1,  121,  122,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [   1,   62, 2529,  ...,    0,    0,    0],\n",
            "        [   1, 1366,  470,  ...,    0,    0,    0],\n",
            "        [   1,  584,  964,  ...,    0,    0,    0]])\n",
            "tensor([[   1,   15, 2375,  ...,    0,    0,    0],\n",
            "        [   1,   57,   67,  ...,    0,    0,    0],\n",
            "        [   1,  693,  694,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [   1,    7,   22,  ...,    0,    0,    0],\n",
            "        [   1,  179,  180,  ...,    0,    0,    0],\n",
            "        [   1,  121, 1189,  ...,    0,    0,    0]])\n",
            "tensor([[   1,  774,  476,  ...,    0,    0,    0],\n",
            "        [   1, 2513, 2514,  ...,    0,    0,    0],\n",
            "        [   1,  231,  219,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [   1,   22,  227,  ...,    0,    0,    0],\n",
            "        [   1,  307, 1835,  ...,    0,    0,    0],\n",
            "        [   1,  317,  318,  ...,    0,    0,    0]])\n",
            "tensor([[   1,  884,  885,  ...,    0,    0,    0],\n",
            "        [   1, 1322, 1323,  ...,    0,    0,    0],\n",
            "        [   1,  396,  188,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [   1,  121,  122,  ...,    0,    0,    0],\n",
            "        [   1, 1260, 1261,  ...,    0,    0,    0],\n",
            "        [   1, 2460, 1639,  ...,    0,    0,    0]])\n",
            "tensor([[   1,   77,  121,  ...,    0,    0,    0],\n",
            "        [   1, 2640,   63,  ...,    0,    0,    0],\n",
            "        [   1,   41,  786,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [   1,   83,  859,  ...,    0,    0,    0],\n",
            "        [   1,  726,    5,  ...,    0,    0,    0],\n",
            "        [   1,  121,  275,  ...,    0,    0,    0]])\n",
            "tensor([[   1,  241,  111,  ...,    0,    0,    0],\n",
            "        [   1,  617, 3133,  ...,    0,    0,    0],\n",
            "        [   1,  585,  121,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [   1,  840, 2574,  ...,    0,    0,    0],\n",
            "        [   1, 2310,  543,  ...,    0,    0,    0],\n",
            "        [   1,  949, 2305,  ...,    0,    0,    0]])\n",
            "tensor([[   1,   83,  270,  ...,    0,    0,    0],\n",
            "        [   1, 1362,  650,  ...,    0,    0,    0],\n",
            "        [   1,   62,   22,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [   1, 1112,  516,  ...,    0,    0,    0],\n",
            "        [   1, 1434, 1435,  ...,    0,    0,    0],\n",
            "        [   1,  789,  369,  ...,    0,    0,    0]])\n",
            "tensor([[  1, 371,  22,  ...,   0,   0,   0],\n",
            "        [  1, 712, 334,  ...,   0,   0,   0],\n",
            "        [  1, 190, 406,  ...,   0,   0,   0],\n",
            "        ...,\n",
            "        [  1, 317, 192,  ...,   0,   0,   0],\n",
            "        [  1, 255, 256,  ...,   0,   0,   0],\n",
            "        [  1, 329,   3,  ...,   0,   0,   0]])\n",
            "tensor([[   1, 1103, 1466,  ...,    0,    0,    0],\n",
            "        [   1,  610, 2704,  ...,    0,    0,    0],\n",
            "        [   1, 2370, 2340,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [   1,  516,  574,  ...,    0,    0,    0],\n",
            "        [   1,  577,  334,  ...,    0,    0,    0],\n",
            "        [   1,  299,    4,  ...,    0,    0,    0]])\n",
            "tensor([[   1, 1560,  977,  ...,    0,    0,    0],\n",
            "        [   1,  179,  276,  ...,    0,    0,    0],\n",
            "        [   1,  827,  728,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [   1, 1216,  149,  ...,    0,    0,    0],\n",
            "        [   1,   52,  439,  ...,    0,    0,    0],\n",
            "        [   1,  945, 3045,  ...,    0,    0,    0]])\n",
            "tensor([[   1,  584,   44,  ...,    0,    0,    0],\n",
            "        [   1,  318,  343,  ...,    0,    0,    0],\n",
            "        [   1,  548,   21,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [   1, 1408, 1409,  ...,    0,    0,    0],\n",
            "        [   1, 2842, 2843,  ...,    0,    0,    0],\n",
            "        [   1,  300,   53,  ...,    0,    0,    0]])\n",
            "tensor([[   1, 2458, 1001,  ...,    0,    0,    0],\n",
            "        [   1,  793,  441,  ...,    0,    0,    0],\n",
            "        [   1,  149,  699,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [   1,   21,   41,  ...,    0,    0,    0],\n",
            "        [   1, 1855,  182,  ...,    0,    0,    0],\n",
            "        [   1,  402,    3,  ...,    0,    0,    0]])\n",
            "tensor([[   1,   41,  172,  ...,    0,    0,    0],\n",
            "        [   1,  121,  275,  ...,    0,    0,    0],\n",
            "        [   1,  454,  331,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [   1,  790, 1918,  ...,    0,    0,    0],\n",
            "        [   1,  402, 1278,  ...,    0,    0,    0],\n",
            "        [   1,  174,   45,  ...,    0,    0,    0]])\n",
            "tensor([[   1, 1159,  402,  ...,    0,    0,    0],\n",
            "        [   1,    3,  356,  ...,    0,    0,    0],\n",
            "        [   1,  231,  219,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [   1,  396, 1267,  ...,    0,    0,    0],\n",
            "        [   1, 1946, 1170,  ...,    0,    0,    0],\n",
            "        [   1,  805,  806,  ...,    0,    0,    0]])\n",
            "tensor([[   1,  111,    7,  ...,    0,    0,    0],\n",
            "        [   1,    5,  111,  ...,    0,    0,    0],\n",
            "        [   1, 1781,  473,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [   1,   41,  314,  ...,    0,    0,    0],\n",
            "        [   1,  275, 1413,  ...,    0,    0,    0],\n",
            "        [   1,   62, 3117,  ...,    0,    0,    0]])\n",
            "tensor([[   1, 3037,    3,  ...,    0,    0,    0],\n",
            "        [   1, 2250,  454,  ...,    0,    0,    0],\n",
            "        [   1,  714,   99,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [   1,  275,  384,  ...,    0,    0,    0],\n",
            "        [   1,  215,  129,  ...,    0,    0,    0],\n",
            "        [   1,  776,  777,  ...,    0,    0,    0]])\n",
            "tensor([[   1,  167,   21,  ...,    0,    0,    0],\n",
            "        [   1, 2347, 2520,  ...,    0,    0,    0],\n",
            "        [   1,   16,   17,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [   1, 2687,  121,  ...,    0,    0,    0],\n",
            "        [   1,  318,  412,  ...,    0,    0,    0],\n",
            "        [   1,  555,  827,  ...,    0,    0,    0]])\n",
            "tensor([[   1,  275, 1592,  ...,    0,    0,    0],\n",
            "        [   1,    7,  233,  ...,    0,    0,    0],\n",
            "        [   1,  678,  231,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [   1,  455, 1015,  ...,    0,    0,    0],\n",
            "        [   1, 3097,  546,  ...,    0,    0,    0],\n",
            "        [   1,   54, 1836,  ...,    0,    0,    0]])\n",
            "tensor([[   1,  160, 2995,  ...,    0,    0,    0],\n",
            "        [   1, 1018,  643,  ...,    0,    0,    0],\n",
            "        [   1,   25, 2506,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [   1, 2503, 2504,  ...,    0,    0,    0],\n",
            "        [   1,  402,  365,  ...,    0,    0,    0],\n",
            "        [   1,  121,  122,  ...,    0,    0,    0]])\n",
            "tensor([[   1,  318, 2246,  ...,    0,    0,    0],\n",
            "        [   1, 1355,  121,  ...,    0,    0,    0],\n",
            "        [   1,  967, 2083,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [   1,  326, 1842,  ...,    0,    0,    0],\n",
            "        [   1,  844, 1538,  ...,    0,    0,    0],\n",
            "        [   1, 1466,  278,  ...,    0,    0,    0]])\n",
            "tensor([[   1,   27,   95,  ...,    0,    0,    0],\n",
            "        [   1,  396, 1822,  ...,    0,    0,    0],\n",
            "        [   1,  111,  278,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [   1,  389, 1216,  ...,    0,    0,    0],\n",
            "        [   1,  663,  110,  ...,    0,    0,    0],\n",
            "        [   1,    6, 2439,  ...,    0,    0,    0]])\n",
            "tensor([[   1,   21,   41,  ...,    0,    0,    0],\n",
            "        [   1,  910,  574,  ...,    0,    0,    0],\n",
            "        [   1,  552,  104,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [   1, 1311,  466,  ...,    0,    0,    0],\n",
            "        [   1,  402,  125,  ...,    0,    0,    0],\n",
            "        [   1,  643, 1054,  ...,    0,    0,    0]])\n",
            "tensor([[   1, 1346, 1637,  ...,    0,    0,    0],\n",
            "        [   1,   81,   16,  ...,    0,    0,    0],\n",
            "        [   1,   96, 1466,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [   1,  878,  126,  ...,    0,    0,    0],\n",
            "        [   1, 3124,  168,  ...,    0,    0,    0],\n",
            "        [   1,  766,    5,  ...,    0,    0,    0]])\n",
            "tensor([[   1, 1506, 1507,  ...,    0,    0,    0],\n",
            "        [   1,  121,  179,  ...,    0,    0,    0],\n",
            "        [   1,  406,  713,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [   1,  888, 1696,  ...,    0,    0,    0],\n",
            "        [   1, 1332, 1333,  ...,    0,    0,    0],\n",
            "        [   1,   67, 1545,  ...,    0,    0,    0]])\n",
            "tensor([[  1, 144, 520,  ...,   0,   0,   0],\n",
            "        [  1, 144, 444,  ...,   0,   0,   0],\n",
            "        [  1, 403, 187,  ...,   0,   0,   0],\n",
            "        ...,\n",
            "        [  1, 363, 470,  ...,   0,   0,   0],\n",
            "        [  1, 664, 949,  ...,   0,   0,   0],\n",
            "        [  1, 349, 592,  ...,   0,   0,   0]])\n"
          ]
        }
      ],
      "source": [
        "for (i, targets) in enumerate(dataloader):\n",
        "  print(targets[\"queries\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UbR83Tqevtn"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "QM9_9ul8VlW3"
      },
      "outputs": [],
      "source": [
        "class AttentionBlock(nn.Module):\n",
        "  def __init__(self, embed_dim, n_heads, drop_prob):\n",
        "    super(AttentionBlock , self).__init__()\n",
        "    self.att = nn.MultiheadAttention(embed_dim, n_heads)\n",
        "    self.ffn = nn.Sequential(\n",
        "        nn.Linear(embed_dim, 4*embed_dim),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4*embed_dim, embed_dim)\n",
        "    )\n",
        "    self.ln1 = nn.LayerNorm(embed_dim, eps = 1e-6)\n",
        "    self.ln2 = nn.LayerNorm(embed_dim, eps = 1e-6)\n",
        "    self.dropout1 = nn.Dropout(drop_prob)\n",
        "    self.dropout2 = nn.Dropout(drop_prob)\n",
        "  def forward(self, inputs):\n",
        "    attn_out, _ = self.att(inputs, inputs,inputs)\n",
        "    attn_out = self.dropout1(attn_out)\n",
        "    out1 = self.ln1(inputs + attn_out)\n",
        "    ffn_output = self.ffn(out1)\n",
        "    ffn_output = self.dropout2(ffn_output)\n",
        "    return self.ln2(out1 + ffn_output)\n",
        "class ClassificationModel(nn.Module):\n",
        "  def __init__(self, embed_dim, wordlist_size, block_size, drop_prob, n_classes, n_heads):\n",
        "    super(ClassificationModel , self).__init__()\n",
        "    self.token_embed = nn.Embedding(wordlist_size, embed_dim)\n",
        "    self.pos_embed = nn.Embedding(block_size, embed_dim)\n",
        "    self.AttentionBlock = AttentionBlock(embed_dim, n_heads, drop_prob)\n",
        "    self.ln1 = nn.LayerNorm(embed_dim)\n",
        "    self.ln2 = nn.LayerNorm(embed_dim)\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Linear(embed_dim, embed_dim),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(drop_prob),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(embed_dim, n_classes),\n",
        "        nn.Softmax(dim = 1)\n",
        "    )\n",
        "  def forward(self, token):\n",
        "    r,c = token.shape\n",
        "    emb = self.token_embed(token)\n",
        "    emb = emb+ self.pos_embed(torch.arange(c, device = device))\n",
        "    out = self.AttentionBlock(emb)\n",
        "    out = out.mean(dim = 1)\n",
        "    out = self.classifier(out)\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMf9NRPXyNXR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "1xdV1Q0r7y46"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "classification_model = ClassificationModel(embed_dim = 512, wordlist_size = len(id2token), block_size = 280, drop_prob = 0.1, n_classes = 4, n_heads = 4)\n",
        "classification_model.to(device)\n",
        "lr = 0.0001\n",
        "optimizer = torch.optim.Adam(classification_model.parameters(), lr=lr)\n",
        "loss_t = nn.CrossEntropyLoss()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqhdO0ltTZYG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8u05f613YY4q"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "W1syr50WNgCl"
      },
      "outputs": [],
      "source": [
        "#import time\n",
        "def train():\n",
        "  classification_model.train()\n",
        "  total_loss = 0.\n",
        "  #start_time = time.time()\n",
        "  for (i, targets) in enumerate(dataloader):\n",
        "    optimizer.zero_grad()\n",
        "    out = classification_model(targets[\"queries\"].cuda())\n",
        "    loss = loss_t(out, targets[\"labels\"].cuda())\n",
        "    #loss = loss\n",
        "    #torch.nn.utils.clip_grad_norm_()\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    total_loss += loss.item()\n",
        "\n",
        "\n",
        "def evaluate(model, input, out):\n",
        "    lst = []\n",
        "    start = 0\n",
        "    batch_size = 50\n",
        "    in_len = len(input[0])\n",
        "    model.eval()\n",
        "    correct_count = 0\n",
        "    while start < len(out):\n",
        "        end = min(start+ batch_size, len(out))\n",
        "\n",
        "        model_input = torch.LongTensor(input[start:end]).view(-1, in_len).cuda()\n",
        "\n",
        "        model_output = model(model_input)\n",
        "        model_output = torch.argmax(model_output, 1).tolist()\n",
        "\n",
        "        for i, j in zip(model_output, out[start: end]):\n",
        "            if i == j:\n",
        "                correct_count += 1\n",
        "\n",
        "        start = end\n",
        "    lst = correct_count / len(out)\n",
        "    print(\"\\n\")\n",
        "    print(\"Classification Accuracy: %.3f\" % lst)\n",
        "    print(\"\\n\")\n",
        "\n",
        "    model.train()\n",
        "    return lst\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "6huyeMhiL1Cz",
        "outputId": "9a47dbe1-6e8b-45ec-db98-32a4ed551f3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Classification Accuracy: 0.357\n",
            "\n",
            "\n",
            "fscore 0.35714285714285715\n",
            "\n",
            "\n",
            "Classification Accuracy: 0.370\n",
            "\n",
            "\n",
            "fscore 0.37012987012987014\n",
            "\n",
            "\n",
            "Classification Accuracy: 0.344\n",
            "\n",
            "\n",
            "fscore 0.34415584415584416\n",
            "\n",
            "\n",
            "Classification Accuracy: 0.409\n",
            "\n",
            "\n",
            "fscore 0.4090909090909091\n",
            "\n",
            "\n",
            "Classification Accuracy: 0.377\n",
            "\n",
            "\n",
            "fscore 0.37662337662337664\n",
            "\n",
            "\n",
            "Classification Accuracy: 0.422\n",
            "\n",
            "\n",
            "fscore 0.42207792207792205\n",
            "\n",
            "\n",
            "Classification Accuracy: 0.409\n",
            "\n",
            "\n",
            "fscore 0.4090909090909091\n",
            "\n",
            "\n",
            "Classification Accuracy: 0.416\n",
            "\n",
            "\n",
            "fscore 0.4155844155844156\n",
            "\n",
            "\n",
            "Classification Accuracy: 0.409\n",
            "\n",
            "\n",
            "fscore 0.4090909090909091\n",
            "\n",
            "\n",
            "Classification Accuracy: 0.422\n",
            "\n",
            "\n",
            "fscore 0.42207792207792205\n"
          ]
        }
      ],
      "source": [
        "\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "  loss = 0\n",
        "  train()\n",
        "  f_score = evaluate(classification_model, dev_in, dev_label)\n",
        "  print(\"fscore\", f_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict( dev_input, model):\n",
        "    lst = []\n",
        "    start = 0\n",
        "    batch_size = 50\n",
        "    in_len = len(dev_input[0])\n",
        "    model.eval()\n",
        "\n",
        "    \n",
        "    \n",
        "    while start < len(dev_input):\n",
        "        end = min(start+ batch_size, len(dev_in))\n",
        "        \n",
        "        model_input= torch.LongTensor(dev_input[start:end]).view(-1, in_len).cuda()\n",
        "        \n",
        "        model_output = model(model_input)\n",
        "        model_output = torch.argmax(model_output, 1).tolist()\n",
        "        lst.extend(model_output)\n",
        "        \n",
        "        start = end\n",
        "\n",
        "    return lst\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [],
      "source": [
        "dev_classes = predict(dev_in, classification_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0]"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dev_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vxW29ievJ05G"
      },
      "outputs": [],
      "source": [
        "# class MultiHeadAttention(nn.Module):\n",
        "#   def __init__(self , d_model, nhead):\n",
        "#     #d_model / nhead should equal to 0\n",
        "#     super(MultiHeadAttention, self).__init__()\n",
        "#     self.d_model = d_model\n",
        "#     self.d_model_type = 'Transformer'\n",
        "#     self.nhead = nhead\n",
        "#     self.d_k = d_model//nhead\n",
        "#     self.Q = nn.Linear(d_model, d_model)\n",
        "#     self.K = nn.Linear(d_model, d_model)\n",
        "#     self.V = nn.Linear(d_model, d_model)\n",
        "#     self.O = nn.Linear(d_model, d_model)\n",
        "#   def scaled_dot_prod_attention(self, Q,K,V,mask = None):\n",
        "#     attention_score = torch.matmul(Q,K.transpose(-2,-1))/ math.sqrt(self.d_k)\n",
        "#     if mask:\n",
        "#       attention_score = attention_score.masked_fill(mask == 0, -1e9)\n",
        "#       attention_probs = torch.softmax(attention_score, dim = -1)\n",
        "#       out = torch.matmul(attention_probs, V)\n",
        "#       return out\n",
        "#   def split_heads(self, n):\n",
        "#     bat_size, seq_len, d_model = n.size()\n",
        "#     return n.view(bat_size, seq_len, self.nhead, self.K).transpose(1,2)\n",
        "\n",
        "#   def combine_heads(self, n):\n",
        "#     bat_size, _, seq_len, d_k = n.size()\n",
        "#     return n.transpose(1,2).contiguous().view(bat_size, seq_len, self.d_model)\n",
        "\n",
        "\n",
        "#   def forward(self, Q,K,V,mask = None):\n",
        "#     Q = self.split_heads(self.Q(Q))\n",
        "#     K = self.split_heads(self.K(K))\n",
        "#     V = self.split_heads(self.V(V))\n",
        "#     attention_out = self.scaled_dot_prod_attention(Q,K,V,mask)\n",
        "#     output = self.O(self.combine_heads(attention_out))\n",
        "#     return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3o4qft8J177"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, d_model, dropout = 0.1, max_seq_len = 5000):\n",
        "    super(PositionalEncoding, self).__init__()\n",
        "    #self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    pe = torch.zero(max_seq_len, d_model)\n",
        "    pos = torch.arange(0, max_seq_len, dtype = torch.float).unsqueeze(1)\n",
        "    div = torch.exp(torch.arange(0,d_model, 2).float() * -(math.log(10000.0)/d_model))\n",
        "    pe[:, 0::2] = torch.sin(pos * div)\n",
        "    pe[:, 1::2] = torch.cos(pos * div)\n",
        "    self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x = x + self.pe[:x.size(0), :]\n",
        "    # return self.dropout(x)\n",
        "    return x + self.pe[:, :x.size(1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGeME2voJ2Qt"
      },
      "outputs": [],
      "source": [
        "class PositionWiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(PositionWiseFeedForward, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.relu(self.fc1(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amt7ZbV-J2mP"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        attn_output = self.self_attn(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm2(x + self.dropout(ff_output))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ImJZLv19QGBb"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
        "\n",
        "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "\n",
        "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def generate_mask(self, src, tgt):\n",
        "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
        "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
        "        seq_length = tgt.size(1)\n",
        "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
        "        tgt_mask = tgt_mask & nopeak_mask\n",
        "        return src_mask, tgt_mask\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
        "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
        "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
        "\n",
        "        enc_output = src_embedded\n",
        "        for enc_layer in self.encoder_layers:\n",
        "            enc_output = enc_layer(enc_output, src_mask)\n",
        "\n",
        "        output = self.fc(enc_output)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yC35hEVGRtmM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qddcR0wvQGe0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2kanwVDQGn_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnNiS5byTvJM"
      },
      "source": [
        "# Two steps for the this task\n",
        "# first. find all relavent evidence, either use contextual embedding or similarity scoring\n",
        "# second. classify the evidents into 4 classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FA2ao2l8hOg"
      },
      "source": [
        "# 2. Model Implementation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIEqDDT78q39"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzGuzHPE87Ya"
      },
      "source": [
        "# 3.Testing and Evaluation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZVeNYIH9IaL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mefSOe8eTmGP"
      },
      "source": [
        "## Object Oriented Programming codes here\n",
        "\n",
        "*You can use multiple code snippets. Just add more if needed*"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
